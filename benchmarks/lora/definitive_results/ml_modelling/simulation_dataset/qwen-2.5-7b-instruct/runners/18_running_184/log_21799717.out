INFO 06-01 00:47:00 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:01 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9158239890821278,
    "estimated_duration": 3599.8938051305195,
    "input_throughput": 358.7916949547823,
    "output_throughput": 327.65633206150494,
    "total_throughput": 686.4480270162873,
    "itl": 18.902923817322595,
    "ttft": 4016.676788930109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.610777040058865,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9159257272258401. Arrivals time: 0.02732556313276291 Scheduler time: 0.47409404441714287 Scheduler overhead time: 0.14861606806516647 Adapter cache time: 0.03861815249547362 Engine time: 0.15174567745998502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9114573942497373,
    "estimated_duration": 3599.893929777641,
    "input_throughput": 358.79168253154074,
    "output_throughput": 327.6563207163321,
    "total_throughput": 686.4480032478729,
    "itl": 18.901986961239956,
    "ttft": 4016.8434238622135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.641928013395017,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9115534638985991. Arrivals time: 0.027382988948374987 Scheduler time: 0.47108861105516553 Scheduler overhead time: 0.15198032464832067 Adapter cache time: 0.03839030163362622 Engine time: 0.14754967717453837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.9130859537981451,
    "estimated_duration": 3599.893396535201,
    "input_throughput": 358.79173567837904,
    "output_throughput": 327.65636925117377,
    "total_throughput": 686.4481049295528,
    "itl": 18.900667521472013,
    "ttft": 4016.6219590919486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.20340750757389,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9131688447669148. Arrivals time: 0.027009683661162853 Scheduler time: 0.47204771684482694 Scheduler overhead time: 0.15043441532179713 Adapter cache time: 0.0386288003064692 Engine time: 0.14969032490625978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9019315745681524,
    "estimated_duration": 3599.894055531427,
    "input_throughput": 358.79166999800174,
    "output_throughput": 327.6563092704334,
    "total_throughput": 686.4479792684351,
    "itl": 18.902939765739276,
    "ttft": 4016.636361193445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.761343254539554,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9020143137313426. Arrivals time: 0.02694867178797722 Scheduler time: 0.4656756888143718 Scheduler overhead time: 0.14719929732382298 Adapter cache time: 0.038670319598168135 Engine time: 0.14851118391379714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.9150682711042464,
    "estimated_duration": 3599.892987939883,
    "input_throughput": 358.791776401985,
    "output_throughput": 327.6564064408511,
    "total_throughput": 686.448182842836,
    "itl": 18.899018812848862,
    "ttft": 4016.459756719781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.75653411226601,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9151966972276568. Arrivals time: 0.027332162484526634 Scheduler time: 0.4754674225114286 Scheduler overhead time: 0.1471603363752365 Adapter cache time: 0.03884280985221267 Engine time: 0.15118078468367457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 540, 540, 66, 540, 66, 135, 66, 540, 135, 540, 66, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 540, 66, 66, 66, 540, 135, 66, 540, 540, 540, 540, 135, 135, 135, 540, 66, 135, 135, 540, 540, 135, 540, 66, 540, 135, 135, 135, 540, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 540, 135]
Prompts retrieved: 16101 . Total input tokens: 3531683 . Total output tokens: 3251275
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9111769702285528,
    "estimated_duration": 3599.894181285213,
    "input_throughput": 358.79165746446364,
    "output_throughput": 327.65629782453544,
    "total_throughput": 686.4479552889991,
    "itl": 18.90308287655118,
    "ttft": 4016.657653807335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.899335172101228,
    "arrivals": 5423,
    "finished_requests": 5417,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9113859012722969. Arrivals time: 0.0274376948364079 Scheduler time: 0.4713171683251858 Scheduler overhead time: 0.14854392735287547 Adapter cache time: 0.039262058679014444 Engine time: 0.14956008596345782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.9124534311704338,
    "estimated_duration": 3598.516266109344,
    "input_throughput": 351.0841431782515,
    "output_throughput": 312.5896666339593,
    "total_throughput": 663.6738098122108,
    "itl": 18.872338080217908,
    "ttft": 2128.240414448617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2923,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.94580567551544,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9127433588728309. Arrivals time: 0.027145941741764545 Scheduler time: 0.4703923165798187 Scheduler overhead time: 0.1478254832327366 Adapter cache time: 0.037774189841002226 Engine time: 0.15398665517568588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9075034819543362,
    "estimated_duration": 3598.5168015501854,
    "input_throughput": 351.0840909387319,
    "output_throughput": 312.58962012222037,
    "total_throughput": 663.6737110609523,
    "itl": 18.876864874233068,
    "ttft": 2128.471707447763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.50508616037194,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9075861889868975. Arrivals time: 0.02722664037719369 Scheduler time: 0.4648594041354954 Scheduler overhead time: 0.14939398877322674 Adapter cache time: 0.0381825240328908 Engine time: 0.15245543513447046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.898223633877933,
    "estimated_duration": 3598.5168727287532,
    "input_throughput": 351.08408399429794,
    "output_throughput": 312.58961393920606,
    "total_throughput": 663.673697933504,
    "itl": 18.87585906040398,
    "ttft": 2128.2670652302395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.527187365814724,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.898281937930733. Arrivals time: 0.026905399281531572 Scheduler time: 0.45906181121245027 Scheduler overhead time: 0.14865776849910617 Adapter cache time: 0.03792864317074418 Engine time: 0.15037640277296305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.8966279900632799,
    "estimated_duration": 3598.5168015501854,
    "input_throughput": 351.0840909387319,
    "output_throughput": 312.58962012222037,
    "total_throughput": 663.6737110609523,
    "itl": 18.872194841526063,
    "ttft": 2128.276615581242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.143479303552246,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8967084921896458. Arrivals time: 0.026851054280996323 Scheduler time: 0.45760846976190805 Scheduler overhead time: 0.14822674449533224 Adapter cache time: 0.03781900927424431 Engine time: 0.15085268439725041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8924904624000192,
    "estimated_duration": 3598.5168727287532,
    "input_throughput": 351.08408399429794,
    "output_throughput": 312.58961393920606,
    "total_throughput": 663.673697933504,
    "itl": 18.87614450788656,
    "ttft": 2128.394030057951,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.638605220578198,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8925431971438229. Arrivals time: 0.027020281180739403 Scheduler time: 0.45458777714520693 Scheduler overhead time: 0.14928208431228995 Adapter cache time: 0.037724826484918594 Engine time: 0.14835558785125613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.905742927454412,
    "estimated_duration": 3598.515984359549,
    "input_throughput": 351.08417066677345,
    "output_throughput": 312.5896911085136,
    "total_throughput": 663.673861775287,
    "itl": 18.870763475192575,
    "ttft": 2128.208797742276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2919,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.727956810819393,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9058352252468467. Arrivals time: 0.027059923857450485 Scheduler time: 0.45933810621500015 Scheduler overhead time: 0.14866770524531603 Adapter cache time: 0.037728204391896725 Engine time: 0.15741135273128748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 540, 540, 33, 540, 33, 135, 33, 540, 135, 540, 33, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 540, 33, 33, 33, 540, 135, 33, 540, 540, 540, 540, 135, 135, 135, 540, 33, 135, 135, 540, 540, 135, 540, 33, 540, 135, 135, 135, 540, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 540, 135]
Prompts retrieved: 15408 . Total input tokens: 3371371 . Total output tokens: 3108384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.9093163199722767,
    "estimated_duration": 3598.5171242363253,
    "input_throughput": 351.08405945632785,
    "output_throughput": 312.5895920916916,
    "total_throughput": 663.6736515480194,
    "itl": 18.876489835467606,
    "ttft": 2128.2721005052895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2925,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.773661043382662,
    "arrivals": 5154,
    "finished_requests": 5151,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9093789658509195. Arrivals time: 0.026531650219112635 Scheduler time: 0.46358020370826125 Scheduler overhead time: 0.15329764923080802 Adapter cache time: 0.03819650178775191 Engine time: 0.1518152393400669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.862498571164906,
    "estimated_duration": 3599.399701563978,
    "input_throughput": 319.44160008136924,
    "output_throughput": 281.00685777145327,
    "total_throughput": 600.4484578528225,
    "itl": 18.690166022434216,
    "ttft": 3880.1970961425714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.051363762021274,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8625659300014377. Arrivals time: 0.02602043841034174 Scheduler time: 0.4235308077186346 Scheduler overhead time: 0.1499936217442155 Adapter cache time: 0.036120932549238205 Engine time: 0.1509170802310109 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8611311339773238,
    "estimated_duration": 3599.402392091154,
    "input_throughput": 319.4413613010906,
    "output_throughput": 281.00664772086566,
    "total_throughput": 600.4480090219562,
    "itl": 18.69183120589113,
    "ttft": 3880.1572849106283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.487538137903595,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.861434277612716. Arrivals time: 0.025581519585102797 Scheduler time: 0.42419316666200757 Scheduler overhead time: 0.1495193182490766 Adapter cache time: 0.03574583027511835 Engine time: 0.15038777375593781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8563037011772394,
    "estimated_duration": 3599.403324504285,
    "input_throughput": 319.44127855089755,
    "output_throughput": 281.0065749270538,
    "total_throughput": 600.4478534779514,
    "itl": 18.69220653954444,
    "ttft": 3880.084839542844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.505652220658903,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8565201512537897. Arrivals time: 0.025328371208161116 Scheduler time: 0.4196257395669818 Scheduler overhead time: 0.14994673384353518 Adapter cache time: 0.0359689025208354 Engine time: 0.1492997407913208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.8595964931882918,
    "estimated_duration": 3599.4083599897654,
    "input_throughput": 319.4408316602536,
    "output_throughput": 281.00618180563316,
    "total_throughput": 600.4470134658867,
    "itl": 18.689977122225034,
    "ttft": 3880.1185756275504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.22399415750962,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8597879982553422. Arrivals time: 0.02582344366237521 Scheduler time: 0.42238214518874884 Scheduler overhead time: 0.14904218772426248 Adapter cache time: 0.035969292279332876 Engine time: 0.1504900772124529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8603378841653466,
    "estimated_duration": 3599.408069157066,
    "input_throughput": 319.44085747111956,
    "output_throughput": 281.0062045109738,
    "total_throughput": 600.4470619820934,
    "itl": 18.692028408333254,
    "ttft": 3880.1160566827384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.583256892766639,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8603925318457186. Arrivals time: 0.02544485405087471 Scheduler time: 0.423775477334857 Scheduler overhead time: 0.14921166747808456 Adapter cache time: 0.03592784237116575 Engine time: 0.1501182932406664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.864284950774163,
    "estimated_duration": 3599.410609773793,
    "input_throughput": 319.4406319962089,
    "output_throughput": 281.00600616487196,
    "total_throughput": 600.4466381610808,
    "itl": 18.688795649716518,
    "ttft": 3879.9856652014732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.8890758794544045,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8643713919445872. Arrivals time: 0.025774585083127022 Scheduler time: 0.42437627352774143 Scheduler overhead time: 0.1486288094893098 Adapter cache time: 0.036119778640568256 Engine time: 0.15294950269162655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 540, 540, 33, 540, 33, 66, 33, 540, 66, 540, 33, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 540, 33, 33, 33, 540, 66, 33, 540, 540, 540, 540, 66, 66, 66, 540, 33, 66, 66, 540, 540, 66, 540, 33, 540, 66, 66, 66, 540, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 540, 66]
Prompts retrieved: 13959 . Total input tokens: 3052536 . Total output tokens: 2814890
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8610773608088493,
    "estimated_duration": 3599.4137169605738,
    "input_throughput": 319.44035623971433,
    "output_throughput": 281.00576358699226,
    "total_throughput": 600.4461198267065,
    "itl": 18.69339265213985,
    "ttft": 3880.4247288546194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.690761794261324,
    "arrivals": 4678,
    "finished_requests": 4673,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8611370888538659. Arrivals time: 0.025675984099507332 Scheduler time: 0.42151355650275946 Scheduler overhead time: 0.14936864655464888 Adapter cache time: 0.036039470694959164 Engine time: 0.1518736225552857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.7868971899151802,
    "estimated_duration": 3599.85605111455,
    "input_throughput": 221.6035832191156,
    "output_throughput": 205.27348580263714,
    "total_throughput": 426.87706902175273,
    "itl": 18.34113175692946,
    "ttft": 4394.390785397134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.947307178727557,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7869545691646636. Arrivals time: 0.023022024426609278 Scheduler time: 0.3515551444143057 Scheduler overhead time: 0.15182126546278596 Adapter cache time: 0.033126071095466614 Engine time: 0.15042695309966803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7762979050166905,
    "estimated_duration": 3599.8565865553915,
    "input_throughput": 221.6035502579111,
    "output_throughput": 205.27345527036306,
    "total_throughput": 426.87700552827414,
    "itl": 18.34328765867782,
    "ttft": 4394.51249298576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.398802648880219,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7764159901998937. Arrivals time: 0.02238819282501936 Scheduler time: 0.3401382565498352 Scheduler overhead time: 0.14920888701453805 Adapter cache time: 0.033370413817465305 Engine time: 0.15565671026706696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7688940069638193,
    "estimated_duration": 3599.8566577339593,
    "input_throughput": 221.60354587623016,
    "output_throughput": 205.27345121157074,
    "total_throughput": 426.8769970878009,
    "itl": 18.343818581175256,
    "ttft": 4394.558623872676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.413992824908297,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7689523547887802. Arrivals time: 0.022028323728591204 Scheduler time: 0.34011403284966946 Scheduler overhead time: 0.148067909758538 Adapter cache time: 0.0330674247816205 Engine time: 0.15001903241500258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7659392268396914,
    "estimated_duration": 3599.8565865553915,
    "input_throughput": 221.6035502579111,
    "output_throughput": 205.27345527036306,
    "total_throughput": 426.87700552827414,
    "itl": 18.342802363803635,
    "ttft": 4394.5937771272365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.104205424284779,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7660483196377754. Arrivals time: 0.021983174607157707 Scheduler time: 0.33876453852280974 Scheduler overhead time: 0.14731575595214963 Adapter cache time: 0.03349577262997627 Engine time: 0.14920149790123105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7686338499188423,
    "estimated_duration": 3599.8566577339593,
    "input_throughput": 221.60354587623016,
    "output_throughput": 205.27345121157074,
    "total_throughput": 426.8769970878009,
    "itl": 18.343450978689656,
    "ttft": 4394.50563807608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.501154784783559,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7687104027718306. Arrivals time: 0.022352056577801704 Scheduler time: 0.3396721435710788 Scheduler overhead time: 0.14747569244354963 Adapter cache time: 0.033043688628822565 Engine time: 0.15095759881660342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.7976230885833502,
    "estimated_duration": 3599.855769364755,
    "input_throughput": 221.60360056335608,
    "output_throughput": 205.2735018687704,
    "total_throughput": 426.8771024321265,
    "itl": 18.339853998384214,
    "ttft": 4394.387748925481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.790404219722636,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7976784938946366. Arrivals time: 0.022090659011155367 Scheduler time: 0.3616491458378732 Scheduler overhead time: 0.15262177772819996 Adapter cache time: 0.03341410495340824 Engine time: 0.1509756133891642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 270, 270, 66, 270, 66, 135, 66, 270, 135, 270, 66, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 270, 66, 66, 66, 270, 135, 66, 270, 270, 270, 270, 135, 135, 135, 270, 66, 135, 135, 270, 270, 135, 270, 66, 270, 135, 135, 135, 270, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 270, 135]
Prompts retrieved: 10161 . Total input tokens: 2245831 . Total output tokens: 2048489
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7679999778047204,
    "estimated_duration": 3599.8569092415314,
    "input_throughput": 221.6035303936787,
    "output_throughput": 205.27343686993754,
    "total_throughput": 426.8769672636163,
    "itl": 18.34431809838473,
    "ttft": 4394.646064768318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.598196694031061,
    "arrivals": 3301,
    "finished_requests": 3297,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7680560508742929. Arrivals time: 0.021851098630577326 Scheduler time: 0.34042203705757856 Scheduler overhead time: 0.14867767039686441 Adapter cache time: 0.03286025021225214 Engine time: 0.14881785213947296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.7488291920162737,
    "estimated_duration": 3599.3681024395064,
    "input_throughput": 207.2869400310355,
    "output_throughput": 187.42817650208318,
    "total_throughput": 394.71511653311865,
    "itl": 18.22362881565053,
    "ttft": 5883.583317595328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.111794024633886,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7489011958241463. Arrivals time: 0.021589577663689852 Scheduler time: 0.3232646915130317 Scheduler overhead time: 0.14666468184441328 Adapter cache time: 0.032227261923253536 Engine time: 0.1500941594131291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7472704430110753,
    "estimated_duration": 3599.3688351633195,
    "input_throughput": 207.28689783361588,
    "output_throughput": 187.4281383473137,
    "total_throughput": 394.7150361809296,
    "itl": 18.225005763856586,
    "ttft": 5883.719956171433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.511293449897564,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7473265212029219. Arrivals time: 0.021927851252257824 Scheduler time: 0.3236158387735486 Scheduler overhead time: 0.14618810499086976 Adapter cache time: 0.032023805659264326 Engine time: 0.14813224272802472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7473309179767966,
    "estimated_duration": 3599.368995399725,
    "input_throughput": 207.28688860563523,
    "output_throughput": 187.42813000340362,
    "total_throughput": 394.71501860903885,
    "itl": 18.22508390334186,
    "ttft": 5883.790081470045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.523748328760214,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7473935252055526. Arrivals time: 0.021404137834906578 Scheduler time: 0.3217340619303286 Scheduler overhead time: 0.14788681082427502 Adapter cache time: 0.032099721021950245 Engine time: 0.1492367503233254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7534835762344301,
    "estimated_duration": 3599.368017972683,
    "input_throughput": 207.28694489546427,
    "output_throughput": 187.42818090048385,
    "total_throughput": 394.7151257959481,
    "itl": 18.223785674697076,
    "ttft": 5883.526231866796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.242846325682282,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7535533509217203. Arrivals time: 0.02173429960384965 Scheduler time: 0.3278765841387212 Scheduler overhead time: 0.14864912582561374 Adapter cache time: 0.032012066803872585 Engine time: 0.14722071448341012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7469930481165648,
    "estimated_duration": 3599.369246907297,
    "input_throughput": 207.2868741213691,
    "output_throughput": 187.42811690677735,
    "total_throughput": 394.71499102814647,
    "itl": 18.224459698970822,
    "ttft": 5883.656789154018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1998,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.609875086564452,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7470659883692861. Arrivals time: 0.021591599564999342 Scheduler time: 0.3216531570069492 Scheduler overhead time: 0.1470025973394513 Adapter cache time: 0.032479850109666586 Engine time: 0.14885455602779984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.7448047436773777,
    "estimated_duration": 3599.3676093773647,
    "input_throughput": 207.2869684263965,
    "output_throughput": 187.42820217707617,
    "total_throughput": 394.71517060347264,
    "itl": 18.221803365590326,
    "ttft": 5883.65570816534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.971130438919469,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7448974936269224. Arrivals time: 0.021341213956475258 Scheduler time: 0.3227002206258476 Scheduler overhead time: 0.14607752859592438 Adapter cache time: 0.03199621103703976 Engine time: 0.14795382041484118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 270, 270, 33, 270, 33, 135, 33, 270, 135, 270, 33, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 270, 33, 33, 33, 270, 135, 33, 270, 270, 270, 270, 135, 135, 135, 270, 33, 135, 135, 270, 270, 135, 270, 33, 270, 135, 135, 135, 270, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 270, 135]
Prompts retrieved: 9468 . Total input tokens: 2092242 . Total output tokens: 1915446
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.753425267059356,
    "estimated_duration": 3599.369372661083,
    "input_throughput": 207.28686687923678,
    "output_throughput": 187.4281103584649,
    "total_throughput": 394.7149772377017,
    "itl": 18.226624306951415,
    "ttft": 5883.925028516055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.690457835681531,
    "arrivals": 3076,
    "finished_requests": 3071,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7535037747584283. Arrivals time: 0.022274453192949295 Scheduler time: 0.3282542512752116 Scheduler overhead time: 0.14698316249996424 Adapter cache time: 0.031944484915584326 Engine time: 0.14864382660016418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.7060294123366475,
    "estimated_duration": 3599.4425021977304,
    "input_throughput": 175.99864412702868,
    "output_throughput": 157.61190785895636,
    "total_throughput": 333.610551985985,
    "itl": 18.102759102687887,
    "ttft": 16704.11365872809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6703042972415085,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7061029314063489. Arrivals time: 0.020242424681782722 Scheduler time: 0.29011173779144883 Scheduler overhead time: 0.1444666306488216 Adapter cache time: 0.03017141204327345 Engine time: 0.14677679911255836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7051720819436014,
    "estimated_duration": 3599.432322158014,
    "input_throughput": 175.99914189251692,
    "output_throughput": 157.61235362243744,
    "total_throughput": 333.61149551495436,
    "itl": 18.10585119298839,
    "ttft": 16704.389811112505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.965691734002818,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7052319590002298. Arrivals time: 0.02042967500165105 Scheduler time: 0.28903305996209383 Scheduler overhead time: 0.14610245218500495 Adapter cache time: 0.030117259360849857 Engine time: 0.1455139913596213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7104912712238729,
    "estimated_duration": 3599.432666167417,
    "input_throughput": 175.9991250717106,
    "output_throughput": 157.61233855891584,
    "total_throughput": 333.6114636306264,
    "itl": 18.10592396727139,
    "ttft": 16704.373300607363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.976935792826061,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7105463943444192. Arrivals time: 0.02008088631555438 Scheduler time: 0.293771396856755 Scheduler overhead time: 0.14529397059231997 Adapter cache time: 0.030245435889810324 Engine time: 0.1473783776164055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7099881009198725,
    "estimated_duration": 3599.425721686212,
    "input_throughput": 175.99946463216014,
    "output_throughput": 157.61264264518053,
    "total_throughput": 333.61210727734067,
    "itl": 18.1042165728833,
    "ttft": 16704.039277064105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.778146482838685,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7100409758277237. Arrivals time: 0.020502306055277586 Scheduler time: 0.287901459261775 Scheduler overhead time: 0.14758981578052044 Adapter cache time: 0.030484759714454412 Engine time: 0.14936640113592148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7094614626839757,
    "estimated_duration": 3599.4353693187672,
    "input_throughput": 175.99899289757113,
    "output_throughput": 157.61222019312729,
    "total_throughput": 333.6112130906984,
    "itl": 18.10625835200075,
    "ttft": 16704.489742284608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.034656780790485,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7095588040538132. Arrivals time: 0.020267133601009846 Scheduler time: 0.29027238115668297 Scheduler overhead time: 0.146650071721524 Adapter cache time: 0.030175670515745878 Engine time: 0.14827369200065732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.709776340983808,
    "estimated_duration": 3599.438499181383,
    "input_throughput": 175.99883985907124,
    "output_throughput": 157.6120831426968,
    "total_throughput": 333.61092300176807,
    "itl": 18.102120374215566,
    "ttft": 16703.977749580492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.562816750020595,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7098317369818687. Arrivals time: 0.020310421474277973 Scheduler time: 0.2890172805637121 Scheduler overhead time: 0.14925772743299603 Adapter cache time: 0.030184431932866573 Engine time: 0.14702330343425274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 270, 270, 33, 270, 33, 66, 33, 270, 66, 270, 33, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 270, 33, 33, 33, 270, 66, 33, 270, 270, 270, 270, 66, 66, 66, 270, 33, 66, 66, 270, 270, 66, 270, 33, 270, 66, 66, 66, 270, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 270, 66]
Prompts retrieved: 8019 . Total input tokens: 1768810 . Total output tokens: 1619736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7057323795743287,
    "estimated_duration": 3599.4382542646285,
    "input_throughput": 175.998851834569,
    "output_throughput": 157.61209386710357,
    "total_throughput": 333.61094570167256,
    "itl": 18.106634157565864,
    "ttft": 16704.529449680904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.10092902623112,
    "arrivals": 2591,
    "finished_requests": 2579,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7057807589881122. Arrivals time: 0.020370515063405037 Scheduler time: 0.28888344345614314 Scheduler overhead time: 0.14505132008343935 Adapter cache time: 0.030215340666472912 Engine time: 0.14701342023909092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.575370084028691,
    "estimated_duration": 3599.293080398231,
    "input_throughput": 115.53018626479628,
    "output_throughput": 97.39829243391678,
    "total_throughput": 212.92847869871306,
    "itl": 17.748540100597733,
    "ttft": 8783.747837364228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3542945673503786,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5754305659793317. Arrivals time: 0.016581361647695303 Scheduler time: 0.21186048770323396 Scheduler overhead time: 0.12722814921289682 Adapter cache time: 0.02478363923728466 Engine time: 0.12948267394676805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.5767519823275506,
    "estimated_duration": 3599.278403679067,
    "input_throughput": 115.53065736036282,
    "output_throughput": 97.39868959335395,
    "total_throughput": 212.9293469537168,
    "itl": 17.750757093813984,
    "ttft": 8783.781073432907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5729181337636007,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5768125862814486. Arrivals time: 0.016560315620154142 Scheduler time: 0.21170177776366472 Scheduler overhead time: 0.12816652934998274 Adapter cache time: 0.024722403846681118 Engine time: 0.1301084035076201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.575165641028434,
    "estimated_duration": 3599.278402832789,
    "input_throughput": 115.5306573875269,
    "output_throughput": 97.39868961625476,
    "total_throughput": 212.92934700378166,
    "itl": 17.750777916929607,
    "ttft": 8783.771708373948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5798638442903283,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.575265611987561. Arrivals time: 0.016188615933060646 Scheduler time: 0.21126149594783783 Scheduler overhead time: 0.1275453446432948 Adapter cache time: 0.024793533608317375 Engine time: 0.13019357388839126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 0.5767528316937387,
    "estimated_duration": 3599.293714625846,
    "input_throughput": 115.53016590734832,
    "output_throughput": 97.39827527147001,
    "total_throughput": 212.92844117881833,
    "itl": 17.74986642090248,
    "ttft": 8783.682411503667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.429092581672556,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.576804697047919. Arrivals time: 0.01649665553122759 Scheduler time: 0.21079850709065795 Scheduler overhead time: 0.1279355906881392 Adapter cache time: 0.025034192018210888 Engine time: 0.13096506241708994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 0.5734200677834451,
    "estimated_duration": 3599.2796603706497,
    "input_throughput": 115.53061702273466,
    "output_throughput": 97.39865558651789,
    "total_throughput": 212.92927260925254,
    "itl": 17.75105253865722,
    "ttft": 8783.879459449485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6241291771084065,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5734756789170206. Arrivals time: 0.016319405753165483 Scheduler time: 0.21116460161283612 Scheduler overhead time: 0.12724861782044172 Adapter cache time: 0.02464922470971942 Engine time: 0.1286282460205257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 0.5753344460390508,
    "estimated_duration": 3599.291400184388,
    "input_throughput": 115.53024019636132,
    "output_throughput": 97.39833790118826,
    "total_throughput": 212.92857809754958,
    "itl": 17.748061977299024,
    "ttft": 8783.708622253322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.277095123212705,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5753988260403275. Arrivals time: 0.016147791408002377 Scheduler time: 0.21115149930119514 Scheduler overhead time: 0.12699542613700032 Adapter cache time: 0.024723758455365896 Engine time: 0.1311650718562305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 135, 135, 33, 135, 33, 66, 33, 135, 66, 135, 33, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 135, 33, 33, 33, 135, 66, 33, 135, 135, 135, 135, 66, 66, 66, 135, 33, 66, 66, 135, 135, 66, 135, 33, 135, 66, 66, 66, 135, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 135, 66]
Prompts retrieved: 5049 . Total input tokens: 1136537 . Total output tokens: 1002507
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 0.5735083911567926,
    "estimated_duration": 3599.28028913958,
    "input_throughput": 115.53059684034912,
    "output_throughput": 97.39863857165838,
    "total_throughput": 212.9292354120075,
    "itl": 17.751283320611993,
    "ttft": 8783.904579238459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1096,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6709095856547864,
    "arrivals": 1645,
    "finished_requests": 1641,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5735803782008588. Arrivals time: 0.016572130378335714 Scheduler time: 0.211914271581918 Scheduler overhead time: 0.12713438365608454 Adapter cache time: 0.024683484341949224 Engine time: 0.12801389070227742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 86.73712258972228,
    "estimated_duration": 3600.078927598314,
    "input_throughput": 7710.287901526007,
    "output_throughput": 6821.326002533696,
    "total_throughput": 14531.613904059703,
    "itl": 125.28036855022695,
    "ttft": 1748188.5453635203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3091092621372078,
    "arrivals": 435333,
    "finished_requests": 111515,
    "scheduler_time": 154.56224403638063
}
#Debug simulation 
Total elapsed time: 86.73728482285514. Arrivals time: 0.4631411926820874 Scheduler time: 86.10444580763578 Scheduler overhead time: 0.06573758041486144 Adapter cache time: 0.011997330468147993 Engine time: 0.06613316806033254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.79897806420922,
    "estimated_duration": 3600.0761503618414,
    "input_throughput": 7706.485874531147,
    "output_throughput": 6817.918281404407,
    "total_throughput": 14524.404155935554,
    "itl": 125.25999206576647,
    "ttft": 1747631.0494632274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3297795614390634,
    "arrivals": 435333,
    "finished_requests": 111472,
    "scheduler_time": 154.61022793407653
}
#Debug simulation 
Total elapsed time: 84.79914738005027. Arrivals time: 0.4584582122042775 Scheduler time: 84.1732025584206 Scheduler overhead time: 0.06473567662760615 Adapter cache time: 0.011644827667623758 Engine time: 0.06544799311086535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.67451911838725,
    "estimated_duration": 3600.0767048079206,
    "input_throughput": 7706.48468765897,
    "output_throughput": 6817.917231380096,
    "total_throughput": 14524.401919039066,
    "itl": 125.26000117568812,
    "ttft": 1747631.3369389202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.330328233353794,
    "arrivals": 435333,
    "finished_requests": 111472,
    "scheduler_time": 154.61023370823727
}
#Debug simulation 
Total elapsed time: 84.67467872798443. Arrivals time: 0.4561300599016249 Scheduler time: 84.05010801553726 Scheduler overhead time: 0.06489741988480091 Adapter cache time: 0.0120389717631042 Engine time: 0.06582734920084476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 84.87451625801623,
    "estimated_duration": 3600.0624335175658,
    "input_throughput": 7706.51523754043,
    "output_throughput": 6817.944258821487,
    "total_throughput": 14524.459496361917,
    "itl": 125.25962726403925,
    "ttft": 1747626.5231278406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3171131065674127,
    "arrivals": 435333,
    "finished_requests": 111472,
    "scheduler_time": 154.60970530835243
}
#Debug simulation 
Total elapsed time: 84.87467882083729. Arrivals time: 0.47061230847612023 Scheduler time: 84.23668867256492 Scheduler overhead time: 0.06498848786577582 Adapter cache time: 0.011670072097331285 Engine time: 0.06481843208894134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 83.79852447099984,
    "estimated_duration": 3600.08137878581,
    "input_throughput": 7707.278553063737,
    "output_throughput": 6819.768059876606,
    "total_throughput": 14527.046612940343,
    "itl": 125.27591685850031,
    "ttft": 1748192.6646472905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3342266007326544,
    "arrivals": 435333,
    "finished_requests": 111480,
    "scheduler_time": 154.61067330948487
}
#Debug simulation 
Total elapsed time: 83.7986940429546. Arrivals time: 0.47463730117306113 Scheduler time: 83.15877475729212 Scheduler overhead time: 0.06372718932107091 Adapter cache time: 0.011753226164728403 Engine time: 0.06459349114447832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.04091176576912,
    "estimated_duration": 3600.028434536654,
    "input_throughput": 7706.884960638083,
    "output_throughput": 6817.736150230983,
    "total_throughput": 14524.621110869066,
    "itl": 125.25516952588505,
    "ttft": 1747657.872304666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3019950797851201,
    "arrivals": 435333,
    "finished_requests": 111474,
    "scheduler_time": 154.60810859097808
}
#Debug simulation 
Total elapsed time: 85.04116790974513. Arrivals time: 0.46632743487134576 Scheduler time: 84.4055525995791 Scheduler overhead time: 0.06558552477508783 Adapter cache time: 0.012261989526450634 Engine time: 0.06524579552933574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [8640, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 17280, 34560, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 8640, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 8640, 34560, 17280]
Prompts retrieved: 1304640 . Total input tokens: 290666445 . Total output tokens: 261325878
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 84.14859434124082,
    "estimated_duration": 3600.0851734528055,
    "input_throughput": 7707.270429212733,
    "output_throughput": 6819.760871505352,
    "total_throughput": 14527.031300718085,
    "itl": 125.27588887684286,
    "ttft": 1748194.312783772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33887949083000385,
    "arrivals": 435333,
    "finished_requests": 111480,
    "scheduler_time": 154.61071992663855
}
#Debug simulation 
Total elapsed time: 84.14875519834459. Arrivals time: 0.470827859826386 Scheduler time: 83.51225773105398 Scheduler overhead time: 0.06376464804634452 Adapter cache time: 0.01174157578498125 Engine time: 0.06451760372146964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 101.39490305399522,
    "estimated_duration": 3600.074040472198,
    "input_throughput": 7736.173947229984,
    "output_throughput": 6843.2295344594195,
    "total_throughput": 14579.403481689404,
    "itl": 124.72768189702138,
    "ttft": 1718078.889689787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3274721886008043,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.60931521580417
}
#Debug simulation 
Total elapsed time: 101.39506542403251. Arrivals time: 0.49776932038366795 Scheduler time: 100.72138782497495 Scheduler overhead time: 0.06815965427085757 Adapter cache time: 0.012929274700582027 Engine time: 0.06829241942614317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 100.85418696701527,
    "estimated_duration": 3600.0940418558816,
    "input_throughput": 7736.1309666351535,
    "output_throughput": 6843.1915148804965,
    "total_throughput": 14579.32248151565,
    "itl": 124.72768977689988,
    "ttft": 1718086.691995039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34935424448223784,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.6095472675738
}
#Debug simulation 
Total elapsed time: 100.85434512002394. Arrivals time: 0.5298257651738822 Scheduler time: 100.14825159031898 Scheduler overhead time: 0.06867903377860785 Adapter cache time: 0.012810526881366968 Engine time: 0.06864495715126395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 101.13213379215449,
    "estimated_duration": 3600.09462319285,
    "input_throughput": 7736.129717418287,
    "output_throughput": 6843.190409853927,
    "total_throughput": 14579.320127272214,
    "itl": 124.72769876951605,
    "ttft": 1718086.965971077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3499383364245301,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.60955104582223
}
#Debug simulation 
Total elapsed time: 101.1322963302955. Arrivals time: 0.5298533113673329 Scheduler time: 100.42519984720275 Scheduler overhead time: 0.06827405700460076 Adapter cache time: 0.012839736882597208 Engine time: 0.06929745897650719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 101.08212447213009,
    "estimated_duration": 3600.081529026605,
    "input_throughput": 7736.157855161224,
    "output_throughput": 6843.215299810488,
    "total_throughput": 14579.373154971712,
    "itl": 124.72775796433545,
    "ttft": 1718081.4172240696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3346448130183855,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.60941466754457
}
#Debug simulation 
Total elapsed time: 101.0822864160873. Arrivals time: 0.4989142660051584 Scheduler time: 100.40696450415999 Scheduler overhead time: 0.06847657216712832 Adapter cache time: 0.01305578788742423 Engine time: 0.06849439907819033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 101.31739382492378,
    "estimated_duration": 3600.0992602071424,
    "input_throughput": 7736.119753097452,
    "output_throughput": 6843.181595660361,
    "total_throughput": 14579.301348757814,
    "itl": 124.72762198108333,
    "ttft": 1718088.8619375955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35446547273546475,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.60961539813297
}
#Debug simulation 
Total elapsed time: 101.31756346393377. Arrivals time: 0.5325299939140677 Scheduler time: 100.60919111780822 Scheduler overhead time: 0.06826583202928305 Adapter cache time: 0.012800929136574268 Engine time: 0.06816351460292935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 101.21536188293248,
    "estimated_duration": 3600.06309294699,
    "input_throughput": 7736.197472361937,
    "output_throughput": 6843.250344213554,
    "total_throughput": 14579.447816575492,
    "itl": 124.72713405878504,
    "ttft": 1718077.5657138193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31993538155453316,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.6086922422449
}
#Debug simulation 
Total elapsed time: 101.21551766991615. Arrivals time: 0.49504646845161915 Scheduler time: 100.54381995927542 Scheduler overhead time: 0.06822861917316914 Adapter cache time: 0.013108359184116125 Engine time: 0.06876696506515145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 17280, 34560, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 4320, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 4320, 34560, 17280]
Prompts retrieved: 1213920 . Total input tokens: 270411658 . Total output tokens: 243281069
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.73669549683109,
    "estimated_duration": 3600.1056233274367,
    "input_throughput": 7736.106079648463,
    "output_throughput": 6843.1695004630965,
    "total_throughput": 14579.275580111558,
    "itl": 124.72775104339013,
    "ttft": 1718091.7791265629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3589926090463993,
    "arrivals": 404972,
    "finished_requests": 112259,
    "scheduler_time": 154.60986062982158
}
#Debug simulation 
Total elapsed time: 104.73685731273144. Arrivals time: 0.5279888310469687 Scheduler time: 104.03025729581714 Scheduler overhead time: 0.07047770172357559 Adapter cache time: 0.01258456101641059 Engine time: 0.06889821402728558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 130.36648681294173,
    "estimated_duration": 3600.105503936229,
    "input_throughput": 7717.57910139629,
    "output_throughput": 6835.207738521845,
    "total_throughput": 14552.786839918135,
    "itl": 125.72696818769529,
    "ttft": 1686514.2165200524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20505267884349432,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.714735428935
}
#Debug simulation 
Total elapsed time: 130.36664886120707. Arrivals time: 0.5348010361194611 Scheduler time: 129.6421458045952 Scheduler overhead time: 0.0743046784773469 Adapter cache time: 0.013687832280993462 Engine time: 0.07446307735517621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 130.34819002775475,
    "estimated_duration": 3600.1266890107418,
    "input_throughput": 7717.533687025507,
    "output_throughput": 6835.1675164969665,
    "total_throughput": 14552.701203522474,
    "itl": 125.7270871573923,
    "ttft": 1686529.2813901098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21872015908826142,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71580610561912
}
#Debug simulation 
Total elapsed time: 130.34834769601002. Arrivals time: 0.5221297089010477 Scheduler time: 129.636531741824 Scheduler overhead time: 0.07480482524260879 Adapter cache time: 0.013476554304361343 Engine time: 0.07384745171293616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 130.12208552425727,
    "estimated_duration": 3600.1269426094577,
    "input_throughput": 7717.533143390056,
    "output_throughput": 6835.167035016804,
    "total_throughput": 14552.70017840686,
    "itl": 125.72706768736894,
    "ttft": 1686529.4023687227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21909190012142052,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71581086816727
}
#Debug simulation 
Total elapsed time: 130.12224461929873. Arrivals time: 0.5058744759298861 Scheduler time: 129.42719315132126 Scheduler overhead time: 0.07294829608872533 Adapter cache time: 0.013999367132782936 Engine time: 0.07485860818997025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 130.48482185788453,
    "estimated_duration": 3600.1121945289096,
    "input_throughput": 7717.564758738212,
    "output_throughput": 6835.195035698046,
    "total_throughput": 14552.759794436257,
    "itl": 125.72696379266598,
    "ttft": 1686517.2468448526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20932246676413352,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71489576637174
}
#Debug simulation 
Total elapsed time: 130.4849703349173. Arrivals time: 0.5253375540487468 Scheduler time: 129.76779783610255 Scheduler overhead time: 0.07381794089451432 Adapter cache time: 0.014036420732736588 Engine time: 0.07568447384983301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 129.83980149403214,
    "estimated_duration": 3600.1325801495764,
    "input_throughput": 7717.521058306592,
    "output_throughput": 6835.156331653103,
    "total_throughput": 14552.677389959696,
    "itl": 125.7271494108867,
    "ttft": 1686532.2946769907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22198423720896224,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71595465548785
}
#Debug simulation 
Total elapsed time: 129.8400592734106. Arrivals time: 0.5166777824051678 Scheduler time: 129.13246714090928 Scheduler overhead time: 0.07449801499024034 Adapter cache time: 0.014045285992324352 Engine time: 0.0742065878584981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 130.66474515385926,
    "estimated_duration": 3600.130429759523,
    "input_throughput": 7717.525668050835,
    "output_throughput": 6835.160414353016,
    "total_throughput": 14552.68608240385,
    "itl": 125.7258384198013,
    "ttft": 1686549.7525316114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20033336975844596,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71641649071591
}
#Debug simulation 
Total elapsed time: 130.66490603704005. Arrivals time: 0.5171482446603477 Scheduler time: 129.95856158947572 Scheduler overhead time: 0.07362805446609855 Adapter cache time: 0.01386022986844182 Engine time: 0.07414723560214043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 17280, 34560, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 1080, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 1080, 34560, 17280]
Prompts retrieved: 1145880 . Total input tokens: 255426271 . Total output tokens: 229483873
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 130.5105454060249,
    "estimated_duration": 3600.1325377275557,
    "input_throughput": 7717.521149245699,
    "output_throughput": 6835.1564121948995,
    "total_throughput": 14552.6775614406,
    "itl": 125.72706353772176,
    "ttft": 1686532.1159475378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 67,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2247508205100891,
    "arrivals": 382341,
    "finished_requests": 112344,
    "scheduler_time": 153.71593687689847
}
#Debug simulation 
Total elapsed time: 130.51070878794417. Arrivals time: 0.5141648096032441 Scheduler time: 129.80757095757872 Scheduler overhead time: 0.0735459215939045 Adapter cache time: 0.014106775168329477 Engine time: 0.0736629324965179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 136.8155994298868,
    "estimated_duration": 3600.1078128457825,
    "input_throughput": 7766.0327005316985,
    "output_throughput": 6842.302864404579,
    "total_throughput": 14608.335564936277,
    "itl": 125.08683885523922,
    "ttft": 1671791.2456197082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22341560530709081,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.09502195923275
}
#Debug simulation 
Total elapsed time: 136.81575805228204. Arrivals time: 0.5368131319992244 Scheduler time: 136.08283785171807 Scheduler overhead time: 0.07688328810036182 Adapter cache time: 0.014758910983800888 Engine time: 0.07573733199387789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 136.91701226308942,
    "estimated_duration": 3600.125505226981,
    "input_throughput": 7765.994535303642,
    "output_throughput": 6842.269238735035,
    "total_throughput": 14608.263774038678,
    "itl": 125.08750118549631,
    "ttft": 1671789.0540397875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23829484213143584,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.09650219729332
}
#Debug simulation 
Total elapsed time: 136.9171687909402. Arrivals time: 0.5422193664126098 Scheduler time: 136.1789152994752 Scheduler overhead time: 0.07624532235786319 Adapter cache time: 0.014907346107065678 Engine time: 0.07644177041947842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.15333493426442,
    "estimated_duration": 3600.1260387794478,
    "input_throughput": 7765.99338435351,
    "output_throughput": 6842.26822468453,
    "total_throughput": 14608.26160903804,
    "itl": 125.08750675620404,
    "ttft": 1671789.3861322457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2387020031921564,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.09651630031686
}
#Debug simulation 
Total elapsed time: 137.15349439298734. Arrivals time: 0.5314337834715843 Scheduler time: 136.42559196148068 Scheduler overhead time: 0.07645456260070205 Adapter cache time: 0.014542933087795973 Engine time: 0.076460144482553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 137.15313994698226,
    "estimated_duration": 3600.111735753563,
    "input_throughput": 7766.024238174878,
    "output_throughput": 6842.295408601783,
    "total_throughput": 14608.31964677666,
    "itl": 125.08772440376207,
    "ttft": 1671785.7718060797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22807995917042728,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.0958234619193
}
#Debug simulation 
Total elapsed time: 137.15330054564402. Arrivals time: 0.533374004997313 Scheduler time: 136.4243149566464 Scheduler overhead time: 0.07648123567923903 Adapter cache time: 0.01434865826740861 Engine time: 0.07658583298325539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 136.9617660632357,
    "estimated_duration": 3600.133807328029,
    "input_throughput": 7765.97662650502,
    "output_throughput": 6842.253460096335,
    "total_throughput": 14608.230086601356,
    "itl": 125.08751168030186,
    "ttft": 1671792.3948239987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24184584785252783,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.0967584909016
}
#Debug simulation 
Total elapsed time: 136.9619180872105. Arrivals time: 0.5306477141566575 Scheduler time: 136.23518937081099 Scheduler overhead time: 0.07664439408108592 Adapter cache time: 0.014477460645139217 Engine time: 0.07595407729968429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 136.92569887591526,
    "estimated_duration": 3600.130619827022,
    "input_throughput": 7765.98350238285,
    "output_throughput": 6842.259518123695,
    "total_throughput": 14608.243020506545,
    "itl": 125.08653297675122,
    "ttft": 1671792.096472278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21827367152785904,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.0973570909834
}
#Debug simulation 
Total elapsed time: 136.92586170509458. Arrivals time: 0.5262891775928438 Scheduler time: 136.20427169976756 Scheduler overhead time: 0.07686508074402809 Adapter cache time: 0.014577336143702269 Engine time: 0.07604856370016932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 34560, 17280, 34560, 540, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 540, 540, 34560, 17280, 540, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 540, 17280, 17280, 34560, 34560, 17280, 34560, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 540, 17280, 17280, 540, 17280, 540, 540, 540, 34560, 17280]
Prompts retrieved: 1134540 . Total input tokens: 252921097 . Total output tokens: 227201613
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.9134846907109,
    "estimated_duration": 3600.1345190463594,
    "input_throughput": 7765.97509123241,
    "output_throughput": 6842.252107436544,
    "total_throughput": 14608.227198668954,
    "itl": 125.08770718908374,
    "ttft": 1671788.2989407312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24486393872648443,
    "arrivals": 378554,
    "finished_requests": 112631,
    "scheduler_time": 154.09712493324804
}
#Debug simulation 
Total elapsed time: 137.91365095088258. Arrivals time: 0.7934723035432398 Scheduler time: 136.92338314279914 Scheduler overhead time: 0.07673914963379502 Adapter cache time: 0.014667645562440157 Engine time: 0.07645333651453257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 108.41632890282199,
    "estimated_duration": 3600.1364380297177,
    "input_throughput": 7765.534579378413,
    "output_throughput": 6926.174723990883,
    "total_throughput": 14691.709303369295,
    "itl": 124.529277545842,
    "ttft": 1686961.4440395555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2846253601857458,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.4700015213334
}
#Debug simulation 
Total elapsed time: 108.41649767383933. Arrivals time: 0.5244964039884508 Scheduler time: 107.70907820481807 Scheduler overhead time: 0.07079869182780385 Adapter cache time: 0.014291367027908564 Engine time: 0.07101830514147878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.13173628924415,
    "estimated_duration": 3600.122534510153,
    "input_throughput": 7765.564569541503,
    "output_throughput": 6926.201472582038,
    "total_throughput": 14691.766042123541,
    "itl": 124.53049599275069,
    "ttft": 1686956.2237752592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30381618248764425,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.46827532891967
}
#Debug simulation 
Total elapsed time: 114.13190282322466. Arrivals time: 0.5010052267462015 Scheduler time: 113.44778826087713 Scheduler overhead time: 0.07069331267848611 Adapter cache time: 0.013895257376134396 Engine time: 0.07163333985954523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 108.43802670994774,
    "estimated_duration": 3600.123973213174,
    "input_throughput": 7765.561466220259,
    "output_throughput": 6926.198704692083,
    "total_throughput": 14691.760170912343,
    "itl": 124.53052019675447,
    "ttft": 1686957.194183477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30429384509101537,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.46833152907996
}
#Debug simulation 
Total elapsed time: 108.43819400388747. Arrivals time: 0.5280741131864488 Scheduler time: 107.726716728881 Scheduler overhead time: 0.07045306917279959 Adapter cache time: 0.01446025213226676 Engine time: 0.07110698893666267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 109.28051771782339,
    "estimated_duration": 3600.1414680227563,
    "input_throughput": 7765.523729642306,
    "output_throughput": 6926.165046979311,
    "total_throughput": 14691.688776621617,
    "itl": 124.52927405317071,
    "ttft": 1686962.9286879266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2895153463422322,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.47006632810914
}
#Debug simulation 
Total elapsed time: 109.28081760182977. Arrivals time: 0.7633984899148345 Scheduler time: 108.33326275926083 Scheduler overhead time: 0.07110114162787795 Adapter cache time: 0.014493751339614391 Engine time: 0.07163128303363919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 108.48173256218433,
    "estimated_duration": 3600.133595272805,
    "input_throughput": 7765.540711241723,
    "output_throughput": 6926.180193074336,
    "total_throughput": 14691.720904316058,
    "itl": 124.53084006499415,
    "ttft": 1686957.8595350257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30869522761553525,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.4688161557618
}
#Debug simulation 
Total elapsed time: 108.48190008336678. Arrivals time: 0.5074825426563621 Scheduler time: 107.78968524979427 Scheduler overhead time: 0.07091428199782968 Adapter cache time: 0.014301205519586802 Engine time: 0.07232166826725006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 109.3374476111494,
    "estimated_duration": 3600.1298755183197,
    "input_throughput": 7765.54873481473,
    "output_throughput": 6926.187349396671,
    "total_throughput": 14691.736084211401,
    "itl": 124.52934111337879,
    "ttft": 1686959.153919476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27807467742590264,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.46985411964172
}
#Debug simulation 
Total elapsed time: 109.33761607808992. Arrivals time: 0.5118841486983001 Scheduler time: 108.64179486175999 Scheduler overhead time: 0.07183752302080393 Adapter cache time: 0.014485311694443226 Engine time: 0.07120965607464314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 108.23904514405876,
    "estimated_duration": 3600.1346635554237,
    "input_throughput": 7765.538406941206,
    "output_throughput": 6926.178137840683,
    "total_throughput": 14691.71654478189,
    "itl": 124.5308641421004,
    "ttft": 1686954.7973762897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31221633363515117,
    "arrivals": 376669,
    "finished_requests": 113418,
    "scheduler_time": 155.46914715839114
}
#Debug simulation 
Total elapsed time: 108.23921422706917. Arrivals time: 0.5148315960541368 Scheduler time: 107.52814575098455 Scheduler overhead time: 0.07161729456856847 Adapter cache time: 0.014421736355870962 Engine time: 0.08408710034564137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 139.1277497438714,
    "estimated_duration": 3600.124166174125,
    "input_throughput": 7751.5170899386885,
    "output_throughput": 6896.085483180309,
    "total_throughput": 14647.602573118997,
    "itl": 124.78137974238707,
    "ttft": 1664793.4931906261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20199219109956157,
    "arrivals": 375735,
    "finished_requests": 113139,
    "scheduler_time": 154.8845799725874
}
#Debug simulation 
Total elapsed time: 139.1279072049074. Arrivals time: 0.5469143674708903 Scheduler time: 138.38354831421748 Scheduler overhead time: 0.07705576159060001 Adapter cache time: 0.014771366491913795 Engine time: 0.07708126166835427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 139.68925614189357,
    "estimated_duration": 3600.0103579264337,
    "input_throughput": 7751.554086103619,
    "output_throughput": 6896.137102866448,
    "total_throughput": 14647.691188970068,
    "itl": 124.78087585297449,
    "ttft": 1664763.1931350904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21532151347491885,
    "arrivals": 375735,
    "finished_requests": 113137,
    "scheduler_time": 154.87898182205274
}
#Debug simulation 
Total elapsed time: 139.68942232895643. Arrivals time: 0.5413996246643364 Scheduler time: 138.95137732755393 Scheduler overhead time: 0.07623472064733505 Adapter cache time: 0.01462657144293189 Engine time: 0.07695996994152665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 138.8514193277806,
    "estimated_duration": 3600.0111107308435,
    "input_throughput": 7751.552465163039,
    "output_throughput": 6896.135660803559,
    "total_throughput": 14647.688125966599,
    "itl": 124.78088840927488,
    "ttft": 1664763.6530562998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21571113377809512,
    "arrivals": 375735,
    "finished_requests": 113137,
    "scheduler_time": 154.8790056910616
}
#Debug simulation 
Total elapsed time: 138.85157049866393. Arrivals time: 0.5508840149268508 Scheduler time: 138.10351047851145 Scheduler overhead time: 0.07689835038036108 Adapter cache time: 0.014866662211716175 Engine time: 0.076485357247293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 138.81065186066553,
    "estimated_duration": 3600.001497945898,
    "input_throughput": 7751.57316348966,
    "output_throughput": 6896.154074981748,
    "total_throughput": 14647.727238471409,
    "itl": 124.78100095998646,
    "ttft": 1664759.0922639617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20674101178767165,
    "arrivals": 375735,
    "finished_requests": 113137,
    "scheduler_time": 154.8787859807236
}
#Debug simulation 
Total elapsed time: 138.8108023707755. Arrivals time: 0.5360455103218555 Scheduler time: 138.07874265266582 Scheduler overhead time: 0.07669699285179377 Adapter cache time: 0.014284685719758272 Engine time: 0.07664711168035865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 138.4254001188092,
    "estimated_duration": 3600.120702704028,
    "input_throughput": 7751.424828350874,
    "output_throughput": 6896.039341501221,
    "total_throughput": 14647.464169852095,
    "itl": 124.78257176566272,
    "ttft": 1664771.310210569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21835196329280712,
    "arrivals": 375735,
    "finished_requests": 113138,
    "scheduler_time": 154.88297460485617
}
#Debug simulation 
Total elapsed time: 138.42555148387328. Arrivals time: 0.5373426792211831 Scheduler time: 137.69186873128638 Scheduler overhead time: 0.07704484183341265 Adapter cache time: 0.013632331974804401 Engine time: 0.07661131117492914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 137.2784263556823,
    "estimated_duration": 3600.1244042716576,
    "input_throughput": 7751.51657728499,
    "output_throughput": 6896.085027101365,
    "total_throughput": 14647.601604386355,
    "itl": 124.7817188410819,
    "ttft": 1664797.5554536178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19734331946354378,
    "arrivals": 375735,
    "finished_requests": 113139,
    "scheduler_time": 154.88404972917132
}
#Debug simulation 
Total elapsed time: 137.2785693667829. Arrivals time: 0.44855673564597964 Scheduler time: 136.6493191216141 Scheduler overhead time: 0.0708478675223887 Adapter cache time: 0.012474824208766222 Engine time: 0.07051743846386671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.15264256298542,
    "estimated_duration": 3600.121639490478,
    "input_throughput": 7751.422811355207,
    "output_throughput": 6896.037547085126,
    "total_throughput": 14647.460358440334,
    "itl": 124.78254878286864,
    "ttft": 1664771.9527188006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22124430038034884,
    "arrivals": 375735,
    "finished_requests": 113138,
    "scheduler_time": 154.88292509405326
}
#Debug simulation 
Total elapsed time: 137.1527872737497. Arrivals time: 0.46713085332885385 Scheduler time: 136.50443314295262 Scheduler overhead time: 0.07074261084198952 Adapter cache time: 0.013069339096546173 Engine time: 0.07095164526253939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 118.28063164791092,
    "estimated_duration": 3600.0013976728687,
    "input_throughput": 7881.880273252718,
    "output_throughput": 7000.271171086364,
    "total_throughput": 14882.151444339082,
    "itl": 122.90085589003814,
    "ttft": 1686635.522636045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28156487244181305,
    "arrivals": 375251,
    "finished_requests": 115138,
    "scheduler_time": 157.21682317841547
}
#Debug simulation 
Total elapsed time: 118.28078072704375. Arrivals time: 0.4524684678763151 Scheduler time: 117.65248485933989 Scheduler overhead time: 0.06823888653889298 Adapter cache time: 0.012373185716569424 Engine time: 0.06872875522822142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.56921503506601,
    "estimated_duration": 3600.1328181725107,
    "input_throughput": 7881.895039195826,
    "output_throughput": 7000.190068763289,
    "total_throughput": 14882.085107959114,
    "itl": 122.90213555762392,
    "ttft": 1686661.781498395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2983745602821,
    "arrivals": 375251,
    "finished_requests": 115142,
    "scheduler_time": 157.2220115759591
}
#Debug simulation 
Total elapsed time: 117.56936279404908. Arrivals time: 0.4562529567629099 Scheduler time: 116.9366301689297 Scheduler overhead time: 0.06842559156939387 Adapter cache time: 0.01270943321287632 Engine time: 0.06901502190157771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.87439737422392,
    "estimated_duration": 3600.134036308892,
    "input_throughput": 7881.892372288704,
    "output_throughput": 7000.187700188643,
    "total_throughput": 14882.080072477347,
    "itl": 122.90156425726471,
    "ttft": 1686661.4603489803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2992268412746491,
    "arrivals": 375251,
    "finished_requests": 115142,
    "scheduler_time": 157.22199469017968
}
#Debug simulation 
Total elapsed time: 117.87459951499477. Arrivals time: 0.45670110220089555 Scheduler time: 117.24230408435687 Scheduler overhead time: 0.06831596419215202 Adapter cache time: 0.012831457424908876 Engine time: 0.06822734465822577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 117.85244410298765,
    "estimated_duration": 3600.0516429427257,
    "input_throughput": 7880.333621217261,
    "output_throughput": 6998.942931664177,
    "total_throughput": 14879.276552881438,
    "itl": 122.9270707521563,
    "ttft": 1686689.0918677994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28693389136577035,
    "arrivals": 375251,
    "finished_requests": 115119,
    "scheduler_time": 157.18451423171504
}
#Debug simulation 
Total elapsed time: 117.85259592300281. Arrivals time: 0.45812532445415854 Scheduler time: 117.21807862399146 Scheduler overhead time: 0.06815000344067812 Adapter cache time: 0.012478584423661232 Engine time: 0.06894092541188002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 117.696481697727,
    "estimated_duration": 3600.138081673056,
    "input_throughput": 7881.883515649257,
    "output_throughput": 7000.179834293552,
    "total_throughput": 14882.06334994281,
    "itl": 122.90151885942241,
    "ttft": 1686663.3063319304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30274794729426496,
    "arrivals": 375251,
    "finished_requests": 115142,
    "scheduler_time": 157.22207551139664
}
#Debug simulation 
Total elapsed time: 117.69663408771157. Arrivals time: 0.459581577219069 Scheduler time: 117.05982651701197 Scheduler overhead time: 0.06937883887439966 Adapter cache time: 0.013188961893320084 Engine time: 0.06869975617155433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 117.66538440110162,
    "estimated_duration": 3600.1376420187285,
    "input_throughput": 7881.884478196955,
    "output_throughput": 7000.1806891662445,
    "total_throughput": 14882.065167363198,
    "itl": 122.90019014424365,
    "ttft": 1686669.2056008903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27508462713100046,
    "arrivals": 375251,
    "finished_requests": 115142,
    "scheduler_time": 157.22293977524788
}
#Debug simulation 
Total elapsed time: 117.66551671084017. Arrivals time: 0.45404978608712554 Scheduler time: 117.03592832572758 Scheduler overhead time: 0.06846059067174792 Adapter cache time: 0.013118463568389416 Engine time: 0.06801383616402745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 118.14260114496574,
    "estimated_duration": 3600.0034751134003,
    "input_throughput": 7881.875724885569,
    "output_throughput": 7000.267131466079,
    "total_throughput": 14882.14285635165,
    "itl": 122.9021587985767,
    "ttft": 1686630.6387370252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3063948071002957,
    "arrivals": 375251,
    "finished_requests": 115138,
    "scheduler_time": 157.2161627382039
}
#Debug simulation 
Total elapsed time: 118.14274161681533. Arrivals time: 0.4593950225971639 Scheduler time: 117.50490134721622 Scheduler overhead time: 0.0700444714166224 Adapter cache time: 0.013159309048205614 Engine time: 0.0689804651774466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 128.55255696130916,
    "estimated_duration": 3600.0213798542823,
    "input_throughput": 7895.532831849604,
    "output_throughput": 7009.903647022637,
    "total_throughput": 14905.436478872241,
    "itl": 122.81452087960987,
    "ttft": 1688319.6088861923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21423414207529257,
    "arrivals": 375015,
    "finished_requests": 115159,
    "scheduler_time": 157.41437304459464
}
#Debug simulation 
Total elapsed time: 128.552687976975. Arrivals time: 0.46422777231782675 Scheduler time: 127.91029288154095 Scheduler overhead time: 0.06971720047295094 Adapter cache time: 0.012204661034047604 Engine time: 0.06965821469202638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.94960854575038,
    "estimated_duration": 3600.0109945074737,
    "input_throughput": 7895.40227609463,
    "output_throughput": 7009.722758764095,
    "total_throughput": 14905.125034858724,
    "itl": 122.81587485352088,
    "ttft": 1688316.1172304621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22809890529140828,
    "arrivals": 375015,
    "finished_requests": 115157,
    "scheduler_time": 157.4128149281778
}
#Debug simulation 
Total elapsed time: 128.94974028877914. Arrivals time: 0.4682344035245478 Scheduler time: 128.30223728576675 Scheduler overhead time: 0.06992759509012103 Adapter cache time: 0.013521536253392696 Engine time: 0.0690529840067029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.9036633009091,
    "estimated_duration": 3600.0114621980892,
    "input_throughput": 7895.401250374131,
    "output_throughput": 7009.7218481054515,
    "total_throughput": 14905.123098479584,
    "itl": 122.81587476985099,
    "ttft": 1688316.3762601153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22855970416218033,
    "arrivals": 375015,
    "finished_requests": 115157,
    "scheduler_time": 157.41282181991804
}
#Debug simulation 
Total elapsed time: 128.90379595197737. Arrivals time: 0.4725493108853698 Scheduler time: 128.25082063023 Scheduler overhead time: 0.06920696701854467 Adapter cache time: 0.012963368091732264 Engine time: 0.07067099679261446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 128.90130274184048,
    "estimated_duration": 3600.0259698887858,
    "input_throughput": 7895.52276504219,
    "output_throughput": 7009.894709392777,
    "total_throughput": 14905.417474434968,
    "itl": 122.814440108033,
    "ttft": 1688320.6870155013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21910980828572074,
    "arrivals": 375015,
    "finished_requests": 115159,
    "scheduler_time": 157.41444796092676
}
#Debug simulation 
Total elapsed time: 128.90143114002421. Arrivals time: 0.47379390243440866 Scheduler time: 128.24808581871912 Scheduler overhead time: 0.06965819047763944 Adapter cache time: 0.013349303975701332 Engine time: 0.07010194100439548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 128.64599973335862,
    "estimated_duration": 3600.01721638655,
    "input_throughput": 7895.388630538159,
    "output_throughput": 7009.710643919986,
    "total_throughput": 14905.099274458145,
    "itl": 122.81595185664783,
    "ttft": 1688319.3261076224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2313262874633072,
    "arrivals": 375015,
    "finished_requests": 115157,
    "scheduler_time": 157.412981799306
}
#Debug simulation 
Total elapsed time: 128.64613792207092. Arrivals time: 0.47445714427158237 Scheduler time: 127.9925716072321 Scheduler overhead time: 0.06986803840845823 Adapter cache time: 0.013513719663023949 Engine time: 0.06942162103950977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 128.98612235020846,
    "estimated_duration": 3600.019023993026,
    "input_throughput": 7895.537998705604,
    "output_throughput": 7009.908234320732,
    "total_throughput": 14905.446233026336,
    "itl": 122.8146956808414,
    "ttft": 1688317.7481137936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2093035206431525,
    "arrivals": 375015,
    "finished_requests": 115159,
    "scheduler_time": 157.41428470447272
}
#Debug simulation 
Total elapsed time: 128.98625322431326. Arrivals time: 0.47123984480276704 Scheduler time: 128.33551996946335 Scheduler overhead time: 0.06906321598216891 Adapter cache time: 0.012939791195094585 Engine time: 0.0711907222867012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.55322656594217,
    "estimated_duration": 3600.0191716334375,
    "input_throughput": 7895.384342384872,
    "output_throughput": 7009.706836797228,
    "total_throughput": 14905.0911791821,
    "itl": 122.81579683678784,
    "ttft": 1688319.9522334363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23434437833726376,
    "arrivals": 375015,
    "finished_requests": 115157,
    "scheduler_time": 157.41303034655348
}
#Debug simulation 
Total elapsed time: 128.5533570642583. Arrivals time: 0.46874348167330027 Scheduler time: 127.9045247468166 Scheduler overhead time: 0.07067647483199835 Adapter cache time: 0.013064016122370958 Engine time: 0.07006214838474989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 134.9452472841367,
    "estimated_duration": 3600.1254578632643,
    "input_throughput": 7745.2681375580705,
    "output_throughput": 6844.908125681198,
    "total_throughput": 14590.176263239267,
    "itl": 125.28610192057118,
    "ttft": 1626691.8757044282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21423414207529257,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57458054433764
}
#Debug simulation 
Total elapsed time: 134.94538086513057. Arrivals time: 0.46971550071612 Scheduler time: 134.2930044257082 Scheduler overhead time: 0.07130943518131971 Adapter cache time: 0.013454409781843424 Engine time: 0.07127946242690086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.47580413194373,
    "estimated_duration": 3600.111061170331,
    "input_throughput": 7745.29911056006,
    "output_throughput": 6844.935498181314,
    "total_throughput": 14590.234608741373,
    "itl": 125.28689269046764,
    "ttft": 1626691.5542279042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22809890529140828,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57291227589664
}
#Debug simulation 
Total elapsed time: 135.47597826179117. Arrivals time: 0.47052905848249793 Scheduler time: 134.82389533147216 Scheduler overhead time: 0.07076203124597669 Adapter cache time: 0.013115637470036745 Engine time: 0.07108632568269968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 134.75479168444872,
    "estimated_duration": 3600.1118696828744,
    "input_throughput": 7745.297371122035,
    "output_throughput": 6844.933960946803,
    "total_throughput": 14590.231332068839,
    "itl": 125.28687633793203,
    "ttft": 1626692.075260116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22855970416218027,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57293945749586
}
#Debug simulation 
Total elapsed time: 134.7549266461283. Arrivals time: 0.4658854943700135 Scheduler time: 134.1092277066782 Scheduler overhead time: 0.0703584048897028 Adapter cache time: 0.012940903194248676 Engine time: 0.0703773875720799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 135.23982942756265,
    "estimated_duration": 3600.1305491294993,
    "input_throughput": 7745.257184282456,
    "output_throughput": 6844.898445685112,
    "total_throughput": 14590.155629967569,
    "itl": 125.28638855634871,
    "ttft": 1626690.799707135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21870121296728043,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.5748776743695
}
#Debug simulation 
Total elapsed time: 135.23996153799817. Arrivals time: 0.4721007915213704 Scheduler time: 134.58669102098793 Scheduler overhead time: 0.07198760332539678 Adapter cache time: 0.012981097679585218 Engine time: 0.06978705013170838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 135.03116926178336,
    "estimated_duration": 3600.116800932823,
    "input_throughput": 7745.28676202256,
    "output_throughput": 6844.924585117598,
    "total_throughput": 14590.211347140157,
    "itl": 125.28684181616245,
    "ttft": 1626694.582648379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.231452041249722,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57307851913137
}
#Debug simulation 
Total elapsed time: 135.0312959458679. Arrivals time: 0.47567880153656006 Scheduler time: 134.3744062287733 Scheduler overhead time: 0.07058002054691315 Adapter cache time: 0.01319462712854147 Engine time: 0.07082215789705515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 139.2632449339144,
    "estimated_duration": 3600.12116665209,
    "input_throughput": 7745.277369631004,
    "output_throughput": 6844.916284558323,
    "total_throughput": 14590.193654189328,
    "itl": 125.28616020788274,
    "ttft": 1626690.3986109404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2093035206431525,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57450702498025
}
#Debug simulation 
Total elapsed time: 139.26337321288884. Arrivals time: 0.4660187284462154 Scheduler time: 138.6130506498739 Scheduler overhead time: 0.0716815902851522 Adapter cache time: 0.013334669638425112 Engine time: 0.072580655105412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 134.60974458884448,
    "estimated_duration": 3600.11824871253,
    "input_throughput": 7745.283647272369,
    "output_throughput": 6844.921832446095,
    "total_throughput": 14590.205479718465,
    "itl": 125.28688944408559,
    "ttft": 1626691.7945641845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23434437833726374,
    "arrivals": 344340,
    "finished_requests": 112897,
    "scheduler_time": 153.57328433838063
}
#Debug simulation 
Total elapsed time: 134.60987228667364. Arrivals time: 0.4715619869530201 Scheduler time: 133.95901283621788 Scheduler overhead time: 0.06977092241868377 Adapter cache time: 0.012679422739893198 Engine time: 0.07053229818120599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 100.9739000191912,
    "estimated_duration": 3600.134121123533,
    "input_throughput": 7757.7834770455365,
    "output_throughput": 6848.833451878484,
    "total_throughput": 14606.61692892402,
    "itl": 125.84259205638018,
    "ttft": 1539465.4338784383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11935902201337734,
    "arrivals": 321710,
    "finished_requests": 112505,
    "scheduler_time": 152.72722207015184
}
#Debug simulation 
Total elapsed time: 100.97405030904338. Arrivals time: 0.4607542813755572 Scheduler time: 100.33112084539607 Scheduler overhead time: 0.07087890012189746 Adapter cache time: 0.012895043473690748 Engine time: 0.07187868840992451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 101.27910166792572,
    "estimated_duration": 3600.0133867088193,
    "input_throughput": 7757.876985433261,
    "output_throughput": 6848.81981023429,
    "total_throughput": 14606.69679566755,
    "itl": 125.84170879330827,
    "ttft": 1539412.5289710732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12641824914375321,
    "arrivals": 321710,
    "finished_requests": 112502,
    "scheduler_time": 152.72190047278943
}
#Debug simulation 
Total elapsed time: 101.27924360102043. Arrivals time: 0.4627988007850945 Scheduler time: 100.63657540176064 Scheduler overhead time: 0.07055296981707215 Adapter cache time: 0.011858447920531034 Engine time: 0.07119730953127146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 101.09841381385922,
    "estimated_duration": 3600.035548043862,
    "input_throughput": 7757.995616230987,
    "output_throughput": 6848.961814668056,
    "total_throughput": 14606.957430899043,
    "itl": 125.84238876420528,
    "ttft": 1539387.6061245464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12679117497056724,
    "arrivals": 321710,
    "finished_requests": 112504,
    "scheduler_time": 152.72299066130833
}
#Debug simulation 
Total elapsed time: 101.09856007387862. Arrivals time: 0.4683777787722647 Scheduler time: 100.44701162911952 Scheduler overhead time: 0.0715390220284462 Adapter cache time: 0.01264310535043478 Engine time: 0.07207467313855886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 101.12673658272251,
    "estimated_duration": 3600.1416467217605,
    "input_throughput": 7757.767260472048,
    "output_throughput": 6848.8191353393195,
    "total_throughput": 14606.586395811368,
    "itl": 125.84236564081696,
    "ttft": 1539466.9360586558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12110651000402872,
    "arrivals": 321710,
    "finished_requests": 112505,
    "scheduler_time": 152.72752670071662
}
#Debug simulation 
Total elapsed time: 101.12688487768173. Arrivals time: 0.46332960901781917 Scheduler time: 100.4825040078722 Scheduler overhead time: 0.07107862224802375 Adapter cache time: 0.011835727375000715 Engine time: 0.07116340892389417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 100.70509811071679,
    "estimated_duration": 3600.040875222252,
    "input_throughput": 7757.984136298389,
    "output_throughput": 6848.951679882748,
    "total_throughput": 14606.935816181136,
    "itl": 125.84241687284367,
    "ttft": 1539389.6139184467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1284259741939604,
    "arrivals": 321710,
    "finished_requests": 112504,
    "scheduler_time": 152.7231809357867
}
#Debug simulation 
Total elapsed time: 100.70523842284456. Arrivals time: 0.4578575389459729 Scheduler time: 100.06837460771203 Scheduler overhead time: 0.0698713450692594 Adapter cache time: 0.012410589959472418 Engine time: 0.07027274090796709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 100.75417944090441,
    "estimated_duration": 3600.1327349660824,
    "input_throughput": 7757.78646402134,
    "output_throughput": 6848.836088881678,
    "total_throughput": 14606.622552903018,
    "itl": 125.84266543914535,
    "ttft": 1539464.344829748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11661196150118491,
    "arrivals": 321710,
    "finished_requests": 112505,
    "scheduler_time": 152.72719957993058
}
#Debug simulation 
Total elapsed time: 100.75432704482228. Arrivals time: 0.45984793780371547 Scheduler time: 100.11379134422168 Scheduler overhead time: 0.07024009199813008 Adapter cache time: 0.01232825918123126 Engine time: 0.07143962057307363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 100.92673309659585,
    "estimated_duration": 3600.0437838795183,
    "input_throughput": 7757.977868231031,
    "output_throughput": 6848.94614626864,
    "total_throughput": 14606.924014499671,
    "itl": 125.84231416287733,
    "ttft": 1539391.7232336695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 39,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12980926584452385,
    "arrivals": 321710,
    "finished_requests": 112504,
    "scheduler_time": 152.7232939863863
}
#Debug simulation 
Total elapsed time: 100.92687848489732. Arrivals time: 0.4515716047026217 Scheduler time: 100.29420616896823 Scheduler overhead time: 0.07051893323659897 Adapter cache time: 0.012339647859334946 Engine time: 0.07163081038743258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.40814268914983,
    "estimated_duration": 3600.0263904102835,
    "input_throughput": 7677.924826781582,
    "output_throughput": 6856.401682429592,
    "total_throughput": 14534.326509211174,
    "itl": 126.52030082785495,
    "ttft": 1529698.6513261958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10405658329371359,
    "arrivals": 317887,
    "finished_requests": 112093,
    "scheduler_time": 152.44895874905856
}
#Debug simulation 
Total elapsed time: 112.40835497714579. Arrivals time: 0.4784084144048393 Scheduler time: 111.74678011797369 Scheduler overhead time: 0.07167357672005892 Adapter cache time: 0.01307367393746972 Engine time: 0.07095113443210721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.29389940621331,
    "estimated_duration": 3600.0762056723806,
    "input_throughput": 7677.81858518675,
    "output_throughput": 6856.306808480448,
    "total_throughput": 14534.125393667198,
    "itl": 126.52072642326864,
    "ttft": 1529713.2646123713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11065080703236162,
    "arrivals": 317887,
    "finished_requests": 112093,
    "scheduler_time": 152.4511877905018
}
#Debug simulation 
Total elapsed time: 112.29404297983274. Arrivals time: 0.4580924641340971 Scheduler time: 111.65436673210934 Scheduler overhead time: 0.07088263612240553 Adapter cache time: 0.012079683132469654 Engine time: 0.07172603625804186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.49519240995869,
    "estimated_duration": 3600.0767003578762,
    "input_throughput": 7677.817530179924,
    "output_throughput": 6856.305866357317,
    "total_throughput": 14534.123396537241,
    "itl": 126.52072553240524,
    "ttft": 1529713.6585868853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11089908573776483,
    "arrivals": 317887,
    "finished_requests": 112093,
    "scheduler_time": 152.4512079872291
}
#Debug simulation 
Total elapsed time: 112.49534299969673. Arrivals time: 0.4552672142162919 Scheduler time: 111.85740666044876 Scheduler overhead time: 0.07137567643076181 Adapter cache time: 0.013360199984163046 Engine time: 0.07111951755359769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 112.72727030562237,
    "estimated_duration": 3600.0281578581416,
    "input_throughput": 7677.921057274458,
    "output_throughput": 6856.398316252458,
    "total_throughput": 14534.319373526916,
    "itl": 126.52010430725156,
    "ttft": 1529699.1488618776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10574766321107745,
    "arrivals": 317887,
    "finished_requests": 112093,
    "scheduler_time": 152.44903511699087
}
#Debug simulation 
Total elapsed time: 112.72741482360289. Arrivals time: 0.6563808410428464 Scheduler time: 111.88623489812016 Scheduler overhead time: 0.07263028621673584 Adapter cache time: 0.012865654658526182 Engine time: 0.07194625632837415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 112.58195879403502,
    "estimated_duration": 3600.080397518984,
    "input_throughput": 7677.817700696015,
    "output_throughput": 6856.299380705662,
    "total_throughput": 14534.117081401677,
    "itl": 126.52006608287455,
    "ttft": 1529717.8663853244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11240813117474316,
    "arrivals": 317887,
    "finished_requests": 112094,
    "scheduler_time": 152.45138069669457
}
#Debug simulation 
Total elapsed time: 112.58209651196375. Arrivals time: 0.46866279700770974 Scheduler time: 111.92857221141458 Scheduler overhead time: 0.07283322745934129 Adapter cache time: 0.01308730011805892 Engine time: 0.07189427642151713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.75257048290223,
    "estimated_duration": 3600.012708603715,
    "input_throughput": 7677.9351178236775,
    "output_throughput": 6856.351073708687,
    "total_throughput": 14534.286191532365,
    "itl": 126.52034505992671,
    "ttft": 1529708.5384701786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10166171002667401,
    "arrivals": 317887,
    "finished_requests": 112092,
    "scheduler_time": 152.44838079299265
}
#Debug simulation 
Total elapsed time: 112.75272637512535. Arrivals time: 0.682153772097081 Scheduler time: 111.8885844303295 Scheduler overhead time: 0.07120995409786701 Adapter cache time: 0.012834096793085337 Engine time: 0.07097917096689343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.68972838297486,
    "estimated_duration": 3600.0803922835203,
    "input_throughput": 7677.817711861581,
    "output_throughput": 6856.299390676523,
    "total_throughput": 14534.117102538105,
    "itl": 126.5199846769561,
    "ttft": 1529717.2235515346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11366566903889173,
    "arrivals": 317887,
    "finished_requests": 112094,
    "scheduler_time": 152.45137929643496
}
#Debug simulation 
Total elapsed time: 112.68987920926884. Arrivals time: 0.4572683968581259 Scheduler time: 112.04772760346532 Scheduler overhead time: 0.07200026698410511 Adapter cache time: 0.013107359874993563 Engine time: 0.0721411774866283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.75517945596948,
    "estimated_duration": 3600.0766298193726,
    "input_throughput": 7725.388890234655,
    "output_throughput": 6844.935131627013,
    "total_throughput": 14570.324021861668,
    "itl": 125.95153078497617,
    "ttft": 1531213.712170382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11017755878157909,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.58164776999294
}
#Debug simulation 
Total elapsed time: 104.75532999495044. Arrivals time: 0.45240854285657406 Scheduler time: 104.1187373707071 Scheduler overhead time: 0.072045112028718 Adapter cache time: 0.01304583065211773 Engine time: 0.07225696370005608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.52312830695882,
    "estimated_duration": 3600.1152370380496,
    "input_throughput": 7725.3060440592935,
    "output_throughput": 6844.861727335745,
    "total_throughput": 14570.16777139504,
    "itl": 125.9516047744951,
    "ttft": 1531231.7769462494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11785669357748699,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.5832542275601
}
#Debug simulation 
Total elapsed time: 104.523272652179. Arrivals time: 0.46167816733941436 Scheduler time: 103.8799112951383 Scheduler overhead time: 0.0709391632117331 Adapter cache time: 0.012379225343465805 Engine time: 0.07177582895383239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.34745551692322,
    "estimated_duration": 3600.115383053236,
    "input_throughput": 7725.305730732668,
    "output_throughput": 6844.861449718598,
    "total_throughput": 14570.167180451266,
    "itl": 125.95160357376015,
    "ttft": 1531231.9141675541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11799786591902374,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.58325907040458
}
#Debug simulation 
Total elapsed time: 105.34760073106736. Arrivals time: 0.45151381148025393 Scheduler time: 104.71371235558763 Scheduler overhead time: 0.0714700324460864 Adapter cache time: 0.012208316009491682 Engine time: 0.07178138988092542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.63634446961805,
    "estimated_duration": 3600.1002075132164,
    "input_throughput": 7725.338295294631,
    "output_throughput": 6844.890302934585,
    "total_throughput": 14570.228598229216,
    "itl": 125.95180164900171,
    "ttft": 1531226.9249256037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11213635911932218,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.58269849473115
}
#Debug simulation 
Total elapsed time: 104.63648673193529. Arrivals time: 0.45252577774226665 Scheduler time: 104.00081712100655 Scheduler overhead time: 0.0714313518255949 Adapter cache time: 0.013128950726240873 Engine time: 0.07184204598888755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.54572647577152,
    "estimated_duration": 3600.1177849910946,
    "input_throughput": 7725.300576538997,
    "output_throughput": 6844.856882942499,
    "total_throughput": 14570.157459481497,
    "itl": 125.9514612617929,
    "ttft": 1531232.9786017914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11975841892883174,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.58333983000054
}
#Debug simulation 
Total elapsed time: 104.54586602607742. Arrivals time: 0.4831175971776247 Scheduler time: 103.87599648116156 Scheduler overhead time: 0.0733167752623558 Adapter cache time: 0.013247438706457615 Engine time: 0.07308962987735868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.41930132778361,
    "estimated_duration": 3600.065322988351,
    "input_throughput": 7725.413153590712,
    "output_throughput": 6844.956629716059,
    "total_throughput": 14570.36978330677,
    "itl": 125.95153129991223,
    "ttft": 1531211.569778597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10764181061647837,
    "arrivals": 315978,
    "finished_requests": 112393,
    "scheduler_time": 152.58120229217207
}
#Debug simulation 
Total elapsed time: 104.41944415681064. Arrivals time: 0.4488178472965956 Scheduler time: 103.78644739463925 Scheduler overhead time: 0.07284025102853775 Adapter cache time: 0.012884740252047777 Engine time: 0.0718131479807198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.62086125276983,
    "estimated_duration": 3600.1327818866866,
    "input_throughput": 7725.38394692893,
    "output_throughput": 6844.892811727303,
    "total_throughput": 14570.276758656233,
    "itl": 125.95169966433991,
    "ttft": 1531218.5509060838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 36,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1211417105793952,
    "arrivals": 315978,
    "finished_requests": 112394,
    "scheduler_time": 152.58406762694128
}
#Debug simulation 
Total elapsed time: 104.6210545967333. Arrivals time: 0.4823206081055105 Scheduler time: 103.95533145405352 Scheduler overhead time: 0.07152225682511926 Adapter cache time: 0.012837741523981094 Engine time: 0.07183843711391091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 129.18743512500077,
    "estimated_duration": 3600.0163106890896,
    "input_throughput": 7768.481747419311,
    "output_throughput": 6843.263161572837,
    "total_throughput": 14611.744908992148,
    "itl": 125.62652997849061,
    "ttft": 1505732.561008062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10711707103764634,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57176691584777
}
#Debug simulation 
Total elapsed time: 129.18757504411042. Arrivals time: 0.47563242726027966 Scheduler time: 128.52534168120474 Scheduler overhead time: 0.07330260844901204 Adapter cache time: 0.012772094924002886 Engine time: 0.0732224490493536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.8257216992788,
    "estimated_duration": 3600.0417807392964,
    "input_throughput": 7768.426785940476,
    "output_throughput": 6843.21474595243,
    "total_throughput": 14611.641531892907,
    "itl": 125.6262412445478,
    "ttft": 1505734.7286407675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11364085732726377,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57282441559016
}
#Debug simulation 
Total elapsed time: 128.82586298231035. Arrivals time: 0.46611067233607173 Scheduler time: 128.1740119443275 Scheduler overhead time: 0.07283249823376536 Adapter cache time: 0.012530045118182898 Engine time: 0.07335439277812839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 129.02779461815953,
    "estimated_duration": 3600.0429215342806,
    "input_throughput": 7768.424324252517,
    "output_throughput": 6843.212577449102,
    "total_throughput": 14611.636901701619,
    "itl": 125.62623057164669,
    "ttft": 1505735.2359657052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11394260458648203,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57287172809825
}
#Debug simulation 
Total elapsed time: 129.02794422814623. Arrivals time: 0.4776693577878177 Scheduler time: 128.36532713659108 Scheduler overhead time: 0.07203322136774659 Adapter cache time: 0.01331457868218422 Engine time: 0.07247808203101158 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 128.94818132929504,
    "estimated_duration": 3600.0227619215234,
    "input_throughput": 7768.46782631805,
    "output_throughput": 6843.250898461135,
    "total_throughput": 14611.718724779184,
    "itl": 125.62652740688382,
    "ttft": 1505734.8118570365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10914630882441997,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57200402434086
}
#Debug simulation 
Total elapsed time: 128.94833133229986. Arrivals time: 0.4733205293305218 Scheduler time: 128.28882839670405 Scheduler overhead time: 0.07307745283469558 Adapter cache time: 0.013364471029490232 Engine time: 0.07280875742435455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 128.45516373915598,
    "estimated_duration": 3600.047126558744,
    "input_throughput": 7768.415250367321,
    "output_throughput": 6843.204584254768,
    "total_throughput": 14611.619834622088,
    "itl": 125.6261618508362,
    "ttft": 1505736.2579825069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11532589623704548,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57301739088686
}
#Debug simulation 
Total elapsed time: 128.45530327316374. Arrivals time: 0.47138320142403245 Scheduler time: 127.79754283698276 Scheduler overhead time: 0.07267641788348556 Adapter cache time: 0.013634877745062113 Engine time: 0.07230819761753082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 128.94174341484904,
    "estimated_duration": 3600.00828193054,
    "input_throughput": 7768.499072730633,
    "output_throughput": 6843.278423456509,
    "total_throughput": 14611.777496187142,
    "itl": 125.62684215976236,
    "ttft": 1505730.1354467268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10465176032157619,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57150184383963
}
#Debug simulation 
Total elapsed time: 128.94189113704488. Arrivals time: 0.4664824539795518 Scheduler time: 128.28848732449114 Scheduler overhead time: 0.07219295809045434 Adapter cache time: 0.013159552589058876 Engine time: 0.07402879232540727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 128.6266215001233,
    "estimated_duration": 3600.0494832976797,
    "input_throughput": 7768.410164846477,
    "output_throughput": 6843.200104414486,
    "total_throughput": 14611.610269260964,
    "itl": 125.6260647040747,
    "ttft": 1505752.0757269412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 35,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11670918788760892,
    "arrivals": 314998,
    "finished_requests": 112590,
    "scheduler_time": 152.57310811556718
}
#Debug simulation 
Total elapsed time: 128.62676684511825. Arrivals time: 0.4673217865638435 Scheduler time: 127.97278896393254 Scheduler overhead time: 0.07297511165961623 Adapter cache time: 0.01328713959082961 Engine time: 0.07291573192924261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.01696954062209,
    "estimated_duration": 3600.12927598109,
    "input_throughput": 7753.085753397988,
    "output_throughput": 6846.609693837414,
    "total_throughput": 14599.695447235401,
    "itl": 125.73666317582286,
    "ttft": 1540846.4176723785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10405658329371359,
    "arrivals": 314533,
    "finished_requests": 112450,
    "scheduler_time": 152.52163166529706
}
#Debug simulation 
Total elapsed time: 82.01711895968765. Arrivals time: 0.4366735741496086 Scheduler time: 81.40270675579086 Scheduler overhead time: 0.06952406698837876 Adapter cache time: 0.012875196058303118 Engine time: 0.06947136204689741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.2174918660894,
    "estimated_duration": 3600.042661947314,
    "input_throughput": 7753.242842100654,
    "output_throughput": 6846.661918908316,
    "total_throughput": 14599.904761008971,
    "itl": 125.7365711011439,
    "ttft": 1540810.1479529648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1114679976692423,
    "arrivals": 314533,
    "finished_requests": 112449,
    "scheduler_time": 152.51801618455076
}
#Debug simulation 
Total elapsed time: 82.21762367011979. Arrivals time: 0.45337883615866303 Scheduler time: 81.58515098784119 Scheduler overhead time: 0.0696096708998084 Adapter cache time: 0.012635461520403624 Engine time: 0.07019318081438541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.42300969967619,
    "estimated_duration": 3600.047941959179,
    "input_throughput": 7753.231470803701,
    "output_throughput": 6846.651877248663,
    "total_throughput": 14599.883348052364,
    "itl": 125.73660634700262,
    "ttft": 1540809.8125866791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11157358072698115,
    "arrivals": 314533,
    "finished_requests": 112449,
    "scheduler_time": 152.5182352929704
}
#Debug simulation 
Total elapsed time: 82.42315017059445. Arrivals time: 0.4558737901970744 Scheduler time: 81.78962994366884 Scheduler overhead time: 0.06918257474899292 Adapter cache time: 0.012469623237848282 Engine time: 0.0695470655336976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 82.53513932228088,
    "estimated_duration": 3600.1344245686514,
    "input_throughput": 7753.074665633986,
    "output_throughput": 6846.599902433719,
    "total_throughput": 14599.674568067705,
    "itl": 125.73635693881575,
    "ttft": 1540846.3429186558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10615625852951782,
    "arrivals": 314533,
    "finished_requests": 112450,
    "scheduler_time": 152.5218404311797
}
#Debug simulation 
Total elapsed time: 82.53528519906104. Arrivals time: 0.4427457023411989 Scheduler time: 81.9128473722376 Scheduler overhead time: 0.07044508261606097 Adapter cache time: 0.013021472841501236 Engine time: 0.06994422432035208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 82.12787612015381,
    "estimated_duration": 3600.049522181923,
    "input_throughput": 7753.228067563652,
    "output_throughput": 6846.648871946945,
    "total_throughput": 14599.876939510597,
    "itl": 125.73643087522723,
    "ttft": 1540810.3599483287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11320837995037432,
    "arrivals": 314533,
    "finished_requests": 112449,
    "scheduler_time": 152.5183019880493
}
#Debug simulation 
Total elapsed time: 82.12800984084606. Arrivals time: 0.4561166800558567 Scheduler time: 81.49081310350448 Scheduler overhead time: 0.07063067704439163 Adapter cache time: 0.013162228278815746 Engine time: 0.07101972913369536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.33272904530168,
    "estimated_duration": 3600.0946070589516,
    "input_throughput": 7753.160415637638,
    "output_throughput": 6846.675626709823,
    "total_throughput": 14599.83604234746,
    "itl": 125.73627572873397,
    "ttft": 1540817.1847712055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10166171002667401,
    "arrivals": 314533,
    "finished_requests": 112450,
    "scheduler_time": 152.5201362028648
}
#Debug simulation 
Total elapsed time: 82.33293462311849. Arrivals time: 0.4572130744345486 Scheduler time: 81.69808009080589 Scheduler overhead time: 0.06925299065187573 Adapter cache time: 0.012689079158008099 Engine time: 0.06940784817561507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.23076587170362,
    "estimated_duration": 3600.055128326202,
    "input_throughput": 7753.215993938769,
    "output_throughput": 6846.6382100820465,
    "total_throughput": 14599.854204020816,
    "itl": 125.73648282869534,
    "ttft": 1540814.066039575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 34,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11459167160093778,
    "arrivals": 314533,
    "finished_requests": 112449,
    "scheduler_time": 152.51851845982782
}
#Debug simulation 
Total elapsed time: 82.23090833192691. Arrivals time: 0.46134364465251565 Scheduler time: 81.59105737321079 Scheduler overhead time: 0.0699868518859148 Adapter cache time: 0.012172100599855185 Engine time: 0.07026006374508142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.85711932415143,
    "estimated_duration": 3600.0720428469303,
    "input_throughput": 7742.048955764487,
    "output_throughput": 6849.21182313362,
    "total_throughput": 14591.260778898108,
    "itl": 125.88651532590292,
    "ttft": 1543132.8588285272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10099609554978084,
    "arrivals": 314275,
    "finished_requests": 112238,
    "scheduler_time": 152.48701768900798
}
#Debug simulation 
Total elapsed time: 82.8572615343146. Arrivals time: 0.4542962866835296 Scheduler time: 82.22321635205299 Scheduler overhead time: 0.07043595518916845 Adapter cache time: 0.012863673269748688 Engine time: 0.07023632200434804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.52253754995763,
    "estimated_duration": 3600.135437520265,
    "input_throughput": 7742.181782804972,
    "output_throughput": 6849.199267065408,
    "total_throughput": 14591.381049870379,
    "itl": 125.88679110423162,
    "ttft": 1543131.5520280467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1072521614190191,
    "arrivals": 314275,
    "finished_requests": 112241,
    "scheduler_time": 152.4899074639864
}
#Debug simulation 
Total elapsed time: 82.52267889818177. Arrivals time: 0.47213575011119246 Scheduler time: 81.8743736972101 Scheduler overhead time: 0.06924508698284626 Adapter cache time: 0.012155146803706884 Engine time: 0.068987968377769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 85.9708644989878,
    "estimated_duration": 3600.134641972298,
    "input_throughput": 7742.183493651256,
    "output_throughput": 6849.200780582843,
    "total_throughput": 14591.384274234098,
    "itl": 125.88673000128975,
    "ttft": 1543130.319799224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10751831939443945,
    "arrivals": 314275,
    "finished_requests": 112241,
    "scheduler_time": 152.48986370331514
}
#Debug simulation 
Total elapsed time: 85.97100496804342. Arrivals time: 0.4840285750105977 Scheduler time: 85.30664861015975 Scheduler overhead time: 0.07076239818707108 Adapter cache time: 0.012909230776131153 Engine time: 0.07022017939016223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 82.80528301233426,
    "estimated_duration": 3600.0802304242357,
    "input_throughput": 7742.031348205691,
    "output_throughput": 6849.196246133194,
    "total_throughput": 14591.227594338885,
    "itl": 125.8865258203635,
    "ttft": 1543136.9990960988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10316620823461564,
    "arrivals": 314275,
    "finished_requests": 112238,
    "scheduler_time": 152.48733138065222
}
#Debug simulation 
Total elapsed time: 82.80542299896479. Arrivals time: 0.45938950637355447 Scheduler time: 82.16775403963402 Scheduler overhead time: 0.06916027376428246 Adapter cache time: 0.01261291466653347 Engine time: 0.07017832528799772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 82.78371046530083,
    "estimated_duration": 3600.0003931574183,
    "input_throughput": 7742.154432254042,
    "output_throughput": 6849.272029766081,
    "total_throughput": 14591.426462020123,
    "itl": 125.8866584069949,
    "ttft": 1543127.0416304306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10877585725858806,
    "arrivals": 314275,
    "finished_requests": 112237,
    "scheduler_time": 152.4840067320859
}
#Debug simulation 
Total elapsed time: 82.78385438211262. Arrivals time: 0.4870963543653488 Scheduler time: 82.11480722157285 Scheduler overhead time: 0.0713575417175889 Adapter cache time: 0.012953819707036018 Engine time: 0.07143219094723463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 82.65915538696572,
    "estimated_duration": 3600.0699400386975,
    "input_throughput": 7742.053477911155,
    "output_throughput": 6849.215823772288,
    "total_throughput": 14591.269301683444,
    "itl": 125.88665963925635,
    "ttft": 1543132.2165332958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.09867165973177183,
    "arrivals": 314275,
    "finished_requests": 112238,
    "scheduler_time": 152.48695668943503
}
#Debug simulation 
Total elapsed time: 82.65929805720225. Arrivals time: 0.44928741781041026 Scheduler time: 82.0322526814416 Scheduler overhead time: 0.06931752758100629 Adapter cache time: 0.01284527126699686 Engine time: 0.0698590581305325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.71257342817262,
    "estimated_duration": 3600.0126568034634,
    "input_throughput": 7742.12805816855,
    "output_throughput": 6849.248697334824,
    "total_throughput": 14591.376755503374,
    "itl": 125.88681533037649,
    "ttft": 1543127.1423951748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 33,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11015914890915152,
    "arrivals": 314275,
    "finished_requests": 112237,
    "scheduler_time": 152.48459534530488
}
#Debug simulation 
Total elapsed time: 82.71271774219349. Arrivals time: 0.4836705164052546 Scheduler time: 82.05025830492377 Scheduler overhead time: 0.06843454670161009 Adapter cache time: 0.013017345685511827 Engine time: 0.07107686577364802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 33.80143159488216,
    "estimated_duration": 3600.0382858832922,
    "input_throughput": 7703.073355842363,
    "output_throughput": 6851.422413122529,
    "total_throughput": 14554.495768964893,
    "itl": 126.24045883777316,
    "ttft": 1563437.2511746448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2723834092100148,
    "arrivals": 291201,
    "finished_requests": 112152,
    "scheduler_time": 151.9186171346746
}
#Debug simulation 
Total elapsed time: 33.80157070886344. Arrivals time: 0.40947627602145076 Scheduler time: 33.241934224031866 Scheduler overhead time: 0.057713262271136045 Adapter cache time: 0.01019940571859479 Engine time: 0.05880329106003046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 32.14422333985567,
    "estimated_duration": 3600.1154439898614,
    "input_throughput": 7701.0349893874345,
    "output_throughput": 6849.375355772464,
    "total_throughput": 14550.410345159898,
    "itl": 126.24150055428832,
    "ttft": 1562654.2399682198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29022160003427416,
    "arrivals": 291201,
    "finished_requests": 112142,
    "scheduler_time": 151.91888081371027
}
#Debug simulation 
Total elapsed time: 32.144321088213474. Arrivals time: 0.3727802666835487 Scheduler time: 31.62404505070299 Scheduler overhead time: 0.055908767972141504 Adapter cache time: 0.009985273238271475 Engine time: 0.05839271517470479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 31.950271456036717,
    "estimated_duration": 3600.121436422333,
    "input_throughput": 7701.022170949793,
    "output_throughput": 6849.36395492946,
    "total_throughput": 14550.386125879253,
    "itl": 126.24153139362535,
    "ttft": 1562652.5676277215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29077077971771376,
    "arrivals": 291201,
    "finished_requests": 112142,
    "scheduler_time": 151.91916621901123
}
#Debug simulation 
Total elapsed time: 31.950392405036837. Arrivals time: 0.36044360790401697 Scheduler time: 31.441196302417666 Scheduler overhead time: 0.05608262354508042 Adapter cache time: 0.01007414935156703 Engine time: 0.05938184866681695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.811723933089525,
    "estimated_duration": 3600.0698304385724,
    "input_throughput": 7703.00585992291,
    "output_throughput": 6851.3623795445055,
    "total_throughput": 14554.368239467414,
    "itl": 126.24066851555024,
    "ttft": 1563437.7360955141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27755514516262353,
    "arrivals": 291201,
    "finished_requests": 112152,
    "scheduler_time": 151.91985539292577
}
#Debug simulation 
Total elapsed time: 34.81186115927994. Arrivals time: 0.3863756600767374 Scheduler time: 34.2730209659785 Scheduler overhead time: 0.05837736092507839 Adapter cache time: 0.01041549863293767 Engine time: 0.06059146486222744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 32.750532265752554,
    "estimated_duration": 3600.0094367390766,
    "input_throughput": 7701.130924010243,
    "output_throughput": 6849.349545687637,
    "total_throughput": 14550.48046969788,
    "itl": 126.24175982940075,
    "ttft": 1562613.6959282793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.294794900882989,
    "arrivals": 291201,
    "finished_requests": 112140,
    "scheduler_time": 151.91431225852855
}
#Debug simulation 
Total elapsed time: 32.75071840872988. Arrivals time: 0.38269779086112976 Scheduler time: 32.21695435931906 Scheduler overhead time: 0.05778587702661753 Adapter cache time: 0.009988098870962858 Engine time: 0.05990420002490282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 34.06861085211858,
    "estimated_duration": 3600.0200811560876,
    "input_throughput": 7702.897588030891,
    "output_throughput": 6851.335949237054,
    "total_throughput": 14554.233537267944,
    "itl": 126.2404767007217,
    "ttft": 1563429.020945393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2661144762462939,
    "arrivals": 291201,
    "finished_requests": 112150,
    "scheduler_time": 151.91792630727667
}
#Debug simulation 
Total elapsed time: 34.06872558314353. Arrivals time: 0.3991435281932354 Scheduler time: 33.51930749323219 Scheduler overhead time: 0.057043887209147215 Adapter cache time: 0.01020042784512043 Engine time: 0.0595492166467011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 33.83752022404224,
    "estimated_duration": 3600.013506503279,
    "input_throughput": 7702.684989905535,
    "output_throughput": 6851.167351301584,
    "total_throughput": 14553.852341207119,
    "itl": 126.24185494077284,
    "ttft": 1563452.43671871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2981902531161902,
    "arrivals": 291201,
    "finished_requests": 112148,
    "scheduler_time": 151.9169593739921
}
#Debug simulation 
Total elapsed time: 33.837623494677246. Arrivals time: 0.36608800245448947 Scheduler time: 33.31934684328735 Scheduler overhead time: 0.05797273479402065 Adapter cache time: 0.010250381659716368 Engine time: 0.06040685484185815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.737835162784904,
    "estimated_duration": 3600.104924100356,
    "input_throughput": 7743.003770082048,
    "output_throughput": 6846.146853944568,
    "total_throughput": 14589.150624026615,
    "itl": 125.75884663931225,
    "ttft": 1551202.8016826697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28156487244181305,
    "arrivals": 287382,
    "finished_requests": 112544,
    "scheduler_time": 151.88865432303749
}
#Debug simulation 
Total elapsed time: 25.737965889740735. Arrivals time: 0.3735119542106986 Scheduler time: 25.224142085760832 Scheduler overhead time: 0.053326071705669165 Adapter cache time: 0.0090869409032166 Engine time: 0.05526612140238285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.669495230074972,
    "estimated_duration": 3600.019750772398,
    "input_throughput": 7742.895019956322,
    "output_throughput": 6846.1418842805115,
    "total_throughput": 14589.036904236833,
    "itl": 125.75896289874011,
    "ttft": 1551203.140910895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.300826132192742,
    "arrivals": 287382,
    "finished_requests": 112540,
    "scheduler_time": 151.88455334190192
}
#Debug simulation 
Total elapsed time: 25.669602571986616. Arrivals time: 0.3504242985509336 Scheduler time: 25.17871639179066 Scheduler overhead time: 0.053569094743579626 Adapter cache time: 0.009015266317874193 Engine time: 0.055263666436076164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.83222500188276,
    "estimated_duration": 3600.0204209075837,
    "input_throughput": 7742.893578634945,
    "output_throughput": 6846.140609887583,
    "total_throughput": 14589.034188522528,
    "itl": 125.75896270143258,
    "ttft": 1551203.5679874336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3012503262422981,
    "arrivals": 287382,
    "finished_requests": 112540,
    "scheduler_time": 151.8845730729711
}
#Debug simulation 
Total elapsed time: 25.832315971609205. Arrivals time: 0.3790168729610741 Scheduler time: 25.312439726199955 Scheduler overhead time: 0.053194222040474415 Adapter cache time: 0.009265851229429245 Engine time: 0.055756038054823875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 26.621956676244736,
    "estimated_duration": 3600.1071829515745,
    "input_throughput": 7742.998911811832,
    "output_throughput": 6846.142558398247,
    "total_throughput": 14589.14147021008,
    "itl": 125.75868844769563,
    "ttft": 1551203.3404185656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28611670072888973,
    "arrivals": 287382,
    "finished_requests": 112544,
    "scheduler_time": 151.88864141296267
}
#Debug simulation 
Total elapsed time: 26.622067399322987. Arrivals time: 0.3444246780127287 Scheduler time: 26.13571034790948 Scheduler overhead time: 0.054810218047350645 Adapter cache time: 0.008662119973450899 Engine time: 0.05558300204575062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 26.36550369206816,
    "estimated_duration": 3600.0241691135784,
    "input_throughput": 7742.885517033476,
    "output_throughput": 6846.133481950639,
    "total_throughput": 14589.018998984115,
    "itl": 125.75890800992947,
    "ttft": 1551204.437709918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30577746255323274,
    "arrivals": 287382,
    "finished_requests": 112540,
    "scheduler_time": 151.88462099396787
}
#Debug simulation 
Total elapsed time: 26.365592406131327. Arrivals time: 0.5814492679201066 Scheduler time: 25.64156939694658 Scheduler overhead time: 0.05387584399431944 Adapter cache time: 0.00924878055229783 Engine time: 0.0564794740639627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.87440044991672,
    "estimated_duration": 3600.05025211489,
    "input_throughput": 7742.944139078204,
    "output_throughput": 6846.128046551903,
    "total_throughput": 14589.072185630108,
    "itl": 125.75741761369683,
    "ttft": 1551206.5228773335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27508462713100046,
    "arrivals": 287382,
    "finished_requests": 112541,
    "scheduler_time": 151.88637233702886
}
#Debug simulation 
Total elapsed time: 25.874511355068535. Arrivals time: 0.3485068194568157 Scheduler time: 25.3844036902301 Scheduler overhead time: 0.05344791943207383 Adapter cache time: 0.009091196581721306 Engine time: 0.05622171517461538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 25.934274741914123,
    "estimated_duration": 3600.0270169651735,
    "input_throughput": 7742.879391915868,
    "output_throughput": 6846.128066221239,
    "total_throughput": 14589.007458137108,
    "itl": 125.75889320536757,
    "ttft": 1551205.1121043446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3091728147864338,
    "arrivals": 287382,
    "finished_requests": 112540,
    "scheduler_time": 151.88464146844777
}
#Debug simulation 
Total elapsed time: 25.934362557251006. Arrivals time: 0.37932625599205494 Scheduler time: 25.414179489482194 Scheduler overhead time: 0.05357499094679952 Adapter cache time: 0.00919106462970376 Engine time: 0.05552282137796283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 41.959089565090835,
    "estimated_duration": 3600.061697597666,
    "input_throughput": 7722.4482065270995,
    "output_throughput": 6851.398690322266,
    "total_throughput": 14573.846896849365,
    "itl": 126.113128444712,
    "ttft": 1529076.954460902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4070448699430558,
    "arrivals": 285504,
    "finished_requests": 112280,
    "scheduler_time": 151.78039378675618
}
#Debug simulation 
Total elapsed time: 41.95921835210174. Arrivals time: 0.5704349158331752 Scheduler time: 41.234234888572246 Scheduler overhead time: 0.059014194179326296 Adapter cache time: 0.01131568057462573 Engine time: 0.060543501283973455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.351914673112333,
    "estimated_duration": 3600.0199987930696,
    "input_throughput": 7703.273595507057,
    "output_throughput": 6855.446916482144,
    "total_throughput": 14558.7205119892,
    "itl": 126.32328985003433,
    "ttft": 1545623.8633562801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5383037836872975,
    "arrivals": 285504,
    "finished_requests": 112144,
    "scheduler_time": 151.72487482594002
}
#Debug simulation 
Total elapsed time: 23.352027704939246. Arrivals time: 0.3726769918575883 Scheduler time: 22.840879395138472 Scheduler overhead time: 0.05226963385939598 Adapter cache time: 0.00958286365494132 Engine time: 0.05412070034071803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.25786161236465,
    "estimated_duration": 3600.01798818664,
    "input_throughput": 7703.277897777621,
    "output_throughput": 6855.450745242359,
    "total_throughput": 14558.72864301998,
    "itl": 126.32311314974758,
    "ttft": 1545623.2278646482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.53927783444524,
    "arrivals": 285504,
    "finished_requests": 112144,
    "scheduler_time": 151.72477829485922
}
#Debug simulation 
Total elapsed time: 23.257971464190632. Arrivals time: 0.3353807716630399 Scheduler time: 22.786268877331167 Scheduler overhead time: 0.05148298013955355 Adapter cache time: 0.009144747164100409 Engine time: 0.05326785985380411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 22.51999675715342,
    "estimated_duration": 3600.1225004362277,
    "input_throughput": 7705.948616092494,
    "output_throughput": 6854.131212760132,
    "total_throughput": 14560.079828852626,
    "itl": 126.27471759191508,
    "ttft": 1544253.0182356117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.400704631758854,
    "arrivals": 285504,
    "finished_requests": 112165,
    "scheduler_time": 151.7464426579986
}
#Debug simulation 
Total elapsed time: 22.520135204307735. Arrivals time: 0.3714223471470177 Scheduler time: 22.012490247376263 Scheduler overhead time: 0.051197550259530544 Adapter cache time: 0.009401812218129635 Engine time: 0.05310215102508664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 23.266069385688752,
    "estimated_duration": 3600.079440248024,
    "input_throughput": 7702.773636042189,
    "output_throughput": 6855.604552520554,
    "total_throughput": 14558.378188562741,
    "itl": 126.31879282596795,
    "ttft": 1546350.9112227275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5529558253847073,
    "arrivals": 285504,
    "finished_requests": 112151,
    "scheduler_time": 151.72904289519437
}
#Debug simulation 
Total elapsed time: 23.266185258049518. Arrivals time: 0.3424308313988149 Scheduler time: 22.785225356929004 Scheduler overhead time: 0.05197419924661517 Adapter cache time: 0.009493667166680098 Engine time: 0.05454998090863228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 33.700007285922766,
    "estimated_duration": 3600.0305399784324,
    "input_throughput": 7707.061285142937,
    "output_throughput": 6856.857664365227,
    "total_throughput": 14563.918949508165,
    "itl": 126.2730747682918,
    "ttft": 1545393.214322908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5980100589804359,
    "arrivals": 285504,
    "finished_requests": 112160,
    "scheduler_time": 151.8006897279621
}
#Debug simulation 
Total elapsed time: 33.700090950354934. Arrivals time: 0.3885221788659692 Scheduler time: 33.16491163149476 Scheduler overhead time: 0.05536898924037814 Adapter cache time: 0.010626926086843014 Engine time: 0.057535065803676844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 23.183558396995068,
    "estimated_duration": 3600.0878016875936,
    "input_throughput": 7702.755745846221,
    "output_throughput": 6855.588629930235,
    "total_throughput": 14558.344375776454,
    "itl": 126.31877724315488,
    "ttft": 1546354.4469509425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5601237912103537,
    "arrivals": 285504,
    "finished_requests": 112151,
    "scheduler_time": 151.72916336901122
}
#Debug simulation 
Total elapsed time: 23.183670707978308. Arrivals time: 0.33697913819923997 Scheduler time: 22.710310814902186 Scheduler overhead time: 0.0517496932297945 Adapter cache time: 0.009576818905770779 Engine time: 0.05293332738801837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.87709072511643,
    "estimated_duration": 3600.052431144212,
    "input_throughput": 7746.4157907101535,
    "output_throughput": 6845.374746991516,
    "total_throughput": 14591.790537701669,
    "itl": 125.77118195587728,
    "ttft": 1539026.2237898249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31829072536900604,
    "arrivals": 284495,
    "finished_requests": 112491,
    "scheduler_time": 151.76702402260304
}
#Debug simulation 
Total elapsed time: 18.877175647299737. Arrivals time: 0.35006477078422904 Scheduler time: 18.39656031411141 Scheduler overhead time: 0.04884201940149069 Adapter cache time: 0.008408168330788612 Engine time: 0.05143370758742094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.80260017886758,
    "estimated_duration": 3600.1193095301087,
    "input_throughput": 7746.4054944445625,
    "output_throughput": 6845.313969113027,
    "total_throughput": 14591.71946355759,
    "itl": 125.77236436809025,
    "ttft": 1539036.0045930964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34079268891597164,
    "arrivals": 284495,
    "finished_requests": 112492,
    "scheduler_time": 151.76928570026288
}
#Debug simulation 
Total elapsed time: 18.802675001788884. Arrivals time: 0.32474840711802244 Scheduler time: 18.34769102698192 Scheduler overhead time: 0.04928209912031889 Adapter cache time: 0.008271146565675735 Engine time: 0.050914228428155184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.928551021963358,
    "estimated_duration": 3600.1199809257178,
    "input_throughput": 7746.404049797534,
    "output_throughput": 6845.312692512868,
    "total_throughput": 14591.716742310402,
    "itl": 125.77231522762105,
    "ttft": 1539035.1809663055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34114502737298663,
    "arrivals": 284495,
    "finished_requests": 112492,
    "scheduler_time": 151.76927115889393
}
#Debug simulation 
Total elapsed time: 18.92866482073441. Arrivals time: 0.35058204689994454 Scheduler time: 18.44635296612978 Scheduler overhead time: 0.049856210593134165 Adapter cache time: 0.008505159988999367 Engine time: 0.0515078641474247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 18.842264447826892,
    "estimated_duration": 3600.0602933592154,
    "input_throughput": 7746.398873219476,
    "output_throughput": 6845.3597972952175,
    "total_throughput": 14591.758670514693,
    "itl": 125.77134229958892,
    "ttft": 1539028.848551096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32444887617835794,
    "arrivals": 284495,
    "finished_requests": 112491,
    "scheduler_time": 151.76713277631248
}
#Debug simulation 
Total elapsed time: 18.84236559877172. Arrivals time: 0.3361181332729757 Scheduler time: 18.37460872437805 Scheduler overhead time: 0.04985588230192661 Adapter cache time: 0.008316135499626398 Engine time: 0.05168713955208659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 18.96623374382034,
    "estimated_duration": 3600.1307188765263,
    "input_throughput": 7746.380944940482,
    "output_throughput": 6845.292275301188,
    "total_throughput": 14591.67322024167,
    "itl": 125.7724544629914,
    "ttft": 1539054.648285816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3461751788295806,
    "arrivals": 284495,
    "finished_requests": 112492,
    "scheduler_time": 151.76956216657038
}
#Debug simulation 
Total elapsed time: 18.966317884158343. Arrivals time: 0.3516208161599934 Scheduler time: 18.48312524612993 Scheduler overhead time: 0.05015818914398551 Adapter cache time: 0.00839453050866723 Engine time: 0.05115306889638305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.83734457474202,
    "estimated_duration": 3600.0421032934114,
    "input_throughput": 7746.438013735393,
    "output_throughput": 6845.394385097691,
    "total_throughput": 14591.832398833083,
    "itl": 125.77137308446748,
    "ttft": 1539023.8524062233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3109652306698266,
    "arrivals": 284495,
    "finished_requests": 112491,
    "scheduler_time": 151.76679600578743
}
#Debug simulation 
Total elapsed time: 18.837441940791905. Arrivals time: 0.3273604833520949 Scheduler time: 18.379583575297147 Scheduler overhead time: 0.04927273839712143 Adapter cache time: 0.008371670264750719 Engine time: 0.05104946065694094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.869097598828375,
    "estimated_duration": 3600.135594893335,
    "input_throughput": 7746.370453256849,
    "output_throughput": 6845.283004050338,
    "total_throughput": 14591.653457307188,
    "itl": 125.772435641746,
    "ttft": 1539057.1407783418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3503250537812706,
    "arrivals": 284495,
    "finished_requests": 112492,
    "scheduler_time": 151.76964207350454
}
#Debug simulation 
Total elapsed time: 18.869208990130574. Arrivals time: 0.3505628746934235 Scheduler time: 18.387861276976764 Scheduler overhead time: 0.04924403131008148 Adapter cache time: 0.008351936936378479 Engine time: 0.05109772179275751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.44651392986998,
    "estimated_duration": 3600.027310362637,
    "input_throughput": 7757.401984038526,
    "output_throughput": 6847.783051267114,
    "total_throughput": 14605.185035305642,
    "itl": 125.67075794239135,
    "ttft": 1539943.2098105722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 284045,
    "finished_requests": 112550,
    "scheduler_time": 151.77023421862415
}
#Debug simulation 
Total elapsed time: 21.446590831968933. Arrivals time: 0.3618900706060231 Scheduler time: 20.948546283412725 Scheduler overhead time: 0.05186742544174194 Adapter cache time: 0.008606269024312496 Engine time: 0.05345475720241666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.876450586132705,
    "estimated_duration": 3600.068526915832,
    "input_throughput": 7757.5217780439025,
    "output_throughput": 6847.8360385878195,
    "total_throughput": 14605.357816631722,
    "itl": 125.67044748850414,
    "ttft": 1539942.936207705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31741076494101433,
    "arrivals": 284045,
    "finished_requests": 112552,
    "scheduler_time": 151.77143678477447
}
#Debug simulation 
Total elapsed time: 20.876582452096045. Arrivals time: 0.36504553398117423 Scheduler time: 20.37757517118007 Scheduler overhead time: 0.050677917432039976 Adapter cache time: 0.00881946925073862 Engine time: 0.05238837469369173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 20.754892728291452,
    "estimated_duration": 3600.0690517443672,
    "input_throughput": 7757.52064712982,
    "output_throughput": 6847.835040290369,
    "total_throughput": 14605.35568742019,
    "itl": 125.67045134541904,
    "ttft": 1539943.2454205344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3178169104643169,
    "arrivals": 284045,
    "finished_requests": 112552,
    "scheduler_time": 151.77144399605774
}
#Debug simulation 
Total elapsed time: 20.755036131944507. Arrivals time: 0.3291744999587536 Scheduler time: 20.29182588867843 Scheduler overhead time: 0.05038019036874175 Adapter cache time: 0.008817626163363457 Engine time: 0.0526618342846632 
