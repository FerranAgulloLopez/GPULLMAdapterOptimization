INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:46:59 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.94521014019847,
    "estimated_duration": 3600.1065074450867,
    "input_throughput": 8559.529540660411,
    "output_throughput": 7589.4737957045745,
    "total_throughput": 16149.003336364985,
    "itl": 112.85424864749814,
    "ttft": 1350295.0795944263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6907461163029105,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.4879126586523
}
#Debug simulation 
Total elapsed time: 114.94544387795031. Arrivals time: 0.5032001407817006 Scheduler time: 114.2418318670243 Scheduler overhead time: 0.07753150258213282 Adapter cache time: 0.016541687306016684 Engine time: 0.0781299234367907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 115.53200350888073,
    "estimated_duration": 3600.071511789137,
    "input_throughput": 8559.612746327275,
    "output_throughput": 7589.5475716317815,
    "total_throughput": 16149.160317959057,
    "itl": 112.85328177102144,
    "ttft": 1350283.8235118592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.656233521967662,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.4877214782575
}
#Debug simulation 
Total elapsed time: 115.5322019350715. Arrivals time: 0.49215817730873823 Scheduler time: 114.83803600817919 Scheduler overhead time: 0.07913633389398456 Adapter cache time: 0.01703021489083767 Engine time: 0.0772248674184084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 115.50155382696539,
    "estimated_duration": 3600.1168479286484,
    "input_throughput": 8559.504955437695,
    "output_throughput": 7589.45199673739,
    "total_throughput": 16148.956952175085,
    "itl": 112.85455044705434,
    "ttft": 1350298.2888035015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7010579267889284,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.48794133174874
}
#Debug simulation 
Total elapsed time: 115.50173314800486. Arrivals time: 0.507386913523078 Scheduler time: 114.79371590865776 Scheduler overhead time: 0.07791820308193564 Adapter cache time: 0.016639513429254293 Engine time: 0.07742093782871962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 118.90318067604676,
    "estimated_duration": 3600.047728840848,
    "input_throughput": 8559.66929358516,
    "output_throughput": 7589.597710360773,
    "total_throughput": 16149.267003945934,
    "itl": 112.852708286669,
    "ttft": 1350277.8933777595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6309006122243599,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.48739730588576
}
#Debug simulation 
Total elapsed time: 118.90335306618363. Arrivals time: 0.4979006773792207 Scheduler time: 118.20417686691508 Scheduler overhead time: 0.07838161243125796 Adapter cache time: 0.016599442344158888 Engine time: 0.07814540062099695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 115.24796457216144,
    "estimated_duration": 3600.004260161321,
    "input_throughput": 8559.712092846006,
    "output_throughput": 7589.321296740823,
    "total_throughput": 16149.03338958683,
    "itl": 112.85488525343943,
    "ttft": 1350280.6305198479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7088546615466498,
    "arrivals": 254308,
    "finished_requests": 124370,
    "scheduler_time": 211.48049422203482
}
#Debug simulation 
Total elapsed time: 115.24814523989335. Arrivals time: 0.508527456317097 Scheduler time: 114.53624969301745 Scheduler overhead time: 0.07924743928015232 Adapter cache time: 0.016931444872170687 Engine time: 0.07881783787161112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.6386865992099,
    "estimated_duration": 3600.0368409570965,
    "input_throughput": 8565.442622471064,
    "output_throughput": 7612.395431129589,
    "total_throughput": 16177.838053600653,
    "itl": 112.84490860546846,
    "ttft": 1346615.3612201847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6916702301288015,
    "arrivals": 253785,
    "finished_requests": 124520,
    "scheduler_time": 211.71625071484019
}
#Debug simulation 
Total elapsed time: 112.63886624900624. Arrivals time: 0.4953286787495017 Scheduler time: 111.94174843421206 Scheduler overhead time: 0.0795412459410727 Adapter cache time: 0.016603905241936445 Engine time: 0.07753883814439178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.38337247725576,
    "estimated_duration": 3600.0807044344115,
    "input_throughput": 8557.455382056498,
    "output_throughput": 7605.012567156132,
    "total_throughput": 16162.467949212629,
    "itl": 112.9498829418665,
    "ttft": 1343412.567103499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.711077285462527,
    "arrivals": 253785,
    "finished_requests": 124392,
    "scheduler_time": 211.52105622020804
}
#Debug simulation 
Total elapsed time: 116.38355468725786. Arrivals time: 0.5026376293972135 Scheduler time: 115.67813231376931 Scheduler overhead time: 0.07863921718671918 Adapter cache time: 0.01677418127655983 Engine time: 0.0780172566883266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.54010928608477,
    "estimated_duration": 3600.0810115813815,
    "input_throughput": 8557.45465196279,
    "output_throughput": 7605.011918321687,
    "total_throughput": 16162.466570284476,
    "itl": 112.94985519715479,
    "ttft": 1343411.9055800203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7123879957385393,
    "arrivals": 253785,
    "finished_requests": 124392,
    "scheduler_time": 211.52095382075836
}
#Debug simulation 
Total elapsed time: 116.540288050659. Arrivals time: 0.5199736319482327 Scheduler time: 115.81959942914546 Scheduler overhead time: 0.07801896752789617 Adapter cache time: 0.016787865664809942 Engine time: 0.07797086518257856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 112.93315535504371,
    "estimated_duration": 3600.0539060958376,
    "input_throughput": 8565.402020171614,
    "output_throughput": 7612.359346507644,
    "total_throughput": 16177.761366679259,
    "itl": 112.84529682409392,
    "ttft": 1346621.9380015538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7043530389387177,
    "arrivals": 253785,
    "finished_requests": 124520,
    "scheduler_time": 211.71655194727722
}
#Debug simulation 
Total elapsed time: 112.93332696100697. Arrivals time: 0.5159700168296695 Scheduler time: 112.21689316350967 Scheduler overhead time: 0.0778577383607626 Adapter cache time: 0.01657926430925727 Engine time: 0.07796583510935307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 117.06073221610859,
    "estimated_duration": 3600.0913582456346,
    "input_throughput": 8557.430057834104,
    "output_throughput": 7604.990061513864,
    "total_throughput": 16162.420119347968,
    "itl": 112.95011226431026,
    "ttft": 1343415.6338657576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7219452835060682,
    "arrivals": 253785,
    "finished_requests": 124392,
    "scheduler_time": 211.52105109373372
}
#Debug simulation 
Total elapsed time: 117.06102067418396. Arrivals time: 0.5052672857418656 Scheduler time: 116.35286550363526 Scheduler overhead time: 0.07906217547133565 Adapter cache time: 0.016782431863248348 Engine time: 0.07854511262848973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.80760582722723,
    "estimated_duration": 3600.0216571795363,
    "input_throughput": 8565.478748858033,
    "output_throughput": 7612.4275378583625,
    "total_throughput": 16177.906286716396,
    "itl": 112.84458135899747,
    "ttft": 1346610.676271089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6757513666478926,
    "arrivals": 253785,
    "finished_requests": 124520,
    "scheduler_time": 211.7161364248685
}
#Debug simulation 
Total elapsed time: 112.807782352902. Arrivals time: 0.5065937037579715 Scheduler time: 112.10170817701146 Scheduler overhead time: 0.07796456664800644 Adapter cache time: 0.01643119426444173 Engine time: 0.07676955126225948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.77315984386951,
    "estimated_duration": 3600.1010385291893,
    "input_throughput": 8557.407047827282,
    "output_throughput": 7604.969612515506,
    "total_throughput": 16162.376660342788,
    "itl": 112.94917668050222,
    "ttft": 1343415.697354316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7306222947686933,
    "arrivals": 253785,
    "finished_requests": 124392,
    "scheduler_time": 211.52150237365626
}
#Debug simulation 
Total elapsed time: 117.77334395609796. Arrivals time: 0.5094385640695691 Scheduler time: 117.06080051651224 Scheduler overhead time: 0.07939890353009105 Adapter cache time: 0.017158426344394684 Engine time: 0.07851100852712989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 129.87590703601018,
    "estimated_duration": 3600.003661274026,
    "input_throughput": 7865.537556142124,
    "output_throughput": 6943.379049551842,
    "total_throughput": 14808.916605693967,
    "itl": 102.47861096000211,
    "ttft": 1426809.654947046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38562145573552653,
    "arrivals": 216813,
    "finished_requests": 114330,
    "scheduler_time": 224.60342846690446
}
#Debug simulation 
Total elapsed time: 129.87607905175537. Arrivals time: 0.5376365650445223 Scheduler time: 129.09719856083393 Scheduler overhead time: 0.09612806374207139 Adapter cache time: 0.018745622597634792 Engine time: 0.09244990488514304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 140.84768238989636,
    "estimated_duration": 3600.1050372538944,
    "input_throughput": 7828.4663109434705,
    "output_throughput": 6926.059862692143,
    "total_throughput": 14754.526173635613,
    "itl": 102.03597159086239,
    "ttft": 1428609.451818503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3812977240234613,
    "arrivals": 216813,
    "finished_requests": 113926,
    "scheduler_time": 225.18350438159814
}
#Debug simulation 
Total elapsed time: 140.8478635638021. Arrivals time: 0.5410613301210105 Scheduler time: 140.06134677072987 Scheduler overhead time: 0.09697411442175508 Adapter cache time: 0.019328510388731956 Engine time: 0.09470824152231216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.16083546867594,
    "estimated_duration": 3600.1059376967128,
    "input_throughput": 7828.46435292157,
    "output_throughput": 6926.058130376214,
    "total_throughput": 14754.522483297784,
    "itl": 102.0359873427701,
    "ttft": 1428609.9405058227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3820597623847433,
    "arrivals": 216813,
    "finished_requests": 113926,
    "scheduler_time": 225.18355996519955
}
#Debug simulation 
Total elapsed time: 141.16101683676243. Arrivals time: 0.544850847683847 Scheduler time: 140.36755125131458 Scheduler overhead time: 0.09785475675016642 Adapter cache time: 0.019776318687945604 Engine time: 0.09545039804652333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 141.1052905828692,
    "estimated_duration": 3600.08902067086,
    "input_throughput": 7828.501139326875,
    "output_throughput": 6926.090676322655,
    "total_throughput": 14754.59181564953,
    "itl": 102.03558150070884,
    "ttft": 1428602.4899695108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.365362506604288,
    "arrivals": 216813,
    "finished_requests": 113926,
    "scheduler_time": 225.18310568250627
}
#Debug simulation 
Total elapsed time: 141.1054665260017. Arrivals time: 0.5524413492530584 Scheduler time: 140.3083113064058 Scheduler overhead time: 0.09693194134160876 Adapter cache time: 0.019362699706107378 Engine time: 0.09367356495931745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 140.16691329609603,
    "estimated_duration": 3600.1110799321573,
    "input_throughput": 7828.453171098016,
    "output_throughput": 6926.048237508794,
    "total_throughput": 14754.501408606811,
    "itl": 102.03611980461685,
    "ttft": 1428612.2425977737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38696416005492235,
    "arrivals": 216813,
    "finished_requests": 113926,
    "scheduler_time": 225.18369776439607
}
#Debug simulation 
Total elapsed time: 140.16709594009444. Arrivals time: 0.5705032446421683 Scheduler time: 139.35139044607058 Scheduler overhead time: 0.09743663854897022 Adapter cache time: 0.019044467713683844 Engine time: 0.0942364470101893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 130.52864569844678,
    "estimated_duration": 3600.0196041236995,
    "input_throughput": 7865.5027232532375,
    "output_throughput": 6943.348300483619,
    "total_throughput": 14808.851023736855,
    "itl": 102.4781629578691,
    "ttft": 1426823.2563123154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3767463371576746,
    "arrivals": 216813,
    "finished_requests": 114330,
    "scheduler_time": 224.60702808789412
}
#Debug simulation 
Total elapsed time: 130.52882106928155. Arrivals time: 0.5499969883821905 Scheduler time: 129.73584320070222 Scheduler overhead time: 0.09585444070398808 Adapter cache time: 0.018615898676216602 Engine time: 0.094276771415025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 140.58759486675262,
    "estimated_duration": 3600.116238300284,
    "input_throughput": 7828.441954225937,
    "output_throughput": 6926.038313632978,
    "total_throughput": 14754.480267858915,
    "itl": 102.03610015713204,
    "ttft": 1428614.554638577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3917428039386866,
    "arrivals": 216813,
    "finished_requests": 113926,
    "scheduler_time": 225.18387741147848
}
#Debug simulation 
Total elapsed time: 140.5877831466496. Arrivals time: 0.5408752113580704 Scheduler time: 139.79921641759574 Scheduler overhead time: 0.09816142497584224 Adapter cache time: 0.019506321288645267 Engine time: 0.09571232087910175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 136.76352234976366,
    "estimated_duration": 3600.050338703404,
    "input_throughput": 7768.551372554605,
    "output_throughput": 6900.090738438571,
    "total_throughput": 14668.642110993176,
    "itl": 101.23924956248976,
    "ttft": 1342907.4901399028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32441170085687154,
    "arrivals": 194341,
    "finished_requests": 113021,
    "scheduler_time": 222.06062591270907
}
#Debug simulation 
Total elapsed time: 136.763693574816. Arrivals time: 0.5230006235651672 Scheduler time: 135.99200607137755 Scheduler overhead time: 0.09835514472797513 Adapter cache time: 0.01950041763484478 Engine time: 0.09512910433113575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 136.47109438339248,
    "estimated_duration": 3600.072257390059,
    "input_throughput": 7768.504074491921,
    "output_throughput": 6900.048727913234,
    "total_throughput": 14668.552802405155,
    "itl": 101.23971371925883,
    "ttft": 1342918.2838578064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.345547003550455,
    "arrivals": 194341,
    "finished_requests": 113021,
    "scheduler_time": 222.061409296666
}
#Debug simulation 
Total elapsed time: 136.47127396520227. Arrivals time: 0.5224398821592331 Scheduler time: 135.701607842464 Scheduler overhead time: 0.09847845137119293 Adapter cache time: 0.01833492936566472 Engine time: 0.09546610759571195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 136.8130440209061,
    "estimated_duration": 3600.07283912013,
    "input_throughput": 7768.502819191645,
    "output_throughput": 6900.047612945283,
    "total_throughput": 14668.550432136928,
    "itl": 101.23973652601654,
    "ttft": 1342918.568292618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3462203225865966,
    "arrivals": 194341,
    "finished_requests": 113021,
    "scheduler_time": 222.0614177462799
}
#Debug simulation 
Total elapsed time: 136.81322868308052. Arrivals time: 0.520004007499665 Scheduler time: 136.04535340424627 Scheduler overhead time: 0.09773797867819667 Adapter cache time: 0.01899509597569704 Engine time: 0.09562777355313301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 141.22546092281118,
    "estimated_duration": 3600.0541455821667,
    "input_throughput": 7745.630168984792,
    "output_throughput": 6867.811149546415,
    "total_throughput": 14613.441318531208,
    "itl": 100.60056358680399,
    "ttft": 1346417.2178423426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3240402808599176,
    "arrivals": 194341,
    "finished_requests": 112565,
    "scheduler_time": 223.46147365611577
}
#Debug simulation 
Total elapsed time: 141.2257680776529. Arrivals time: 0.5322305513545871 Scheduler time: 140.44103457359597 Scheduler overhead time: 0.09955596318468451 Adapter cache time: 0.01953996066004038 Engine time: 0.09769379766657948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 136.15895007690415,
    "estimated_duration": 3600.0777269158443,
    "input_throughput": 7768.492271959705,
    "output_throughput": 6900.038244807784,
    "total_throughput": 14668.53051676749,
    "itl": 101.23986818466486,
    "ttft": 1342920.9946489204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3507474588975313,
    "arrivals": 194341,
    "finished_requests": 113021,
    "scheduler_time": 222.06167836710608
}
#Debug simulation 
Total elapsed time: 136.15912434086204. Arrivals time: 0.5390921742655337 Scheduler time: 135.37387138977647 Scheduler overhead time: 0.09775284864008427 Adapter cache time: 0.019042138941586018 Engine time: 0.09452475048601627 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 140.99352517630905,
    "estimated_duration": 3600.039754068444,
    "input_throughput": 7745.661132904772,
    "output_throughput": 6867.838604298351,
    "total_throughput": 14613.499737203123,
    "itl": 100.60034103514825,
    "ttft": 1346410.1883382455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3109652306698266,
    "arrivals": 194341,
    "finished_requests": 112565,
    "scheduler_time": 223.4609484500262
}
#Debug simulation 
Total elapsed time: 140.9936983170919. Arrivals time: 0.5407075448893011 Scheduler time: 140.2004693299532 Scheduler overhead time: 0.10029610432684422 Adapter cache time: 0.019339130260050297 Engine time: 0.09693658072501421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 136.48371479474008,
    "estimated_duration": 3600.0822022583548,
    "input_throughput": 7768.482614773632,
    "output_throughput": 6900.0296672162885,
    "total_throughput": 14668.51228198992,
    "itl": 101.2402403239008,
    "ttft": 1342922.9564823548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3550230876356361,
    "arrivals": 194341,
    "finished_requests": 113021,
    "scheduler_time": 222.06177804229725
}
#Debug simulation 
Total elapsed time: 136.48388726869598. Arrivals time: 0.527261670678854 Scheduler time: 135.71101453201845 Scheduler overhead time: 0.09649336989969015 Adapter cache time: 0.01878427993506193 Engine time: 0.09488493436947465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 130.4571564416401,
    "estimated_duration": 3600.1226276598527,
    "input_throughput": 7689.095862268897,
    "output_throughput": 6808.80334788307,
    "total_throughput": 14497.899210151967,
    "itl": 100.56753468574607,
    "ttft": 1362669.0526278354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31829072536900604,
    "arrivals": 190487,
    "finished_requests": 111635,
    "scheduler_time": 224.0295336945836
}
#Debug simulation 
Total elapsed time: 130.45733703672886. Arrivals time: 0.5064615299925208 Scheduler time: 129.70240576053038 Scheduler overhead time: 0.0986175793223083 Adapter cache time: 0.018908655270934105 Engine time: 0.09610956627875566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 137.63845524191856,
    "estimated_duration": 3600.029103188475,
    "input_throughput": 7682.772890781263,
    "output_throughput": 6798.339762954601,
    "total_throughput": 14481.112653735865,
    "itl": 100.33091414599835,
    "ttft": 1360602.1471722259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32298227021237835,
    "arrivals": 190487,
    "finished_requests": 111485,
    "scheduler_time": 224.54298154375803
}
#Debug simulation 
Total elapsed time: 137.63863302674145. Arrivals time: 0.5149874114431441 Scheduler time: 136.87205348862335 Scheduler overhead time: 0.09974434971809387 Adapter cache time: 0.01947051752358675 Engine time: 0.09628334222361445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 138.35303370188922,
    "estimated_duration": 3600.029738077177,
    "input_throughput": 7682.771535874203,
    "output_throughput": 6798.338564023085,
    "total_throughput": 14481.110099897289,
    "itl": 100.33091748619995,
    "ttft": 1360602.4462827432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3235667006671432,
    "arrivals": 190487,
    "finished_requests": 111485,
    "scheduler_time": 224.5430320020004
}
#Debug simulation 
Total elapsed time: 138.35322077618912. Arrivals time: 0.543865208979696 Scheduler time: 137.55448571406305 Scheduler overhead time: 0.10176870226860046 Adapter cache time: 0.01908665895462036 Engine time: 0.09776433417573571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 137.7542143780738,
    "estimated_duration": 3600.015424002123,
    "input_throughput": 7682.802083456765,
    "output_throughput": 6798.365594998509,
    "total_throughput": 14481.167678455273,
    "itl": 100.33044838394196,
    "ttft": 1360595.8363672448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3099072200222874,
    "arrivals": 190487,
    "finished_requests": 111485,
    "scheduler_time": 224.54257748475987
}
#Debug simulation 
Total elapsed time: 137.75440473575145. Arrivals time: 0.5341294575482607 Scheduler time: 136.9664560481906 Scheduler overhead time: 0.10111515317112207 Adapter cache time: 0.019359063357114792 Engine time: 0.09749341104179621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 137.49337631091475,
    "estimated_duration": 3600.0336605545162,
    "input_throughput": 7682.763164981014,
    "output_throughput": 6798.331156778744,
    "total_throughput": 14481.094321759758,
    "itl": 100.33101596276077,
    "ttft": 1360604.234702585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32759082183241844,
    "arrivals": 190487,
    "finished_requests": 111485,
    "scheduler_time": 224.5431304353403
}
#Debug simulation 
Total elapsed time: 137.49355745362118. Arrivals time: 0.5419191033579409 Scheduler time: 136.6954507897608 Scheduler overhead time: 0.10128802852705121 Adapter cache time: 0.01953129656612873 Engine time: 0.09939451701939106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 135.34462297800928,
    "estimated_duration": 3600.101504510168,
    "input_throughput": 7694.85942696196,
    "output_throughput": 6808.306090618111,
    "total_throughput": 14503.16551758007,
    "itl": 100.53873493404582,
    "ttft": 1359799.5160537127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3019950797851201,
    "arrivals": 190487,
    "finished_requests": 111683,
    "scheduler_time": 224.07949430445612
}
#Debug simulation 
Total elapsed time: 135.3448186549358. Arrivals time: 0.5359868411906064 Scheduler time: 134.55715964082628 Scheduler overhead time: 0.09991208650171757 Adapter cache time: 0.018939476925879717 Engine time: 0.09735146118327975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 145.88336538802832,
    "estimated_duration": 3600.0907540536778,
    "input_throughput": 7657.1969662043075,
    "output_throughput": 6782.841786003462,
    "total_throughput": 14440.03875220777,
    "itl": 100.19119037685849,
    "ttft": 1355536.3561741721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32227289274334875,
    "arrivals": 190487,
    "finished_requests": 111226,
    "scheduler_time": 225.24704589428543
}
#Debug simulation 
Total elapsed time: 145.8835471328348. Arrivals time: 0.5409292154945433 Scheduler time: 145.08626713370904 Scheduler overhead time: 0.10206786263734102 Adapter cache time: 0.019907892681658268 Engine time: 0.09833571407943964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 141.96767270285636,
    "estimated_duration": 3600.0581219510336,
    "input_throughput": 7785.469581477281,
    "output_throughput": 6840.8112218625365,
    "total_throughput": 14626.280803339818,
    "itl": 100.79167639393438,
    "ttft": 1332410.6666264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3335931640886698,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.01359689092268
}
#Debug simulation 
Total elapsed time: 141.9678517151624. Arrivals time: 0.5319874943234026 Scheduler time: 141.1797919650562 Scheduler overhead time: 0.10177582083269954 Adapter cache time: 0.019457338843494654 Engine time: 0.09857341833412647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.45335420779884,
    "estimated_duration": 3600.078551408645,
    "input_throughput": 7785.425401074401,
    "output_throughput": 6840.772402136553,
    "total_throughput": 14626.197803210955,
    "itl": 100.79231652504565,
    "ttft": 1332420.1303438093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35492574975360186,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.0140943029812
}
#Debug simulation 
Total elapsed time: 142.45353149157017. Arrivals time: 0.5192344831302762 Scheduler time: 141.683058434166 Scheduler overhead time: 0.09959441889077425 Adapter cache time: 0.019778285641223192 Engine time: 0.09639385947957635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.83684848900884,
    "estimated_duration": 3600.0791472668216,
    "input_throughput": 7785.424112489014,
    "output_throughput": 6840.771269903065,
    "total_throughput": 14626.19538239208,
    "itl": 100.79232393455247,
    "ttft": 1332420.3849161481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35568812662735644,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.0141278614382
}
#Debug simulation 
Total elapsed time: 142.83716573007405. Arrivals time: 0.5245104245841503 Scheduler time: 142.0589342569001 Scheduler overhead time: 0.09994449513033032 Adapter cache time: 0.019854205660521984 Engine time: 0.09823675360530615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 142.4511387948878,
    "estimated_duration": 3600.0646162633984,
    "input_throughput": 7785.455536931763,
    "output_throughput": 6840.7988814271175,
    "total_throughput": 14626.254418358882,
    "itl": 100.79194399973629,
    "ttft": 1332414.0732534374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3402163182897495,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.01386820339388
}
#Debug simulation 
Total elapsed time: 142.45132408384234. Arrivals time: 0.5317510603927076 Scheduler time: 141.66640396555886 Scheduler overhead time: 0.1000056704506278 Adapter cache time: 0.01971174171194434 Engine time: 0.097888074349612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 141.97867176868021,
    "estimated_duration": 3600.0836479692266,
    "input_throughput": 7785.414379415993,
    "output_throughput": 6840.762717802971,
    "total_throughput": 14626.177097218964,
    "itl": 100.79244287749039,
    "ttft": 1332422.392623294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.360215262938291,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.01420146611926
}
#Debug simulation 
Total elapsed time: 141.9788519940339. Arrivals time: 0.5460629016160965 Scheduler time: 141.17613577470183 Scheduler overhead time: 0.1015953216701746 Adapter cache time: 0.020305601879954338 Engine time: 0.09867073502391577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 137.858551108744,
    "estimated_duration": 3600.035189097449,
    "input_throughput": 7803.374279528347,
    "output_throughput": 6849.082774155229,
    "total_throughput": 14652.457053683576,
    "itl": 100.99391899039288,
    "ttft": 1338844.42746068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3438557839137506,
    "arrivals": 188561,
    "finished_requests": 112806,
    "scheduler_time": 221.78883869963542
}
#Debug simulation 
Total elapsed time: 137.8587258560583. Arrivals time: 0.5309465890750289 Scheduler time: 137.07561248820275 Scheduler overhead time: 0.09991937363520265 Adapter cache time: 0.019950845278799534 Engine time: 0.0964512387290597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.53815425932407,
    "estimated_duration": 3600.088012815681,
    "input_throughput": 7785.404940163889,
    "output_throughput": 6840.754423872715,
    "total_throughput": 14626.159364036603,
    "itl": 100.79254261793332,
    "ttft": 1332424.3642111463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36461664546281075,
    "arrivals": 188561,
    "finished_requests": 112652,
    "scheduler_time": 222.01436500721664
}
#Debug simulation 
Total elapsed time: 142.53832942433655. Arrivals time: 0.53807153692469 Scheduler time: 141.74477478209883 Scheduler overhead time: 0.10120905004441738 Adapter cache time: 0.02007562667131424 Engine time: 0.09862236911430955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 131.9691664511338,
    "estimated_duration": 3600.0082445823887,
    "input_throughput": 7679.100746950339,
    "output_throughput": 6785.733348461761,
    "total_throughput": 14464.834095412101,
    "itl": 101.57277558060476,
    "ttft": 1362429.6932231095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34277462732046804,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.97136030413563
}
#Debug simulation 
Total elapsed time: 131.96934601012617. Arrivals time: 0.5081368451938033 Scheduler time: 131.21137727610767 Scheduler overhead time: 0.09814436826854944 Adapter cache time: 0.01918073045089841 Engine time: 0.09661219408735633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 132.19243348296732,
    "estimated_duration": 3600.029551563031,
    "input_throughput": 7679.055297753708,
    "output_throughput": 6785.693186711134,
    "total_throughput": 14464.748484464842,
    "itl": 101.57334303204398,
    "ttft": 1362439.0897905636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3643044959567487,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.97177698300592
}
#Debug simulation 
Total elapsed time: 132.19261657400057. Arrivals time: 0.5090796654112637 Scheduler time: 131.43098409520462 Scheduler overhead time: 0.09936157846823335 Adapter cache time: 0.01996827172115445 Engine time: 0.09677232801914215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.5335355233401,
    "estimated_duration": 3600.030568592138,
    "input_throughput": 7679.0531283769205,
    "output_throughput": 6785.6912697142225,
    "total_throughput": 14464.744398091143,
    "itl": 101.57335694832847,
    "ttft": 1362439.4298423112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3651559306681163,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.97184253881431
}
#Debug simulation 
Total elapsed time: 131.53371237032115. Arrivals time: 0.5085295680910349 Scheduler time: 130.7784138773568 Scheduler overhead time: 0.09746941877529025 Adapter cache time: 0.018659451510757208 Engine time: 0.09480293281376362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 132.0753029640764,
    "estimated_duration": 3600.014396199996,
    "input_throughput": 7679.087625088545,
    "output_throughput": 6785.72175316459,
    "total_throughput": 14464.809378253134,
    "itl": 101.57297340398937,
    "ttft": 1362432.458411232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34877787385601566,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.9713085980333
}
#Debug simulation 
Total elapsed time: 132.07548458408564. Arrivals time: 0.5135214468464255 Scheduler time: 131.31058986345306 Scheduler overhead time: 0.09938815655186772 Adapter cache time: 0.01957477955147624 Engine time: 0.09685654425993562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 132.3414564956911,
    "estimated_duration": 3600.0354944459386,
    "input_throughput": 7679.042621288005,
    "output_throughput": 6785.681984993786,
    "total_throughput": 14464.72460628179,
    "itl": 101.57348524832497,
    "ttft": 1362441.6260075013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3699345745518805,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.97198974873407
}
#Debug simulation 
Total elapsed time: 132.34164074761793. Arrivals time: 0.508202335331589 Scheduler time: 131.58398350095376 Scheduler overhead time: 0.0989312450401485 Adapter cache time: 0.019624434877187014 Engine time: 0.09541875869035721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 133.24677255004644,
    "estimated_duration": 3600.1267988329264,
    "input_throughput": 7679.177302577833,
    "output_throughput": 6785.594054053676,
    "total_throughput": 14464.771356631509,
    "itl": 101.57269520593408,
    "ttft": 1362439.0426238906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33488563302904406,
    "arrivals": 187655,
    "finished_requests": 111734,
    "scheduler_time": 223.9789663983207
}
#Debug simulation 
Total elapsed time: 133.24695566808805. Arrivals time: 0.5178715013898909 Scheduler time: 132.4778550104238 Scheduler overhead time: 0.09848622744902968 Adapter cache time: 0.019584842026233673 Engine time: 0.09682898968458176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.9892586870119,
    "estimated_duration": 3600.040004042302,
    "input_throughput": 7679.033002121929,
    "output_throughput": 6785.673484897462,
    "total_throughput": 14464.706487019392,
    "itl": 101.57360388265,
    "ttft": 1362443.6370825474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37421020328998544,
    "arrivals": 187655,
    "finished_requests": 111731,
    "scheduler_time": 223.97204412354586
}
#Debug simulation 
Total elapsed time: 131.98944481275976. Arrivals time: 0.5252716159448028 Scheduler time: 131.2118599084206 Scheduler overhead time: 0.09963830234482884 Adapter cache time: 0.019978050608187914 Engine time: 0.0970816663466394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 132.43389497324824,
    "estimated_duration": 3600.105108104406,
    "input_throughput": 7808.809508565246,
    "output_throughput": 6872.612120213257,
    "total_throughput": 14681.421628778504,
    "itl": 100.6008417584788,
    "ttft": 1327967.6752695853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4192868209187868,
    "arrivals": 187118,
    "finished_requests": 113381,
    "scheduler_time": 221.06670712408615
}
#Debug simulation 
Total elapsed time: 132.43406935594976. Arrivals time: 0.5293815424665809 Scheduler time: 131.65456636436284 Scheduler overhead time: 0.0985615230165422 Adapter cache time: 0.01993566006422043 Engine time: 0.09548078384250402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.93152697198093,
    "estimated_duration": 3600.036699414075,
    "input_throughput": 7787.262836671291,
    "output_throughput": 6857.13065203412,
    "total_throughput": 14644.393488705411,
    "itl": 100.23323858883938,
    "ttft": 1329984.0654675644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36294882693560787,
    "arrivals": 187118,
    "finished_requests": 113176,
    "scheduler_time": 221.83033436769682
}
#Debug simulation 
Total elapsed time: 135.9318243251182. Arrivals time: 0.5302696065045893 Scheduler time: 135.15264979284257 Scheduler overhead time: 0.09890143107622862 Adapter cache time: 0.01892556343227625 Engine time: 0.09580701077356935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 136.08104420267045,
    "estimated_duration": 3600.036803344262,
    "input_throughput": 7787.2626118592325,
    "output_throughput": 6857.130454074236,
    "total_throughput": 14644.39306593347,
    "itl": 100.23321557088504,
    "ttft": 1329983.9176372383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36346140179783176,
    "arrivals": 187118,
    "finished_requests": 113176,
    "scheduler_time": 221.8303258773397
}
#Debug simulation 
Total elapsed time: 136.0812215008773. Arrivals time: 0.5340263564139605 Scheduler time: 135.29744744626805 Scheduler overhead time: 0.09867059020325541 Adapter cache time: 0.019371475093066692 Engine time: 0.09625383233651519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 131.99718545516953,
    "estimated_duration": 3600.115352711844,
    "input_throughput": 7808.78728755838,
    "output_throughput": 6872.592563280674,
    "total_throughput": 14681.379850839054,
    "itl": 100.60118478941583,
    "ttft": 1327971.294538288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42965806100517534,
    "arrivals": 187118,
    "finished_requests": 113381,
    "scheduler_time": 221.06678056858232
}
#Debug simulation 
Total elapsed time: 131.99737310688943. Arrivals time: 0.5311158401891589 Scheduler time: 131.2190287783742 Scheduler overhead time: 0.09764728974550962 Adapter cache time: 0.01950032263994217 Engine time: 0.09495810512453318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 139.11200645798817,
    "estimated_duration": 3600.055557197587,
    "input_throughput": 7788.0823099923,
    "output_throughput": 6856.900013847527,
    "total_throughput": 14644.982323839828,
    "itl": 100.18479388888252,
    "ttft": 1328462.2328999988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35150769380852603,
    "arrivals": 187118,
    "finished_requests": 113165,
    "scheduler_time": 221.79310075604593
}
#Debug simulation 
Total elapsed time: 139.11218680813909. Arrivals time: 0.5437518549151719 Scheduler time: 138.31371801206842 Scheduler overhead time: 0.10092594288289547 Adapter cache time: 0.01975358324125409 Engine time: 0.0980183663778007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 132.57541954889894,
    "estimated_duration": 3600.094609815466,
    "input_throughput": 7808.832279949719,
    "output_throughput": 6872.632161538731,
    "total_throughput": 14681.46444148845,
    "itl": 100.60056688098108,
    "ttft": 1327963.4252045953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40963689040159856,
    "arrivals": 187118,
    "finished_requests": 113381,
    "scheduler_time": 221.06645899712402
}
#Debug simulation 
Total elapsed time: 132.57560398383066. Arrivals time: 0.5314259501174092 Scheduler time: 131.79483513580635 Scheduler overhead time: 0.09884485835209489 Adapter cache time: 0.019202443305402994 Engine time: 0.09594012796878815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 138.83752330578864,
    "estimated_duration": 3600.060775644245,
    "input_throughput": 7788.071020823968,
    "output_throughput": 6856.890074468946,
    "total_throughput": 14644.961095292914,
    "itl": 100.1849101064765,
    "ttft": 1328464.3886009525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35641209147870506,
    "arrivals": 187118,
    "finished_requests": 113165,
    "scheduler_time": 221.79321472787854
}
#Debug simulation 
Total elapsed time: 138.83769216714427. Arrivals time: 0.5375444660894573 Scheduler time: 138.04772898415104 Scheduler overhead time: 0.09965835511684418 Adapter cache time: 0.019327781163156033 Engine time: 0.09703072719275951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 139.87812964897603,
    "estimated_duration": 3600.114317879342,
    "input_throughput": 7777.7344071934685,
    "output_throughput": 6852.777112514693,
    "total_throughput": 14630.511519708161,
    "itl": 101.06079874398534,
    "ttft": 1311587.738701878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4101053576869885,
    "arrivals": 186916,
    "finished_requests": 113061,
    "scheduler_time": 221.42538054206744
}
#Debug simulation 
Total elapsed time: 139.87831735191867. Arrivals time: 0.5436307620257139 Scheduler time: 139.08221654919907 Scheduler overhead time: 0.09956761077046394 Adapter cache time: 0.020273207686841488 Engine time: 0.09658038849011064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 132.88244748814031,
    "estimated_duration": 3600.1141252962857,
    "input_throughput": 7765.504099873277,
    "output_throughput": 6851.847508575827,
    "total_throughput": 14617.351608449104,
    "itl": 100.95797987347873,
    "ttft": 1339935.3396118265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5217191509390251,
    "arrivals": 186916,
    "finished_requests": 112970,
    "scheduler_time": 221.85956084404984
}
#Debug simulation 
Total elapsed time: 132.8826150270179. Arrivals time: 0.5318721495568752 Scheduler time: 132.10247818753123 Scheduler overhead time: 0.09788446826860309 Adapter cache time: 0.019532763864845037 Engine time: 0.09574684454128146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 132.44455307116732,
    "estimated_duration": 3600.11541862978,
    "input_throughput": 7765.50131013312,
    "output_throughput": 6851.845047064779,
    "total_throughput": 14617.3463571979,
    "itl": 100.95800899641449,
    "ttft": 1339935.990185091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5227112502232212,
    "arrivals": 186916,
    "finished_requests": 112970,
    "scheduler_time": 221.85966200109073
}
#Debug simulation 
Total elapsed time: 132.4447384690866. Arrivals time: 0.5240120985545218 Scheduler time: 131.6719736903906 Scheduler overhead time: 0.0980103095062077 Adapter cache time: 0.01987684890627861 Engine time: 0.09567023580893874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 140.88892353326082,
    "estimated_duration": 3600.124963937674,
    "input_throughput": 7777.71140737679,
    "output_throughput": 6852.756847922323,
    "total_throughput": 14630.468255299113,
    "itl": 101.0612047204868,
    "ttft": 1311592.3686364698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42027931480202846,
    "arrivals": 186916,
    "finished_requests": 113061,
    "scheduler_time": 221.42552550995634
}
#Debug simulation 
Total elapsed time: 140.8891048762016. Arrivals time: 0.5266688810661435 Scheduler time: 140.1074306438677 Scheduler overhead time: 0.10030369507148862 Adapter cache time: 0.020281491335481405 Engine time: 0.0982789727859199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 132.13348276214674,
    "estimated_duration": 3600.1218917653487,
    "input_throughput": 7765.487347510672,
    "output_throughput": 6851.832727225835,
    "total_throughput": 14617.320074736506,
    "itl": 100.95823886065725,
    "ttft": 1339938.4395963447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5287474319711338,
    "arrivals": 186916,
    "finished_requests": 112970,
    "scheduler_time": 221.85979883917804
}
#Debug simulation 
Total elapsed time: 132.13366044824943. Arrivals time: 0.5047034714370966 Scheduler time: 131.3798430324532 Scheduler overhead time: 0.09742523683235049 Adapter cache time: 0.019694866612553596 Engine time: 0.09641521377488971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 140.76782806869596,
    "estimated_duration": 3600.104134422664,
    "input_throughput": 7777.756407729683,
    "output_throughput": 6852.796496664773,
    "total_throughput": 14630.552904394457,
    "itl": 101.06052102221912,
    "ttft": 1311582.3560159819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.400666739516892,
    "arrivals": 186916,
    "finished_requests": 113061,
    "scheduler_time": 221.42500639033477
}
#Debug simulation 
Total elapsed time: 140.7680153688416. Arrivals time: 0.5318780657835305 Scheduler time: 139.98462756257504 Scheduler overhead time: 0.09953182516619563 Adapter cache time: 0.020140434615314007 Engine time: 0.09626817749813199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 133.0746190128848,
    "estimated_duration": 3600.0004996564085,
    "input_throughput": 7765.54142219374,
    "output_throughput": 6851.730160135866,
    "total_throughput": 14617.271582329606,
    "itl": 100.95715275880042,
    "ttft": 1339965.8058985446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5360411515831949,
    "arrivals": 186916,
    "finished_requests": 112964,
    "scheduler_time": 221.85205097144737
}
#Debug simulation 
Total elapsed time: 133.07479856163263. Arrivals time: 0.518174865283072 Scheduler time: 132.3076818920672 Scheduler overhead time: 0.09838066482916474 Adapter cache time: 0.019293413031846285 Engine time: 0.09549129754304886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 135.33083678781986,
    "estimated_duration": 3600.0553098757596,
    "input_throughput": 7669.080784470728,
    "output_throughput": 6771.874291242859,
    "total_throughput": 14440.955075713588,
    "itl": 97.78253025834738,
    "ttft": 1249900.8171442435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3641980415279973,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.35150902008039
}
#Debug simulation 
Total elapsed time: 135.33115069195628. Arrivals time: 0.5293007069267333 Scheduler time: 134.54062976734713 Scheduler overhead time: 0.10391905438154936 Adapter cache time: 0.020055736880749464 Engine time: 0.10045237420126796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.55915360618383,
    "estimated_duration": 3600.081366577293,
    "input_throughput": 7669.02527712834,
    "output_throughput": 6771.825277709758,
    "total_throughput": 14440.850554838098,
    "itl": 97.78291783696993,
    "ttft": 1249912.71372068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3880950152501464,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.3525814579621
}
#Debug simulation 
Total elapsed time: 135.55933601036668. Arrivals time: 0.5303622330538929 Scheduler time: 134.77014252590016 Scheduler overhead time: 0.10253975680097938 Adapter cache time: 0.019905118737369776 Engine time: 0.10004372615367174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.97053669393063,
    "estimated_duration": 3600.0805225125446,
    "input_throughput": 7669.027075186426,
    "output_throughput": 6771.826865412856,
    "total_throughput": 14440.853940599281,
    "itl": 97.7828584274062,
    "ttft": 1249912.2822932769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3888212950713941,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.35261173067707
}
#Debug simulation 
Total elapsed time: 135.970726526808. Arrivals time: 0.5305741536431015 Scheduler time: 135.18017703993246 Scheduler overhead time: 0.10311124194413424 Adapter cache time: 0.01976713165640831 Engine time: 0.10056387446820736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 135.73155030980706,
    "estimated_duration": 3600.061747076754,
    "input_throughput": 7669.067071535251,
    "output_throughput": 6771.862182585012,
    "total_throughput": 14440.929254120263,
    "itl": 97.78265300516995,
    "ttft": 1249903.2905228897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3713426071940923,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.3517036359327
}
#Debug simulation 
Total elapsed time: 135.73171352781355. Arrivals time: 0.5286794803105295 Scheduler time: 134.9429064085707 Scheduler overhead time: 0.10337985400110483 Adapter cache time: 0.019970083609223366 Engine time: 0.10021739499643445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 135.42373810335994,
    "estimated_duration": 3600.0863933704118,
    "input_throughput": 7669.0145688843495,
    "output_throughput": 6771.815822224253,
    "total_throughput": 14440.830391108602,
    "itl": 97.78305022378031,
    "ttft": 1249914.7806364212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39397720031440286,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.3528379915644
}
#Debug simulation 
Total elapsed time: 135.4239160362631. Arrivals time: 0.5293217115104198 Scheduler time: 134.63555946899578 Scheduler overhead time: 0.1026865323074162 Adapter cache time: 0.019682192243635654 Engine time: 0.10023075388744473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 135.48222782602534,
    "estimated_duration": 3600.045831050922,
    "input_throughput": 7669.100976956277,
    "output_throughput": 6771.892121407596,
    "total_throughput": 14440.993098363873,
    "itl": 97.78236006186934,
    "ttft": 1249896.7144033024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3558159850933593,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.3511125217185
}
#Debug simulation 
Total elapsed time: 135.4824028434232. Arrivals time: 0.5252419915050268 Scheduler time: 134.6985144643113 Scheduler overhead time: 0.1026121387258172 Adapter cache time: 0.020079005975276232 Engine time: 0.09942703973501921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 135.18646091595292,
    "estimated_duration": 3600.09177628436,
    "input_throughput": 7669.0031020529295,
    "output_throughput": 6771.805696898537,
    "total_throughput": 14440.808798951466,
    "itl": 97.78314721019176,
    "ttft": 1249916.9645360883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39875584419816706,
    "arrivals": 164321,
    "finished_requests": 111058,
    "scheduler_time": 217.35304210730803
}
#Debug simulation 
Total elapsed time: 135.18665594700724. Arrivals time: 0.5217281598597765 Scheduler time: 134.40494409948587 Scheduler overhead time: 0.10235045431181788 Adapter cache time: 0.020235920324921608 Engine time: 0.10047574853524566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 151.00906804576516,
    "estimated_duration": 3600.1281008670585,
    "input_throughput": 7676.944049114152,
    "output_throughput": 6806.185033832168,
    "total_throughput": 14483.129082946321,
    "itl": 94.36481741259338,
    "ttft": 1208825.8030889728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3152302376250733,
    "arrivals": 160553,
    "finished_requests": 111394,
    "scheduler_time": 213.94434098594832
}
#Debug simulation 
Total elapsed time: 151.0092512066476. Arrivals time: 0.5202530822716653 Scheduler time: 150.2232720805332 Scheduler overhead time: 0.10461258236318827 Adapter cache time: 0.020490779541432858 Engine time: 0.10344613622874022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 156.69428676413372,
    "estimated_duration": 3600.0973972442307,
    "input_throughput": 7652.1907493635235,
    "output_throughput": 6791.240986623055,
    "total_throughput": 14443.431735986578,
    "itl": 93.88371932308947,
    "ttft": 1208470.116797843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3229822702123783,
    "arrivals": 160553,
    "finished_requests": 111107,
    "scheduler_time": 215.08420899685692
}
#Debug simulation 
Total elapsed time: 156.69445919897407. Arrivals time: 0.5164839886128902 Scheduler time: 155.91175835765898 Scheduler overhead time: 0.1049057487398386 Adapter cache time: 0.021000171545892954 Engine time: 0.10288518201559782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 156.48183789709583,
    "estimated_duration": 3600.0978007393455,
    "input_throughput": 7652.189891714161,
    "output_throughput": 6791.240225468021,
    "total_throughput": 14443.430117182183,
    "itl": 93.88370613443418,
    "ttft": 1208470.2461749457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3235667006671432,
    "arrivals": 160553,
    "finished_requests": 111107,
    "scheduler_time": 215.08422813867318
}
#Debug simulation 
Total elapsed time: 156.48202588083223. Arrivals time: 0.5228062150999904 Scheduler time: 155.6908342638053 Scheduler overhead time: 0.10520118661224842 Adapter cache time: 0.020690823905169964 Engine time: 0.10506856115534902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 161.7582249850966,
    "estimated_duration": 3600.039599821734,
    "input_throughput": 7687.346272904996,
    "output_throughput": 6799.815757918934,
    "total_throughput": 14487.162030823929,
    "itl": 94.3691843336382,
    "ttft": 1169626.9054427163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3252660668152386,
    "arrivals": 160553,
    "finished_requests": 111637,
    "scheduler_time": 213.81187885451556
}
#Debug simulation 
Total elapsed time: 161.75838994886726. Arrivals time: 0.5397977251559496 Scheduler time: 160.94634506711736 Scheduler overhead time: 0.107505161780864 Adapter cache time: 0.021336491219699383 Engine time: 0.10548296896740794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 156.65481420094147,
    "estimated_duration": 3600.1014120878194,
    "input_throughput": 7652.182215618094,
    "output_throughput": 6791.23341301131,
    "total_throughput": 14443.415628629406,
    "itl": 93.88381524679323,
    "ttft": 1208471.6537675073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3275908218324185,
    "arrivals": 160553,
    "finished_requests": 111107,
    "scheduler_time": 215.0843155588831
}
#Debug simulation 
Total elapsed time: 156.65499860886484. Arrivals time: 0.54138322500512 Scheduler time: 155.84174464736134 Scheduler overhead time: 0.10809048404917121 Adapter cache time: 0.020773564465343952 Engine time: 0.10440237121656537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 151.20030908612534,
    "estimated_duration": 3600.120324722128,
    "input_throughput": 7676.960631068134,
    "output_throughput": 6806.199734974484,
    "total_throughput": 14483.160366042617,
    "itl": 94.36531842430827,
    "ttft": 1208823.6877588762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30797518037492444,
    "arrivals": 160553,
    "finished_requests": 111394,
    "scheduler_time": 213.94401997541578
}
#Debug simulation 
Total elapsed time: 151.20048649283126. Arrivals time: 0.5255055292509496 Scheduler time: 150.4094879780896 Scheduler overhead time: 0.10410248255357146 Adapter cache time: 0.02043190971016884 Engine time: 0.10342035023495555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 155.89112053485587,
    "estimated_duration": 3600.1051487515656,
    "input_throughput": 7652.174273174559,
    "output_throughput": 6791.226364173946,
    "total_throughput": 14443.400637348506,
    "itl": 93.88388874571517,
    "ttft": 1208473.0205192438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33186645057052344,
    "arrivals": 160553,
    "finished_requests": 111107,
    "scheduler_time": 215.08447686395874
}
#Debug simulation 
Total elapsed time: 155.89142392203212. Arrivals time: 0.5259508392773569 Scheduler time: 155.09864098625258 Scheduler overhead time: 0.10380530264228582 Adapter cache time: 0.019992866087704897 Engine time: 0.10491494555026293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 168.27577447798103,
    "estimated_duration": 3600.034147072614,
    "input_throughput": 7527.193602325968,
    "output_throughput": 6701.177270670317,
    "total_throughput": 14228.370872996284,
    "itl": 91.27872865619109,
    "ttft": 1167723.8896963624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25095999500248556,
    "arrivals": 158690,
    "finished_requests": 109648,
    "scheduler_time": 216.15597665121922
}
#Debug simulation 
Total elapsed time: 168.2759677451104. Arrivals time: 0.5319800046272576 Scheduler time: 167.4626818234101 Scheduler overhead time: 0.11141973221674562 Adapter cache time: 0.021571060176938772 Engine time: 0.1089910720475018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 160.40112587390468,
    "estimated_duration": 3600.087139985649,
    "input_throughput": 7580.015410434976,
    "output_throughput": 6745.875323478089,
    "total_throughput": 14325.890733913066,
    "itl": 92.56580426402893,
    "ttft": 1163680.9734241297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2808428538311273,
    "arrivals": 158690,
    "finished_requests": 110444,
    "scheduler_time": 214.40185741050485
}
#Debug simulation 
Total elapsed time: 160.40131170116365. Arrivals time: 0.5304426439106464 Scheduler time: 159.59266575425863 Scheduler overhead time: 0.10825848253443837 Adapter cache time: 0.021461701951920986 Engine time: 0.10837648063898087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 161.2158336271532,
    "estimated_duration": 3600.0871946086313,
    "input_throughput": 7580.015295425805,
    "output_throughput": 6745.87522112506,
    "total_throughput": 14325.890516550866,
    "itl": 92.5658371704313,
    "ttft": 1163680.9724407746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2813029756769539,
    "arrivals": 158690,
    "finished_requests": 110444,
    "scheduler_time": 214.4017996641286
}
#Debug simulation 
Total elapsed time: 161.21601679828018. Arrivals time: 0.5308022974058986 Scheduler time: 160.40912343375385 Scheduler overhead time: 0.10898235300555825 Adapter cache time: 0.021185649558901787 Engine time: 0.10735088307410479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 168.27716973796487,
    "estimated_duration": 3600.040929318097,
    "input_throughput": 7527.179421577523,
    "output_throughput": 6701.1646460834945,
    "total_throughput": 14228.344067661017,
    "itl": 91.27880384203998,
    "ttft": 1167726.214150396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2578505790536293,
    "arrivals": 158690,
    "finished_requests": 109648,
    "scheduler_time": 216.15629228592417
}
#Debug simulation 
Total elapsed time: 168.277354049962. Arrivals time: 0.5321611962281168 Scheduler time: 167.46520676324144 Scheduler overhead time: 0.10978227853775024 Adapter cache time: 0.02199019119143486 Engine time: 0.10892920847982168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 159.66517724609002,
    "estimated_duration": 3600.0901865438154,
    "input_throughput": 7580.008995885159,
    "output_throughput": 6745.869614815114,
    "total_throughput": 14325.878610700272,
    "itl": 92.56586823979674,
    "ttft": 1163682.315765632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28444682033732527,
    "arrivals": 158690,
    "finished_requests": 110444,
    "scheduler_time": 214.40190846818584
}
#Debug simulation 
Total elapsed time: 159.66536227893084. Arrivals time: 0.5154360714368522 Scheduler time: 158.87546519143507 Scheduler overhead time: 0.10851943213492632 Adapter cache time: 0.02142435871064663 Engine time: 0.10578801948577166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 168.46989767905325,
    "estimated_duration": 3600.0281592461592,
    "input_throughput": 7527.206122097199,
    "output_throughput": 6701.188416551617,
    "total_throughput": 14228.394538648816,
    "itl": 91.27859734954548,
    "ttft": 1167721.562882279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24518412418197866,
    "arrivals": 158690,
    "finished_requests": 109648,
    "scheduler_time": 216.15575319440438
}
#Debug simulation 
Total elapsed time: 168.4700939548202. Arrivals time: 0.5286450134590268 Scheduler time: 167.66038581961766 Scheduler overhead time: 0.11139326356351376 Adapter cache time: 0.02165765268728137 Engine time: 0.10880703618749976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 160.68711769301444,
    "estimated_duration": 3600.0942790688164,
    "input_throughput": 7580.000379061843,
    "output_throughput": 6745.861946227041,
    "total_throughput": 14325.862325288883,
    "itl": 92.56598481513117,
    "ttft": 1163683.915218855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28859669528901544,
    "arrivals": 158690,
    "finished_requests": 110444,
    "scheduler_time": 214.40207249640423
}
#Debug simulation 
Total elapsed time: 160.68730030814186. Arrivals time: 0.541124764829874 Scheduler time: 159.87213754141703 Scheduler overhead time: 0.10743736056610942 Adapter cache time: 0.02115157712250948 Engine time: 0.10687032388523221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 142.403597519733,
    "estimated_duration": 3600.0447801178716,
    "input_throughput": 7643.516589562824,
    "output_throughput": 6741.85780522578,
    "total_throughput": 14385.374394788603,
    "itl": 93.28801905676721,
    "ttft": 1188620.5342529246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.3926702611136
}
#Debug simulation 
Total elapsed time: 142.4037811709568. Arrivals time: 0.5109959808178246 Scheduler time: 141.6276354319416 Scheduler overhead time: 0.10439907945692539 Adapter cache time: 0.02012175740674138 Engine time: 0.10308090504258871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.48489815369248,
    "estimated_duration": 3600.0658348439188,
    "input_throughput": 7643.471887005922,
    "output_throughput": 6741.818375955414,
    "total_throughput": 14385.290262961336,
    "itl": 93.28846532873285,
    "ttft": 1188629.2017765432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.317002169622574,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.39370901830736
}
#Debug simulation 
Total elapsed time: 141.48508254857734. Arrivals time: 0.5065028127282858 Scheduler time: 140.7169703952968 Scheduler overhead time: 0.10276691662147641 Adapter cache time: 0.01919539738446474 Engine time: 0.10194716835394502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.752846240066,
    "estimated_duration": 3600.066223151076,
    "input_throughput": 7643.471062572522,
    "output_throughput": 6741.817648775366,
    "total_throughput": 14385.288711347888,
    "itl": 93.28848017465185,
    "ttft": 1188629.3456650535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31747966296970875,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.3937198706947
}
#Debug simulation 
Total elapsed time: 141.7530262870714. Arrivals time: 0.5079656396992505 Scheduler time: 140.98157351324335 Scheduler overhead time: 0.10268323309719563 Adapter cache time: 0.01991758542135358 Engine time: 0.10341051034629345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 142.2023957069032,
    "estimated_duration": 3600.0527012871958,
    "input_throughput": 7643.499771589822,
    "output_throughput": 6741.84297116593,
    "total_throughput": 14385.34274275575,
    "itl": 93.2882063120436,
    "ttft": 1188623.4003831171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30433571475092336,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.39302298825493
}
#Debug simulation 
Total elapsed time: 142.20258327573538. Arrivals time: 0.5120545458048582 Scheduler time: 141.42444367241114 Scheduler overhead time: 0.10609527304768562 Adapter cache time: 0.019617363810539246 Engine time: 0.10177504224702716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 142.10099883470684,
    "estimated_duration": 3600.069700175592,
    "input_throughput": 7643.463680344264,
    "output_throughput": 6741.81113738331,
    "total_throughput": 14385.274817727573,
    "itl": 93.28853256687272,
    "ttft": 1188630.8593528382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32137803034856927,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.3937987207342
}
#Debug simulation 
Total elapsed time: 142.10117855574936. Arrivals time: 0.5168134612031281 Scheduler time: 141.31785076996312 Scheduler overhead time: 0.10466403793543577 Adapter cache time: 0.0209634848870337 Engine time: 0.10289962077513337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 142.0948567739688,
    "estimated_duration": 3600.0391912062973,
    "input_throughput": 7643.528455805403,
    "output_throughput": 6741.868271680482,
    "total_throughput": 14385.396727485884,
    "itl": 93.2879708342447,
    "ttft": 1188618.4276367656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29003487860551136,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.39251324196002
}
#Debug simulation 
Total elapsed time: 142.0951562630944. Arrivals time: 0.5290080094709992 Scheduler time: 141.30312221078202 Scheduler overhead time: 0.10342241311445832 Adapter cache time: 0.019687858875840902 Engine time: 0.10229111649096012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 141.65563364699483,
    "estimated_duration": 3600.0763044720566,
    "input_throughput": 7643.449658502533,
    "output_throughput": 6741.79876961227,
    "total_throughput": 14385.248428114803,
    "itl": 93.28867836333764,
    "ttft": 1188633.5503514262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.325779412873089,
    "arrivals": 157801,
    "finished_requests": 110803,
    "scheduler_time": 214.394203390199
}
#Debug simulation 
Total elapsed time: 141.6558135477826. Arrivals time: 0.508430696092546 Scheduler time: 140.88277856679633 Scheduler overhead time: 0.10481541231274605 Adapter cache time: 0.01971868798136711 Engine time: 0.10264177899807692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 153.66798725584522,
    "estimated_duration": 3600.1064776274256,
    "input_throughput": 7594.155386764476,
    "output_throughput": 6709.785154998934,
    "total_throughput": 14303.94054176341,
    "itl": 93.21902021462711,
    "ttft": 1193933.7712926127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3091092621372078,
    "arrivals": 157261,
    "finished_requests": 109831,
    "scheduler_time": 214.99654574005564
}
#Debug simulation 
Total elapsed time: 153.66817070078105. Arrivals time: 0.5139010651037097 Scheduler time: 152.88220685021952 Scheduler overhead time: 0.10706677986308932 Adapter cache time: 0.020936652086675167 Engine time: 0.10594215337187052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 139.01163387298584,
    "estimated_duration": 3600.0792900848564,
    "input_throughput": 7697.919897577251,
    "output_throughput": 6802.805445827483,
    "total_throughput": 14500.725343404734,
    "itl": 94.84754432322016,
    "ttft": 1181377.3896438915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32298227021237835,
    "arrivals": 157261,
    "finished_requests": 111416,
    "scheduler_time": 211.18385031865702
}
#Debug simulation 
Total elapsed time: 139.0118205230683. Arrivals time: 0.5068152961321175 Scheduler time: 138.24429772002622 Scheduler overhead time: 0.10335899330675602 Adapter cache time: 0.020073214545845985 Engine time: 0.10022791102528572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 139.06246811384335,
    "estimated_duration": 3600.079679483703,
    "input_throughput": 7697.919064939811,
    "output_throughput": 6802.80471000916,
    "total_throughput": 14500.72377494897,
    "itl": 94.84755376304832,
    "ttft": 1181377.5553977774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32356670066714327,
    "arrivals": 157261,
    "finished_requests": 111416,
    "scheduler_time": 211.18385536420243
}
#Debug simulation 
Total elapsed time: 139.0626489361748. Arrivals time: 0.5167584656737745 Scheduler time: 138.28648694138974 Scheduler overhead time: 0.10144222993403673 Adapter cache time: 0.01993196550756693 Engine time: 0.10096118366345763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 148.7961866860278,
    "estimated_duration": 3600.0110680710936,
    "input_throughput": 7585.722511301096,
    "output_throughput": 6707.47354477943,
    "total_throughput": 14293.196056080527,
    "itl": 93.22703509474754,
    "ttft": 1201788.6970031396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3154787252936514,
    "arrivals": 157261,
    "finished_requests": 109780,
    "scheduler_time": 214.98116532268563
}
#Debug simulation 
Total elapsed time: 148.79636819893494. Arrivals time: 0.5217991741374135 Scheduler time: 148.003229863476 Scheduler overhead time: 0.10629237443208694 Adapter cache time: 0.020684544928371906 Engine time: 0.1059459988027811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 138.2739213150926,
    "estimated_duration": 3600.055798376838,
    "input_throughput": 7697.970129378286,
    "output_throughput": 6802.849836672567,
    "total_throughput": 14500.819966050854,
    "itl": 94.8476695268645,
    "ttft": 1181364.911215597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3274650680460037,
    "arrivals": 157261,
    "finished_requests": 111416,
    "scheduler_time": 211.1801166184483
}
#Debug simulation 
Total elapsed time: 138.27410573419183. Arrivals time: 0.5151725010946393 Scheduler time: 137.49664099374786 Scheduler overhead time: 0.10281155770644546 Adapter cache time: 0.01992444461211562 Engine time: 0.10204617725685239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 149.80979382479563,
    "estimated_duration": 3600.022000136304,
    "input_throughput": 7585.699475993769,
    "output_throughput": 6707.453176421073,
    "total_throughput": 14293.152652414843,
    "itl": 93.22638404747867,
    "ttft": 1201796.9890971447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3019950797851201,
    "arrivals": 157261,
    "finished_requests": 109780,
    "scheduler_time": 214.98446599134851
}
#Debug simulation 
Total elapsed time: 149.80998080782592. Arrivals time: 0.5098222801461816 Scheduler time: 149.0275015598163 Scheduler overhead time: 0.10710853058844805 Adapter cache time: 0.020531113725155592 Engine time: 0.10543898493051529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 142.44469406595454,
    "estimated_duration": 3600.113370540214,
    "input_throughput": 7690.018938444081,
    "output_throughput": 6795.312392156438,
    "total_throughput": 14485.33133060052,
    "itl": 94.24372151013496,
    "ttft": 1184106.857670038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34542952980846137,
    "arrivals": 157261,
    "finished_requests": 111336,
    "scheduler_time": 211.74065995060636
}
#Debug simulation 
Total elapsed time: 142.44487504893914. Arrivals time: 0.4991162735968828 Scheduler time: 141.68382248096168 Scheduler overhead time: 0.10361707303673029 Adapter cache time: 0.019920299295336008 Engine time: 0.10111702559515834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 144.79411836806685,
    "estimated_duration": 3600.0733567671464,
    "input_throughput": 7633.129738410884,
    "output_throughput": 6760.08725051491,
    "total_throughput": 14393.216988925793,
    "itl": 94.10422986098149,
    "ttft": 1187918.7538349316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33665365183260254,
    "arrivals": 157020,
    "finished_requests": 110729,
    "scheduler_time": 212.15039744036093
}
#Debug simulation 
Total elapsed time: 144.79430315829813. Arrivals time: 0.5062111052684486 Scheduler time: 144.02169193001464 Scheduler overhead time: 0.10512945149093866 Adapter cache time: 0.01997696002945304 Engine time: 0.10367321129888296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 151.32137555303052,
    "estimated_duration": 3600.018327105731,
    "input_throughput": 7604.938784300784,
    "output_throughput": 6737.151257643493,
    "total_throughput": 14342.090041944279,
    "itl": 93.62497775914404,
    "ttft": 1182447.9636808645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3127863333723508,
    "arrivals": 157020,
    "finished_requests": 110340,
    "scheduler_time": 213.0785560809949
}
#Debug simulation 
Total elapsed time: 151.32155491318554. Arrivals time: 0.5239187581464648 Scheduler time: 150.5272472645156 Scheduler overhead time: 0.1056639519520104 Adapter cache time: 0.020605979021638632 Engine time: 0.10575598571449518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 151.33765722205862,
    "estimated_duration": 3600.01890642294,
    "input_throughput": 7604.937560509458,
    "output_throughput": 6737.150173497059,
    "total_throughput": 14342.087734006516,
    "itl": 93.62497666551585,
    "ttft": 1182448.1911964484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31342440163716706,
    "arrivals": 157020,
    "finished_requests": 110340,
    "scheduler_time": 213.07859736851245
}
#Debug simulation 
Total elapsed time: 151.33783025620505. Arrivals time: 0.524050020147115 Scheduler time: 150.53978938888758 Scheduler overhead time: 0.10714942822232842 Adapter cache time: 0.02144607249647379 Engine time: 0.10653986129909754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 144.28394535416737,
    "estimated_duration": 3600.081143726763,
    "input_throughput": 7633.113227984966,
    "output_throughput": 6760.072628476038,
    "total_throughput": 14393.185856461005,
    "itl": 94.10444445511496,
    "ttft": 1187921.7774901486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3444321545399727,
    "arrivals": 157020,
    "finished_requests": 110729,
    "scheduler_time": 212.15070927961094
}
#Debug simulation 
Total elapsed time: 144.28412509476766. Arrivals time: 0.5089846923947334 Scheduler time: 143.50610163109377 Scheduler overhead time: 0.10386798484250903 Adapter cache time: 0.019875555764883757 Engine time: 0.10741815948858857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 150.8110274542123,
    "estimated_duration": 3600.023229344201,
    "input_throughput": 7604.928428472198,
    "output_throughput": 6737.1420835021145,
    "total_throughput": 14342.070511974312,
    "itl": 93.62512043408299,
    "ttft": 1182450.030792286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.316945507656783,
    "arrivals": 157020,
    "finished_requests": 110340,
    "scheduler_time": 213.07880637017428
}
#Debug simulation 
Total elapsed time: 150.81133230030537. Arrivals time: 0.5139608648605645 Scheduler time: 150.02535233274102 Scheduler overhead time: 0.10558640351518989 Adapter cache time: 0.020602872129529715 Engine time: 0.10679091606289148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 163.5406706891954,
    "estimated_duration": 3600.0726081882262,
    "input_throughput": 7639.178148087536,
    "output_throughput": 6758.050363946426,
    "total_throughput": 14397.228512033962,
    "itl": 93.99709303282769,
    "ttft": 1152127.9384873868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.284054778015707,
    "arrivals": 157020,
    "finished_requests": 110873,
    "scheduler_time": 212.15250306603374
}
#Debug simulation 
Total elapsed time: 163.54085192689672. Arrivals time: 0.532192540820688 Scheduler time: 162.734984328039 Scheduler overhead time: 0.10747197363525629 Adapter cache time: 0.02126672025769949 Engine time: 0.10654941340908408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 151.53082559909672,
    "estimated_duration": 3600.027384573612,
    "input_throughput": 7604.919650699448,
    "output_throughput": 6737.134307347119,
    "total_throughput": 14342.053958046567,
    "itl": 93.62517676947819,
    "ttft": 1182451.564763014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3213468901813028,
    "arrivals": 157020,
    "finished_requests": 110340,
    "scheduler_time": 213.0789611880402
}
#Debug simulation 
Total elapsed time: 151.53101111203432. Arrivals time: 0.5282604326494038 Scheduler time: 150.73027312383056 Scheduler overhead time: 0.10723367193713784 Adapter cache time: 0.020607217215001583 Engine time: 0.10645503178238869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 171.44926253380254,
    "estimated_duration": 3600.010498194691,
    "input_throughput": 7202.600107139387,
    "output_throughput": 6354.6000800475485,
    "total_throughput": 13557.200187186936,
    "itl": 80.1546647284878,
    "ttft": 1164414.6604213407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2662624337221493,
    "arrivals": 137849,
    "finished_requests": 104049,
    "scheduler_time": 211.67175825797645
}
#Debug simulation 
Total elapsed time: 171.4494312806055. Arrivals time: 0.4827181133441627 Scheduler time: 170.67196930153295 Scheduler overhead time: 0.11605385271832347 Adapter cache time: 0.021899753715842962 Engine time: 0.11460748221725225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 173.7923067761585,
    "estimated_duration": 3600.048823717363,
    "input_throughput": 7201.325112371687,
    "output_throughput": 6355.147143914457,
    "total_throughput": 13556.472256286144,
    "itl": 80.18931687941419,
    "ttft": 1162402.0951374294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28465009476291014,
    "arrivals": 137849,
    "finished_requests": 104062,
    "scheduler_time": 211.60730741520982
}
#Debug simulation 
Total elapsed time: 173.79247843800113. Arrivals time: 0.48823325149714947 Scheduler time: 173.00604825885966 Scheduler overhead time: 0.11721983319148421 Adapter cache time: 0.02196774771437049 Engine time: 0.11659534880891442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 173.01538149314,
    "estimated_duration": 3600.049193651012,
    "input_throughput": 7201.3243723783335,
    "output_throughput": 6355.14649087261,
    "total_throughput": 13556.470863250945,
    "itl": 80.18933011582905,
    "ttft": 1162402.20247559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28502098951488747,
    "arrivals": 137849,
    "finished_requests": 104062,
    "scheduler_time": 211.60730645410044
}
#Debug simulation 
Total elapsed time: 173.01555681228638. Arrivals time: 0.4968406669795513 Scheduler time: 172.22507849149406 Scheduler overhead time: 0.11548568215221167 Adapter cache time: 0.021757394075393677 Engine time: 0.11456539947539568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 174.87131437426433,
    "estimated_duration": 3600.0685151502835,
    "input_throughput": 7205.219537027943,
    "output_throughput": 6351.874111219636,
    "total_throughput": 13557.093648247579,
    "itl": 80.14597415543783,
    "ttft": 1159106.718657587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26722932525677623,
    "arrivals": 137849,
    "finished_requests": 104064,
    "scheduler_time": 211.62807733911552
}
#Debug simulation 
Total elapsed time: 174.8714864132926. Arrivals time: 0.4946334110572934 Scheduler time: 174.07326690014452 Scheduler overhead time: 0.11943235853686929 Adapter cache time: 0.022653956897556782 Engine time: 0.11838753242045641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 171.6324846111238,
    "estimated_duration": 3600.053280930904,
    "input_throughput": 7201.316196435922,
    "output_throughput": 6355.139275628714,
    "total_throughput": 13556.455472064637,
    "itl": 80.18934691620152,
    "ttft": 1162403.2998992815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28841634174808856,
    "arrivals": 137849,
    "finished_requests": 104062,
    "scheduler_time": 211.6074981888547
}
#Debug simulation 
Total elapsed time: 171.63266290724277. Arrivals time: 0.4897000826895237 Scheduler time: 170.84761289553717 Scheduler overhead time: 0.11591936787590384 Adapter cache time: 0.02110533881932497 Engine time: 0.11496114870533347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 171.67570295697078,
    "estimated_duration": 3600.0475701335968,
    "input_throughput": 7201.327619967512,
    "output_throughput": 6355.149356860019,
    "total_throughput": 13556.476976827531,
    "itl": 80.18841046348074,
    "ttft": 1162404.3047626503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26013437565648956,
    "arrivals": 137849,
    "finished_requests": 104062,
    "scheduler_time": 211.60989875921436
}
#Debug simulation 
Total elapsed time: 171.67587688704953. Arrivals time: 0.48884154157713056 Scheduler time: 170.895523278974 Scheduler overhead time: 0.1148940627463162 Adapter cache time: 0.020800750236958265 Engine time: 0.1134101185016334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 173.19309683470055,
    "estimated_duration": 3600.0586850719633,
    "input_throughput": 7201.3053863542145,
    "output_throughput": 6355.129735765034,
    "total_throughput": 13556.435122119248,
    "itl": 80.18952703115677,
    "ttft": 1162404.815875613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2925662166997787,
    "arrivals": 137849,
    "finished_requests": 104062,
    "scheduler_time": 211.60777010355326
}
#Debug simulation 
Total elapsed time: 173.19325157580897. Arrivals time: 0.4816808635368943 Scheduler time: 172.41765177017078 Scheduler overhead time: 0.11616625217720866 Adapter cache time: 0.021084969397634268 Engine time: 0.11489736894145608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.38838746398687,
    "estimated_duration": 3600.0494532122016,
    "input_throughput": 7097.140006563675,
    "output_throughput": 6267.360294139286,
    "total_throughput": 13364.50030070296,
    "itl": 77.6658322670203,
    "ttft": 1142969.013741332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2601414582342838,
    "arrivals": 135943,
    "finished_requests": 102969,
    "scheduler_time": 211.68273522744965
}
#Debug simulation 
Total elapsed time: 180.38855210598558. Arrivals time: 0.4611660987138748 Scheduler time: 179.620586019475 Scheduler overhead time: 0.12043078942224383 Adapter cache time: 0.02294971700757742 Engine time: 0.1192658836953342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 176.44399160984904,
    "estimated_duration": 3600.1048443183126,
    "input_throughput": 7132.691438285672,
    "output_throughput": 6301.632030468197,
    "total_throughput": 13434.323468753868,
    "itl": 78.69808567355469,
    "ttft": 1147754.4690544144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28383290412602946,
    "arrivals": 135943,
    "finished_requests": 103471,
    "scheduler_time": 210.44653741713236
}
#Debug simulation 
Total elapsed time: 176.4441561019048. Arrivals time: 0.45060347206890583 Scheduler time: 175.69056128477678 Scheduler overhead time: 0.11929469788447022 Adapter cache time: 0.022453679237514734 Engine time: 0.11784050008282065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 176.26570541597903,
    "estimated_duration": 3600.105168964131,
    "input_throughput": 7132.690795082671,
    "output_throughput": 6301.631462207441,
    "total_throughput": 13434.322257290112,
    "itl": 78.69808817497625,
    "ttft": 1147754.4975401901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28434649452567107,
    "arrivals": 135943,
    "finished_requests": 103471,
    "scheduler_time": 210.44654854970386
}
#Debug simulation 
Total elapsed time: 176.26585823390633. Arrivals time: 0.4559388109482825 Scheduler time: 175.5046054054983 Scheduler overhead time: 0.12092981627210975 Adapter cache time: 0.021816987078636885 Engine time: 0.11852334486320615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 176.08486657217145,
    "estimated_duration": 3600.0945844440175,
    "input_throughput": 7132.711765673141,
    "output_throughput": 6301.649989427599,
    "total_throughput": 13434.36175510074,
    "itl": 78.69792384720992,
    "ttft": 1147752.3190222213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2728008305281402,
    "arrivals": 135943,
    "finished_requests": 103471,
    "scheduler_time": 210.44638231964052
}
#Debug simulation 
Total elapsed time: 176.0851362333633. Arrivals time: 0.4621552862226963 Scheduler time: 175.32138525415212 Scheduler overhead time: 0.11853174213320017 Adapter cache time: 0.02200035797432065 Engine time: 0.1181746432557702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 175.38480231212452,
    "estimated_duration": 3600.109797211928,
    "input_throughput": 7132.681625401101,
    "output_throughput": 6301.623360923431,
    "total_throughput": 13434.304986324532,
    "itl": 78.69820834818108,
    "ttft": 1147755.7794499053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28774184675887216,
    "arrivals": 135943,
    "finished_requests": 103471,
    "scheduler_time": 210.446781059475
}
#Debug simulation 
Total elapsed time: 175.38496531313285. Arrivals time: 0.46329008927568793 Scheduler time: 174.61738392338157 Scheduler overhead time: 0.11936930101364851 Adapter cache time: 0.023134056478738785 Engine time: 0.11820236220955849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.72418211679906,
    "estimated_duration": 3600.043351423273,
    "input_throughput": 7097.152035655019,
    "output_throughput": 6267.370916819604,
    "total_throughput": 13364.522952474623,
    "itl": 77.66567105864297,
    "ttft": 1142967.6981277277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2541542750666852,
    "arrivals": 135943,
    "finished_requests": 102969,
    "scheduler_time": 211.68242054449945
}
#Debug simulation 
Total elapsed time: 180.72433791588992. Arrivals time: 0.4619653425179422 Scheduler time: 179.95721054123715 Scheduler overhead time: 0.12029667245224118 Adapter cache time: 0.022306780330836773 Engine time: 0.11948483111336827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 177.08220314001665,
    "estimated_duration": 3600.1129070673082,
    "input_throughput": 7132.675464036471,
    "output_throughput": 6301.617917444901,
    "total_throughput": 13434.293381481371,
    "itl": 78.69824854850825,
    "ttft": 1147756.347331373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29164021413773267,
    "arrivals": 135943,
    "finished_requests": 103471,
    "scheduler_time": 210.44679367278084
}
#Debug simulation 
Total elapsed time: 177.082362215966. Arrivals time: 0.46169821824878454 Scheduler time: 176.31668864144012 Scheduler overhead time: 0.11964953597635031 Adapter cache time: 0.021986286621540785 Engine time: 0.11869531823322177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.7228796808049,
    "estimated_duration": 3600.067703768814,
    "input_throughput": 7027.8761628602815,
    "output_throughput": 6271.038174189231,
    "total_throughput": 13298.914337049513,
    "itl": 78.57533642067717,
    "ttft": 1154997.7196319406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2601414582342838,
    "arrivals": 135044,
    "finished_requests": 102405,
    "scheduler_time": 211.72453738590892
}
#Debug simulation 
Total elapsed time: 180.7230417006649. Arrivals time: 0.4483076548203826 Scheduler time: 179.97003070358187 Scheduler overhead time: 0.11945342551916838 Adapter cache time: 0.022307874634861946 Engine time: 0.118933891877532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 180.13846078608185,
    "estimated_duration": 3600.007163848218,
    "input_throughput": 7029.493233827062,
    "output_throughput": 6272.220296323835,
    "total_throughput": 13301.713530150897,
    "itl": 78.54955000303966,
    "ttft": 1154405.466762673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27961706787580626,
    "arrivals": 135044,
    "finished_requests": 102430,
    "scheduler_time": 211.73876977368977
}
#Debug simulation 
Total elapsed time: 180.13860754529014. Arrivals time: 0.45988807966932654 Scheduler time: 179.37576778791845 Scheduler overhead time: 0.1195330424234271 Adapter cache time: 0.021263090893626213 Engine time: 0.11862616799771786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 179.2310237241909,
    "estimated_duration": 3600.0084357997785,
    "input_throughput": 7029.4907501731905,
    "output_throughput": 6272.218080228919,
    "total_throughput": 13301.708830402109,
    "itl": 78.54953175677426,
    "ttft": 1154405.8878092659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2802912331931294,
    "arrivals": 135044,
    "finished_requests": 102430,
    "scheduler_time": 211.73886736703486
}
#Debug simulation 
Total elapsed time: 179.23117519216612. Arrivals time: 0.45191338984295726 Scheduler time: 178.47755419788882 Scheduler overhead time: 0.11875941464677453 Adapter cache time: 0.021874452475458384 Engine time: 0.11795724229887128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 177.1249440643005,
    "estimated_duration": 3600.073670130622,
    "input_throughput": 7027.8645156397615,
    "output_throughput": 6271.027781267839,
    "total_throughput": 13298.892296907601,
    "itl": 78.57539961081912,
    "ttft": 1154998.785291854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26600353930145515,
    "arrivals": 135044,
    "finished_requests": 102405,
    "scheduler_time": 211.72484174377888
}
#Debug simulation 
Total elapsed time: 177.12509641237557. Arrivals time: 0.39017783058807254 Scheduler time: 176.4514195038937 Scheduler overhead time: 0.11293044313788414 Adapter cache time: 0.01933516375720501 Engine time: 0.110184779856354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 176.6318459403701,
    "estimated_duration": 3600.011267756675,
    "input_throughput": 7029.485220408607,
    "output_throughput": 6272.213146174571,
    "total_throughput": 13301.698366583178,
    "itl": 78.54957491634399,
    "ttft": 1154406.4028004117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2835608316399155,
    "arrivals": 135044,
    "finished_requests": 102430,
    "scheduler_time": 211.73892991840032
}
#Debug simulation 
Total elapsed time: 176.63199285697192. Arrivals time: 0.40114331105723977 Scheduler time: 175.9413721547462 Scheduler overhead time: 0.11459097685292363 Adapter cache time: 0.019720894284546375 Engine time: 0.11266501434147358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 177.13572560437024,
    "estimated_duration": 3600.06301063185,
    "input_throughput": 7027.885324584758,
    "output_throughput": 6271.04634927977,
    "total_throughput": 13298.931673864528,
    "itl": 78.57531614785788,
    "ttft": 1154996.9707788203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2541542750666852,
    "arrivals": 135044,
    "finished_requests": 102405,
    "scheduler_time": 211.72448921322888
}
#Debug simulation 
Total elapsed time: 177.13586637936532. Arrivals time: 0.401028485968709 Scheduler time: 176.44567729206756 Scheduler overhead time: 0.11432860093191266 Adapter cache time: 0.01964849978685379 Engine time: 0.11234931368380785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 177.15736586600542,
    "estimated_duration": 3600.0151942223265,
    "input_throughput": 7029.477553487559,
    "output_throughput": 6272.2063051952555,
    "total_throughput": 13301.683858682814,
    "itl": 78.54963854083883,
    "ttft": 1154407.234781764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2872076914459464,
    "arrivals": 135044,
    "finished_requests": 102430,
    "scheduler_time": 211.73910948566942
}
#Debug simulation 
Total elapsed time: 177.15751616563648. Arrivals time: 0.40854693949222565 Scheduler time: 176.4624251644127 Scheduler overhead time: 0.11283586546778679 Adapter cache time: 0.01929551875218749 Engine time: 0.11251354869455099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.57186511531472,
    "estimated_duration": 3600.012690348319,
    "input_throughput": 7049.173484314756,
    "output_throughput": 6273.113164446916,
    "total_throughput": 13322.286648761672,
    "itl": 78.53935846025959,
    "ttft": 1135283.8714379512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 81,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2478995072585528,
    "arrivals": 134574,
    "finished_requests": 102843,
    "scheduler_time": 211.52230548099925
}
#Debug simulation 
Total elapsed time: 180.5720118242316. Arrivals time: 0.4117344757542014 Scheduler time: 179.86953651020303 Scheduler overhead time: 0.11421226058155298 Adapter cache time: 0.019834130071103573 Engine time: 0.11435832176357508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 174.54922760697082,
    "estimated_duration": 3600.0751742687353,
    "input_throughput": 7065.472738403783,
    "output_throughput": 6283.764617386107,
    "total_throughput": 13349.23735578989,
    "itl": 78.59345857749364,
    "ttft": 1145060.9475758085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28043425851268694,
    "arrivals": 134574,
    "finished_requests": 103002,
    "scheduler_time": 211.1185472797121
}
#Debug simulation 
Total elapsed time: 174.54936239635572. Arrivals time: 0.4101777467876673 Scheduler time: 173.8494865517132 Scheduler overhead time: 0.11341393506154418 Adapter cache time: 0.01976429671049118 Engine time: 0.11398958461359143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 174.36216470273212,
    "estimated_duration": 3600.0489100251853,
    "input_throughput": 7065.524284730357,
    "output_throughput": 6283.810460742252,
    "total_throughput": 13349.33474547261,
    "itl": 78.59404954400662,
    "ttft": 1145051.035314886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2809657281823457,
    "arrivals": 134574,
    "finished_requests": 103002,
    "scheduler_time": 211.1145516799859
}
#Debug simulation 
Total elapsed time: 174.36236923979595. Arrivals time: 0.40481451619416475 Scheduler time: 173.66921365819871 Scheduler overhead time: 0.11401183949783444 Adapter cache time: 0.019344968255609274 Engine time: 0.11268474953249097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 180.09022635780275,
    "estimated_duration": 3600.0171804519687,
    "input_throughput": 7049.16469226794,
    "output_throughput": 6273.105340337502,
    "total_throughput": 13322.270032605442,
    "itl": 78.53930308090055,
    "ttft": 1135284.7663528323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 81,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2520003615296447,
    "arrivals": 134574,
    "finished_requests": 102843,
    "scheduler_time": 211.5225996597241
}
#Debug simulation 
Total elapsed time: 180.09036128874868. Arrivals time: 0.4115935363806784 Scheduler time: 179.38798962160945 Scheduler overhead time: 0.11527991574257612 Adapter cache time: 0.0197480246424675 Engine time: 0.11385343270376325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 166.92956784041598,
    "estimated_duration": 3600.054346097938,
    "input_throughput": 7065.513615806959,
    "output_throughput": 6283.800972204706,
    "total_throughput": 13349.314588011666,
    "itl": 78.59413145479847,
    "ttft": 1145052.639869262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2844868342019617,
    "arrivals": 134574,
    "finished_requests": 103002,
    "scheduler_time": 211.11497845385082
}
#Debug simulation 
Total elapsed time: 166.9297044770792. Arrivals time: 0.4056618930771947 Scheduler time: 166.23845846857876 Scheduler overhead time: 0.11280710017308593 Adapter cache time: 0.019467074424028397 Engine time: 0.11185097275301814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.49877552269027,
    "estimated_duration": 3600.005511785234,
    "input_throughput": 7049.1875406645,
    "output_throughput": 6273.125673299595,
    "total_throughput": 13322.313213964097,
    "itl": 78.53920693202754,
    "ttft": 1135282.473175607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 81,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24219407388707648,
    "arrivals": 134574,
    "finished_requests": 102843,
    "scheduler_time": 211.5220025300592
}
#Debug simulation 
Total elapsed time: 180.4989034156315. Arrivals time: 0.4144958257675171 Scheduler time: 179.79344429820776 Scheduler overhead time: 0.11542508052662015 Adapter cache time: 0.01952253421768546 Engine time: 0.11311769532039762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 167.90455285692587,
    "estimated_duration": 3600.060218070798,
    "input_throughput": 7065.502091415233,
    "output_throughput": 6283.790722845937,
    "total_throughput": 13349.292814261169,
    "itl": 78.59425056114459,
    "ttft": 1145053.981655334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2881336940079924,
    "arrivals": 134574,
    "finished_requests": 103002,
    "scheduler_time": 211.11520279530774
}
#Debug simulation 
Total elapsed time: 167.90468483092263. Arrivals time: 0.4061832297593355 Scheduler time: 167.21212769532576 Scheduler overhead time: 0.1131367702037096 Adapter cache time: 0.01981975045055151 Engine time: 0.11147121665999293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 179.7989722811617,
    "estimated_duration": 3600.049825236042,
    "input_throughput": 7076.510391999824,
    "output_throughput": 6284.675517933019,
    "total_throughput": 13361.185909932843,
    "itl": 77.85913795307523,
    "ttft": 1136796.7066293915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2601414582342838,
    "arrivals": 134344,
    "finished_requests": 103059,
    "scheduler_time": 210.2434438510004
}
#Debug simulation 
Total elapsed time: 179.79910691222176. Arrivals time: 0.4062220202758908 Scheduler time: 179.10301381675526 Scheduler overhead time: 0.11454643169417977 Adapter cache time: 0.019475283101201057 Engine time: 0.11427401006221771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 178.6253301501274,
    "estimated_duration": 3600.075902614863,
    "input_throughput": 7072.9278184121795,
    "output_throughput": 6279.65093279827,
    "total_throughput": 13352.578751210449,
    "itl": 77.7954197998691,
    "ttft": 1136332.561911808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2744541579228826,
    "arrivals": 134344,
    "finished_requests": 102990,
    "scheduler_time": 210.4238116802303
}
#Debug simulation 
Total elapsed time: 178.62545243510976. Arrivals time: 0.40614161686971784 Scheduler time: 177.93110975716263 Scheduler overhead time: 0.11405310593545437 Adapter cache time: 0.01966218138113618 Engine time: 0.11275641620159149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 179.55098593281582,
    "estimated_duration": 3600.076550691958,
    "input_throughput": 7072.926545160778,
    "output_throughput": 6279.64980235066,
    "total_throughput": 13352.576347511438,
    "itl": 77.79544910068707,
    "ttft": 1136332.7292060808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27487869048491126,
    "arrivals": 134344,
    "finished_requests": 102990,
    "scheduler_time": 210.42383514759635
}
#Debug simulation 
Total elapsed time: 179.55111940018833. Arrivals time: 0.40883893705904484 Scheduler time: 178.85187464859337 Scheduler overhead time: 0.11501858988776803 Adapter cache time: 0.019984889309853315 Engine time: 0.11339169694110751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 179.02792782196775,
    "estimated_duration": 3600.0658157413327,
    "input_throughput": 7071.515439713608,
    "output_throughput": 6280.789895876902,
    "total_throughput": 13352.30533559051,
    "itl": 77.81816996103879,
    "ttft": 1136754.1591430071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.269810780233238,
    "arrivals": 134344,
    "finished_requests": 102997,
    "scheduler_time": 210.412892265276
}
#Debug simulation 
Total elapsed time: 179.02806405862793. Arrivals time: 0.40415442595258355 Scheduler time: 178.33294743206352 Scheduler overhead time: 0.11544792400673032 Adapter cache time: 0.019570641685277224 Engine time: 0.11421889392659068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 178.27993251895532,
    "estimated_duration": 3600.010804892122,
    "input_throughput": 7073.464881104175,
    "output_throughput": 6281.057259403861,
    "total_throughput": 13354.522140508036,
    "itl": 77.81347283849016,
    "ttft": 1135200.0383569382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 83,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.274893276374787,
    "arrivals": 134344,
    "finished_requests": 103012,
    "scheduler_time": 210.4157796570839
}
#Debug simulation 
Total elapsed time: 178.2800605110824. Arrivals time: 0.4131753393448889 Scheduler time: 177.57925950037315 Scheduler overhead time: 0.11360780615359545 Adapter cache time: 0.019519143737852573 Engine time: 0.11201538378372788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 178.70516408095136,
    "estimated_duration": 3600.0440290051192,
    "input_throughput": 7076.521785496133,
    "output_throughput": 6284.685636539983,
    "total_throughput": 13361.207422036116,
    "itl": 77.85895379111608,
    "ttft": 1136795.628540455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2541542750666852,
    "arrivals": 134344,
    "finished_requests": 103059,
    "scheduler_time": 210.2432305656476
}
#Debug simulation 
Total elapsed time: 178.70529719116166. Arrivals time: 0.410021161660552 Scheduler time: 178.0044257468544 Scheduler overhead time: 0.11621443461626768 Adapter cache time: 0.01979258516803384 Engine time: 0.11322889989241958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 179.8284492250532,
    "estimated_duration": 3600.0141918533304,
    "input_throughput": 7073.4582262550875,
    "output_throughput": 6281.051350066799,
    "total_throughput": 13354.509576321887,
    "itl": 77.81348767939791,
    "ttft": 1135200.5947472902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 83,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2785401361808178,
    "arrivals": 134344,
    "finished_requests": 103012,
    "scheduler_time": 210.41592814736768
}
#Debug simulation 
Total elapsed time: 179.8285784292966. Arrivals time: 0.4078531712293625 Scheduler time: 179.13116491260007 Scheduler overhead time: 0.11469666240736842 Adapter cache time: 0.019191339146345854 Engine time: 0.11309247184544802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 182.62944186199456,
    "estimated_duration": 3600.0679814738887,
    "input_throughput": 6997.474805930332,
    "output_throughput": 6201.102066650493,
    "total_throughput": 13198.576872580825,
    "itl": 76.12635855224832,
    "ttft": 1138433.2886739091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27544389695394755,
    "arrivals": 132130,
    "finished_requests": 101757,
    "scheduler_time": 209.14975092383713
}
#Debug simulation 
Total elapsed time: 182.6295672878623. Arrivals time: 0.412303751334548 Scheduler time: 181.91863209102303 Scheduler overhead time: 0.11847176682204008 Adapter cache time: 0.020946761593222618 Engine time: 0.11646930547431111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 183.3145289248787,
    "estimated_duration": 3600.06384338233,
    "input_throughput": 6997.482849174197,
    "output_throughput": 6201.109194504118,
    "total_throughput": 13198.592043678316,
    "itl": 76.12678744944176,
    "ttft": 1138429.0274350322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.294028840966057,
    "arrivals": 132130,
    "finished_requests": 101757,
    "scheduler_time": 209.1469744528449
}
#Debug simulation 
Total elapsed time: 183.31473224190995. Arrivals time: 0.4162946972064674 Scheduler time: 182.595793267712 Scheduler overhead time: 0.12000532168895006 Adapter cache time: 0.02144674491137266 Engine time: 0.11781434435397387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 182.72910869587213,
    "estimated_duration": 3600.0640071032603,
    "input_throughput": 6997.4825309480775,
    "output_throughput": 6201.108912494864,
    "total_throughput": 13198.591443442941,
    "itl": 76.12677542901407,
    "ttft": 1138429.0389449194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29448879355564733,
    "arrivals": 132130,
    "finished_requests": 101757,
    "scheduler_time": 209.14697833691022
}
#Debug simulation 
Total elapsed time: 182.72923353128135. Arrivals time: 0.41327306954190135 Scheduler time: 182.01610854873434 Scheduler overhead time: 0.11844355147331953 Adapter cache time: 0.020451695192605257 Engine time: 0.11741026258096099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 182.40826746309176,
    "estimated_duration": 3600.0549168596285,
    "input_throughput": 6997.500199795493,
    "output_throughput": 6201.124570475673,
    "total_throughput": 13198.624770271166,
    "itl": 76.12662977083892,
    "ttft": 1138427.5604869765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 90,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2842225533234888,
    "arrivals": 132130,
    "finished_requests": 101757,
    "scheduler_time": 209.14680626204952
}
#Debug simulation 
Total elapsed time: 182.40840627113357. Arrivals time: 0.42120290035381913 Scheduler time: 181.68759942799807 Scheduler overhead time: 0.11812066473066807 Adapter cache time: 0.02033050451427698 Engine time: 0.11737130163237453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 179.73792657302693,
    "estimated_duration": 3600.02767003121,
    "input_throughput": 6997.2662181737105,
    "output_throughput": 6198.008750251233,
    "total_throughput": 13195.274968424943,
    "itl": 75.74847505924711,
    "ttft": 1142723.3144294675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3002131557837129,
    "arrivals": 132130,
    "finished_requests": 101746,
    "scheduler_time": 209.48114296965323
}
#Debug simulation 
Total elapsed time: 179.7380596199073. Arrivals time: 0.4152474217116833 Scheduler time: 179.02520217746496 Scheduler overhead time: 0.11756895715370774 Adapter cache time: 0.02094407519325614 Engine time: 0.11606395849958062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.26530021894723,
    "estimated_duration": 3600.0149653746184,
    "input_throughput": 6993.496483251454,
    "output_throughput": 6193.569530808818,
    "total_throughput": 13187.066014060272,
    "itl": 75.66807795240126,
    "ttft": 1143066.5645602052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 93,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27807467742590264,
    "arrivals": 132130,
    "finished_requests": 101688,
    "scheduler_time": 209.67710299247034
}
#Debug simulation 
Total elapsed time: 180.26542694214731. Arrivals time: 0.4107214594259858 Scheduler time: 179.55533940484747 Scheduler overhead time: 0.11734364414587617 Adapter cache time: 0.021082947496324778 Engine time: 0.11805755505338311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 179.11575231095776,
    "estimated_duration": 3600.0311833857627,
    "input_throughput": 6997.259389378105,
    "output_throughput": 6198.002701469667,
    "total_throughput": 13195.262090847773,
    "itl": 75.74854035274888,
    "ttft": 1142723.9149043304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3047402920946476,
    "arrivals": 132130,
    "finished_requests": 101746,
    "scheduler_time": 209.48123042893374
}
#Debug simulation 
Total elapsed time: 179.11588824307546. Arrivals time: 0.4196381149813533 Scheduler time: 178.3986340817064 Scheduler overhead time: 0.11778049217537045 Adapter cache time: 0.020210086833685637 Engine time: 0.11632418865337968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 183.95551592390984,
    "estimated_duration": 3600.0235849830506,
    "input_throughput": 7011.461565221618,
    "output_throughput": 6175.896761549877,
    "total_throughput": 13187.358326771495,
    "itl": 75.20825595600694,
    "ttft": 1126849.4261818167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28156487244181305,
    "arrivals": 131205,
    "finished_requests": 101615,
    "scheduler_time": 207.93599735116825
}
#Debug simulation 
Total elapsed time: 183.95564108388498. Arrivals time: 0.40416643349453807 Scheduler time: 183.25295910146087 Scheduler overhead time: 0.11789417453110218 Adapter cache time: 0.020839089527726173 Engine time: 0.11670884769409895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.66350104892626,
    "estimated_duration": 3600.0425867902322,
    "input_throughput": 7003.4574292298785,
    "output_throughput": 6169.804235511458,
    "total_throughput": 13173.261664741336,
    "itl": 75.0884601382583,
    "ttft": 1128245.441305518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3000089415558614,
    "arrivals": 131205,
    "finished_requests": 101483,
    "scheduler_time": 208.2203535555697
}
#Debug simulation 
Total elapsed time: 184.6636425992474. Arrivals time: 0.4111248552799225 Scheduler time: 183.9513581157662 Scheduler overhead time: 0.11854002205654979 Adapter cache time: 0.020743486005812883 Engine time: 0.11802821792662144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.23026548698545,
    "estimated_duration": 3600.0430800316826,
    "input_throughput": 7003.456469687055,
    "output_throughput": 6169.803390187354,
    "total_throughput": 13173.25985987441,
    "itl": 75.08848542788229,
    "ttft": 1128245.5568420514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3005758312530818,
    "arrivals": 131205,
    "finished_requests": 101483,
    "scheduler_time": 208.22037994589195
}
#Debug simulation 
Total elapsed time: 184.23039918905124. Arrivals time: 0.4013909138739109 Scheduler time: 183.53179508913308 Scheduler overhead time: 0.11840006290003657 Adapter cache time: 0.01979559985920787 Engine time: 0.1159379887394607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 186.2494694488123,
    "estimated_duration": 3600.0290885384,
    "input_throughput": 7011.450846428559,
    "output_throughput": 6175.887320129037,
    "total_throughput": 13187.338166557596,
    "itl": 75.20839768186184,
    "ttft": 1126850.090522331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2877510820026511,
    "arrivals": 131205,
    "finished_requests": 101615,
    "scheduler_time": 207.9361150055805
}
#Debug simulation 
Total elapsed time: 186.2495964509435. Arrivals time: 0.41492601577192545 Scheduler time: 185.52845694869757 Scheduler overhead time: 0.1211563479155302 Adapter cache time: 0.02106453711166978 Engine time: 0.12039055628702044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 183.92884656321257,
    "estimated_duration": 3600.0480247905716,
    "input_throughput": 7003.446850258816,
    "output_throughput": 6169.794915803139,
    "total_throughput": 13173.241766061954,
    "itl": 75.08853721617412,
    "ttft": 1128246.4808953428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3043484448455274,
    "arrivals": 131205,
    "finished_requests": 101483,
    "scheduler_time": 208.22055170540145
}
#Debug simulation 
Total elapsed time: 183.92897881101817. Arrivals time: 0.41477691615000367 Scheduler time: 183.21393260825425 Scheduler overhead time: 0.11905823415145278 Adapter cache time: 0.020351181272417307 Engine time: 0.11725516803562641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 183.9639050439,
    "estimated_duration": 3600.015569132891,
    "input_throughput": 7011.477177050019,
    "output_throughput": 6175.910512896806,
    "total_throughput": 13187.387689946825,
    "itl": 75.20797035318922,
    "ttft": 1126847.7202426754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27508462713100046,
    "arrivals": 131205,
    "finished_requests": 101615,
    "scheduler_time": 207.93546213212923
}
#Debug simulation 
Total elapsed time: 183.9640318597667. Arrivals time: 0.40852719359099865 Scheduler time: 183.2577275042422 Scheduler overhead time: 0.11739921569824219 Adapter cache time: 0.0203378745354712 Engine time: 0.11682992242276669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 182.44016821682453,
    "estimated_duration": 3600.052470031645,
    "input_throughput": 7003.438202604412,
    "output_throughput": 6169.787297518126,
    "total_throughput": 13173.225500122539,
    "itl": 75.08857731778932,
    "ttft": 1128247.242120283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3082468122243878,
    "arrivals": 131205,
    "finished_requests": 101483,
    "scheduler_time": 208.22079846338926
}
#Debug simulation 
Total elapsed time: 182.44028689572588. Arrivals time: 0.40296298218891025 Scheduler time: 181.74032089672983 Scheduler overhead time: 0.11733121331781149 Adapter cache time: 0.019895891193300486 Engine time: 0.11619640281423926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 184.44928327016532,
    "estimated_duration": 3600.038934938342,
    "input_throughput": 6994.879904106173,
    "output_throughput": 6157.3981283565345,
    "total_throughput": 13152.278032462707,
    "itl": 74.4358261958618,
    "ttft": 1122129.253903205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2785043846978803,
    "arrivals": 130771,
    "finished_requests": 101342,
    "scheduler_time": 208.07810684739243
}
#Debug simulation 
Total elapsed time: 184.4494638289325. Arrivals time: 0.4033954222686589 Scheduler time: 183.74754252191633 Scheduler overhead time: 0.1177071793936193 Adapter cache time: 0.02017940627411008 Engine time: 0.11705581517890096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 183.96510806586593,
    "estimated_duration": 3600.055888467659,
    "input_throughput": 6994.846963533805,
    "output_throughput": 6157.369131687339,
    "total_throughput": 13152.216095221145,
    "itl": 74.4361502461655,
    "ttft": 1122131.6472132397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29661029594251886,
    "arrivals": 130771,
    "finished_requests": 101342,
    "scheduler_time": 208.0783463612102
}
#Debug simulation 
Total elapsed time: 183.9652257519774. Arrivals time: 0.3974161255173385 Scheduler time: 183.26761820493266 Scheduler overhead time: 0.11964042857289314 Adapter cache time: 0.019865848124027252 Engine time: 0.11647728690877557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 183.5112535939552,
    "estimated_duration": 3600.056236048156,
    "input_throughput": 6994.846288190915,
    "output_throughput": 6157.368537201785,
    "total_throughput": 13152.2148253927,
    "itl": 74.43617047041668,
    "ttft": 1122131.664021886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2971950649097564,
    "arrivals": 130771,
    "finished_requests": 101342,
    "scheduler_time": 208.0783448391749
}
#Debug simulation 
Total elapsed time: 183.5113800400868. Arrivals time: 0.39023107569664717 Scheduler time: 182.82066602120176 Scheduler overhead time: 0.11889081820845604 Adapter cache time: 0.02023457270115614 Engine time: 0.11733047012239695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 182.97017219895497,
    "estimated_duration": 3600.04508810383,
    "input_throughput": 6994.867948518794,
    "output_throughput": 6157.387604185661,
    "total_throughput": 13152.255552704455,
    "itl": 74.43595247106921,
    "ttft": 1122130.4797728832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28435243638930857,
    "arrivals": 130771,
    "finished_requests": 101342,
    "scheduler_time": 208.07827952726896
}
#Debug simulation 
Total elapsed time: 182.97029561828822. Arrivals time: 0.38172152172774076 Scheduler time: 182.29257450345904 Scheduler overhead time: 0.11695103533565998 Adapter cache time: 0.02072356501594186 Engine time: 0.1155497576110065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 182.41539322491735,
    "estimated_duration": 3600.037980106313,
    "input_throughput": 6986.449070533764,
    "output_throughput": 6150.877052509896,
    "total_throughput": 13137.32612304366,
    "itl": 74.27104577497362,
    "ttft": 1121101.6327039886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29091111939400427,
    "arrivals": 130771,
    "finished_requests": 101226,
    "scheduler_time": 208.4078065521718
}
#Debug simulation 
Total elapsed time: 182.4155206028372. Arrivals time: 0.38249948248267174 Scheduler time: 181.73655931977555 Scheduler overhead time: 0.11713507259264588 Adapter cache time: 0.01968372007831931 Engine time: 0.11635034531354904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 183.54622912686318,
    "estimated_duration": 3600.0328143189395,
    "input_throughput": 6994.891796497123,
    "output_throughput": 6157.40859689735,
    "total_throughput": 13152.300393394473,
    "itl": 74.43565332997855,
    "ttft": 1122129.340864396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2720945768360983,
    "arrivals": 130771,
    "finished_requests": 101342,
    "scheduler_time": 208.07805426711823
}
#Debug simulation 
Total elapsed time: 183.54635626077652. Arrivals time: 0.3833889774978161 Scheduler time: 182.86559804994613 Scheduler overhead time: 0.11768085462972522 Adapter cache time: 0.02013255376368761 Engine time: 0.11681608622893691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 183.62303197104484,
    "estimated_duration": 3600.040478718344,
    "input_throughput": 6986.444221581147,
    "output_throughput": 6150.872783486952,
    "total_throughput": 13137.3170050681,
    "itl": 74.27103925332554,
    "ttft": 1121102.081187664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2946837329864499,
    "arrivals": 130771,
    "finished_requests": 101226,
    "scheduler_time": 208.40795880596394
}
#Debug simulation 
Total elapsed time: 183.62315720738843. Arrivals time: 0.38519105268642306 Scheduler time: 182.9403221141547 Scheduler overhead time: 0.1172380237840116 Adapter cache time: 0.01978273969143629 Engine time: 0.11686153570190072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.49304613890126,
    "estimated_duration": 3600.0802848006165,
    "input_throughput": 6925.791378950008,
    "output_throughput": 6162.746729196555,
    "total_throughput": 13088.538108146562,
    "itl": 75.2953284106343,
    "ttft": 1128638.3340944601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2907463356736113,
    "arrivals": 130530,
    "finished_requests": 101187,
    "scheduler_time": 208.1551441320506
}
#Debug simulation 
Total elapsed time: 180.49317912291735. Arrivals time: 0.37859609397128224 Scheduler time: 179.81719863181934 Scheduler overhead time: 0.11689327098429203 Adapter cache time: 0.02027352014556527 Engine time: 0.11674955068156123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 185.26707101101056,
    "estimated_duration": 3600.07935760197,
    "input_throughput": 6914.443412875499,
    "output_throughput": 6146.562562092976,
    "total_throughput": 13061.005974968475,
    "itl": 75.14095585024147,
    "ttft": 1115915.7556430642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2830157134891487,
    "arrivals": 130530,
    "finished_requests": 100994,
    "scheduler_time": 208.71215166480604
}
#Debug simulation 
Total elapsed time: 185.26719158282503. Arrivals time: 0.3802156704477966 Scheduler time: 184.58821954671293 Scheduler overhead time: 0.11776153603568673 Adapter cache time: 0.019981996156275272 Engine time: 0.11728879529982805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.70254275016487,
    "estimated_duration": 3600.079834869064,
    "input_throughput": 6914.442496219073,
    "output_throughput": 6146.561747235476,
    "total_throughput": 13061.004243454548,
    "itl": 75.14092696557164,
    "ttft": 1115915.8068914313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2836719995364548,
    "arrivals": 130530,
    "finished_requests": 100994,
    "scheduler_time": 208.7121792562363
}
#Debug simulation 
Total elapsed time: 184.70266541093588. Arrivals time: 0.3805345739237964 Scheduler time: 184.02476561721414 Scheduler overhead time: 0.11678159423172474 Adapter cache time: 0.019978779833763838 Engine time: 0.11668943194672465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 180.77906884998083,
    "estimated_duration": 3600.085967573161,
    "input_throughput": 6925.780446517436,
    "output_throughput": 6162.7370012377705,
    "total_throughput": 13088.517447755206,
    "itl": 75.29544933181052,
    "ttft": 1128639.1491820503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.297129828205798,
    "arrivals": 130530,
    "finished_requests": 101187,
    "scheduler_time": 208.155447881127
}
#Debug simulation 
Total elapsed time: 180.77919269818813. Arrivals time: 0.3786691748537123 Scheduler time: 180.10705447895452 Scheduler overhead time: 0.11508249491453171 Adapter cache time: 0.019543273840099573 Engine time: 0.1159990755841136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 183.7791660167277,
    "estimated_duration": 3600.08271120058,
    "input_throughput": 6914.436971837979,
    "output_throughput": 6146.556836362398,
    "total_throughput": 13060.993808200377,
    "itl": 75.14097248331387,
    "ttft": 1115916.195295415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.286941597983241,
    "arrivals": 130530,
    "finished_requests": 100994,
    "scheduler_time": 208.71218614362917
}
#Debug simulation 
Total elapsed time: 183.77928746305406. Arrivals time: 0.3850143956951797 Scheduler time: 183.0930315842852 Scheduler overhead time: 0.12011666456237435 Adapter cache time: 0.020421375520527363 Engine time: 0.11751369573175907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 180.7389871859923,
    "estimated_duration": 3600.073915674782,
    "input_throughput": 6925.803631819764,
    "output_throughput": 6162.757632114196,
    "total_throughput": 13088.56126393396,
    "itl": 75.2951940804517,
    "ttft": 1128637.3944301603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.284054778015707,
    "arrivals": 130530,
    "finished_requests": 101187,
    "scheduler_time": 208.15486633236966
}
#Debug simulation 
Total elapsed time: 180.7391159250401. Arrivals time: 0.37965957447886467 Scheduler time: 180.06231944728643 Scheduler overhead time: 0.11724948324263096 Adapter cache time: 0.020814424380660057 Engine time: 0.11628067586570978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.41612284816802,
    "estimated_duration": 3600.08679846032,
    "input_throughput": 6914.429121721734,
    "output_throughput": 6146.549858037789,
    "total_throughput": 13060.978979759524,
    "itl": 75.1410684407918,
    "ttft": 1115916.9018432896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29071421157568667,
    "arrivals": 130530,
    "finished_requests": 100994,
    "scheduler_time": 208.71240075120474
}
#Debug simulation 
Total elapsed time: 184.41628531599417. Arrivals time: 0.3842728305608034 Scheduler time: 183.73393267998472 Scheduler overhead time: 0.11737452819943428 Adapter cache time: 0.020531112793833017 Engine time: 0.1161669590510428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 184.34374461788684,
    "estimated_duration": 3600.065904161721,
    "input_throughput": 6998.007722824253,
    "output_throughput": 6129.841949418012,
    "total_throughput": 13127.849672242264,
    "itl": 73.4448421656875,
    "ttft": 1114371.8122444877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 129375,
    "finished_requests": 100861,
    "scheduler_time": 206.4650335607857
}
#Debug simulation 
Total elapsed time: 184.3438669880852. Arrivals time: 0.38951639691367745 Scheduler time: 183.65135681303218 Scheduler overhead time: 0.11910027451813221 Adapter cache time: 0.02098585804924369 Engine time: 0.1189669189043343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.95154837798327,
    "estimated_duration": 3600.0585432031967,
    "input_throughput": 6996.634554056002,
    "output_throughput": 6128.463116705132,
    "total_throughput": 13125.097670761135,
    "itl": 73.43950953328975,
    "ttft": 1114743.3354748536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41065974858822296,
    "arrivals": 129375,
    "finished_requests": 100804,
    "scheduler_time": 206.46205885791647
}
#Debug simulation 
Total elapsed time: 184.9516640319489. Arrivals time: 0.3862244221381843 Scheduler time: 184.26455332618207 Scheduler overhead time: 0.11840732814744115 Adapter cache time: 0.02097261557355523 Engine time: 0.1175161455757916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 184.19383039698005,
    "estimated_duration": 3600.0594896864322,
    "input_throughput": 6996.632714587146,
    "output_throughput": 6128.461505485202,
    "total_throughput": 13125.094220072348,
    "itl": 73.43953235474247,
    "ttft": 1114743.4975524577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41147491699084743,
    "arrivals": 129375,
    "finished_requests": 100804,
    "scheduler_time": 206.46209013415668
}
#Debug simulation 
Total elapsed time: 184.19395388290286. Arrivals time: 0.38760461704805493 Scheduler time: 183.5060685821809 Scheduler overhead time: 0.11692864308133721 Adapter cache time: 0.020286067854613066 Engine time: 0.11930602416396141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 184.30956575507298,
    "estimated_duration": 3600.039155679383,
    "input_throughput": 6996.672233484799,
    "output_throughput": 6128.4961207141105,
    "total_throughput": 13125.16835419891,
    "itl": 73.4389730648434,
    "ttft": 1114739.933039589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3943159358506093,
    "arrivals": 129375,
    "finished_requests": 100804,
    "scheduler_time": 206.4614160727667
}
#Debug simulation 
Total elapsed time: 184.30968846473843. Arrivals time: 0.3837112905457616 Scheduler time: 183.6210290431045 Scheduler overhead time: 0.12158343940973282 Adapter cache time: 0.020884571131318808 Engine time: 0.11783333169296384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 184.82369197439402,
    "estimated_duration": 3600.0420900197587,
    "input_throughput": 6995.949594539816,
    "output_throughput": 6127.2600287512005,
    "total_throughput": 13123.209623291015,
    "itl": 73.41961895574943,
    "ttft": 1114500.2197608843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4104180307500068,
    "arrivals": 129375,
    "finished_requests": 100786,
    "scheduler_time": 206.46165348800918
}
#Debug simulation 
Total elapsed time: 184.8238176200539. Arrivals time: 0.38523240806534886 Scheduler time: 184.13893060572445 Scheduler overhead time: 0.11842642724514008 Adapter cache time: 0.020285598933696747 Engine time: 0.11714323610067368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 183.59463316295296,
    "estimated_duration": 3600.1244623684875,
    "input_throughput": 6987.364537794483,
    "output_throughput": 6122.353888148434,
    "total_throughput": 13109.718425942918,
    "itl": 73.36322865835326,
    "ttft": 1115265.2683069867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3887065383372833,
    "arrivals": 129375,
    "finished_requests": 100695,
    "scheduler_time": 206.76239566065652
}
#Debug simulation 
Total elapsed time: 183.59476061118767. Arrivals time: 0.38949210615828633 Scheduler time: 182.90433705877513 Scheduler overhead time: 0.11909465212374926 Adapter cache time: 0.02068820409476757 Engine time: 0.11725227814167738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 183.95619167387486,
    "estimated_duration": 3600.1016460884325,
    "input_throughput": 6986.843004093928,
    "output_throughput": 6121.856038133271,
    "total_throughput": 13108.6990422272,
    "itl": 73.35556906890197,
    "ttft": 1115216.8116279477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42892552152276014,
    "arrivals": 129375,
    "finished_requests": 100677,
    "scheduler_time": 206.7613958184271
}
#Debug simulation 
Total elapsed time: 183.95631320867687. Arrivals time: 0.3798041380941868 Scheduler time: 183.27759860735387 Scheduler overhead time: 0.1171608860604465 Adapter cache time: 0.02062912844121456 Engine time: 0.1173705798573792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 188.45738345896825,
    "estimated_duration": 3600.075748336065,
    "input_throughput": 6926.255096583687,
    "output_throughput": 6169.909622114571,
    "total_throughput": 13096.164718698257,
    "itl": 74.31583751764022,
    "ttft": 1115230.7826008692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37950048024766103,
    "arrivals": 128882,
    "finished_requests": 100801,
    "scheduler_time": 204.68637157528002
}
#Debug simulation 
Total elapsed time: 188.4575043451041. Arrivals time: 0.3838847349397838 Scheduler time: 187.77398769557476 Scheduler overhead time: 0.11838354403153062 Adapter cache time: 0.0202028164640069 Engine time: 0.11756092961877584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 188.20105344476178,
    "estimated_duration": 3600.0749588383524,
    "input_throughput": 6925.677183134319,
    "output_throughput": 6171.312334887852,
    "total_throughput": 13096.989518022172,
    "itl": 74.31548789094606,
    "ttft": 1115032.9686968825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39829095209017396,
    "arrivals": 128882,
    "finished_requests": 100819,
    "scheduler_time": 204.69453242419098
}
#Debug simulation 
Total elapsed time: 188.2011748449877. Arrivals time: 0.38321997690945864 Scheduler time: 187.51660453528166 Scheduler overhead time: 0.1180706643499434 Adapter cache time: 0.020825869403779507 Engine time: 0.11892659682780504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 188.16811322793365,
    "estimated_duration": 3600.075774488703,
    "input_throughput": 6925.67561401984,
    "output_throughput": 6171.310936685874,
    "total_throughput": 13096.986550705715,
    "itl": 74.31551678149974,
    "ttft": 1115033.1870815544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39896359410137033,
    "arrivals": 128882,
    "finished_requests": 100819,
    "scheduler_time": 204.6945753939288
}
#Debug simulation 
Total elapsed time: 188.1682248711586. Arrivals time: 0.38769713742658496 Scheduler time: 187.4771467577666 Scheduler overhead time: 0.12013345258310437 Adapter cache time: 0.021295722108334303 Engine time: 0.11853516520932317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 188.7140670795925,
    "estimated_duration": 3600.058137164846,
    "input_throughput": 6925.709544134043,
    "output_throughput": 6171.341171033617,
    "total_throughput": 13097.050715167661,
    "itl": 74.31496992652492,
    "ttft": 1115030.2111964521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38112994871567957,
    "arrivals": 128882,
    "finished_requests": 100819,
    "scheduler_time": 204.69426743930205
}
#Debug simulation 
Total elapsed time: 188.7141917576082. Arrivals time: 0.38784423330798745 Scheduler time: 188.02394647803158 Scheduler overhead time: 0.11848252220079303 Adapter cache time: 0.020958662033081055 Engine time: 0.11898895539343357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 186.9568130960688,
    "estimated_duration": 3600.0989644856727,
    "input_throughput": 6923.999102774998,
    "output_throughput": 6170.538704391888,
    "total_throughput": 13094.537807166887,
    "itl": 74.32383338204468,
    "ttft": 1115344.1537076796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40728877197951113,
    "arrivals": 128882,
    "finished_requests": 100798,
    "scheduler_time": 204.68380949974596
}
#Debug simulation 
Total elapsed time: 186.95693128090352. Arrivals time: 0.38465929543599486 Scheduler time: 186.27371885208413 Scheduler overhead time: 0.11858964944258332 Adapter cache time: 0.02003139676526189 Engine time: 0.11675536865368485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 188.73612055415288,
    "estimated_duration": 3600.06579677494,
    "input_throughput": 6926.274242636801,
    "output_throughput": 6169.926677423058,
    "total_throughput": 13096.20092005986,
    "itl": 74.31558809871913,
    "ttft": 1115229.1164667804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3707662365678702,
    "arrivals": 128882,
    "finished_requests": 100801,
    "scheduler_time": 204.68615464362955
}
#Debug simulation 
Total elapsed time: 188.73628764087334. Arrivals time: 0.38476534839719534 Scheduler time: 188.0483990497887 Scheduler overhead time: 0.11942177405580878 Adapter cache time: 0.020830795168876648 Engine time: 0.11893994268029928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 190.27688471740112,
    "estimated_duration": 3600.1038839574762,
    "input_throughput": 6923.989641265151,
    "output_throughput": 6170.530272470991,
    "total_throughput": 13094.519913736141,
    "itl": 74.32390404390428,
    "ttft": 1115344.981585392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.412318923436105,
    "arrivals": 128882,
    "finished_requests": 100798,
    "scheduler_time": 204.6841047590232
}
#Debug simulation 
Total elapsed time: 190.27700095809996. Arrivals time: 0.390084411483258 Scheduler time: 189.58223753608763 Scheduler overhead time: 0.11993751861155033 Adapter cache time: 0.021485465578734875 Engine time: 0.11922572832554579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 189.728818369098,
    "estimated_duration": 3600.0102647970775,
    "input_throughput": 6984.288418804625,
    "output_throughput": 6137.9033321299485,
    "total_throughput": 13122.191750934573,
    "itl": 73.4338193846497,
    "ttft": 1113006.0125330687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39786340671125753,
    "arrivals": 128637,
    "finished_requests": 100741,
    "scheduler_time": 205.29839527244252
}
#Debug simulation 
Total elapsed time: 189.72895069094375. Arrivals time: 0.3976889019832015 Scheduler time: 189.02386852679774 Scheduler overhead time: 0.12240908201783895 Adapter cache time: 0.021506405901163816 Engine time: 0.11986369919031858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 190.15993615565822,
    "estimated_duration": 3600.0401450307077,
    "input_throughput": 6986.31181508268,
    "output_throughput": 6137.986552872598,
    "total_throughput": 13124.298367955278,
    "itl": 73.43118773647205,
    "ttft": 1113053.2049204418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4216728760651312,
    "arrivals": 128637,
    "finished_requests": 100753,
    "scheduler_time": 205.2990203998554
}
#Debug simulation 
Total elapsed time: 190.16005316376686. Arrivals time: 0.3926009680144489 Scheduler time: 189.45898031257093 Scheduler overhead time: 0.12239305069670081 Adapter cache time: 0.021728078834712505 Engine time: 0.11966053443029523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 190.4722236469388,
    "estimated_duration": 3600.0405110441075,
    "input_throughput": 6986.311104789635,
    "output_throughput": 6137.9859288281405,
    "total_throughput": 13124.297033617775,
    "itl": 73.43117835868266,
    "ttft": 1113053.2283316816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4222917110100399,
    "arrivals": 128637,
    "finished_requests": 100753,
    "scheduler_time": 205.29906769402766
}
#Debug simulation 
Total elapsed time: 190.4723420101218. Arrivals time: 0.38675316935405135 Scheduler time: 189.77677576290444 Scheduler overhead time: 0.12163784075528383 Adapter cache time: 0.021675950847566128 Engine time: 0.11987961316481233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 189.67692253319547,
    "estimated_duration": 3600.021417973856,
    "input_throughput": 6986.348157382727,
    "output_throughput": 6138.018482244616,
    "total_throughput": 13124.366639627344,
    "itl": 73.4306233076942,
    "ttft": 1113050.4435524282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40246889609843517,
    "arrivals": 128637,
    "finished_requests": 100753,
    "scheduler_time": 205.2987422011074
}
#Debug simulation 
Total elapsed time: 189.67704238416627. Arrivals time: 0.3884597672149539 Scheduler time: 188.9832554203458 Scheduler overhead time: 0.12093475414440036 Adapter cache time: 0.021723373793065548 Engine time: 0.11841251607984304 
