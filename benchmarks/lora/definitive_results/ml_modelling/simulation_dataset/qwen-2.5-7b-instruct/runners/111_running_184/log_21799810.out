INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 27.34289685683325,
    "estimated_duration": 3600.1904840587395,
    "input_throughput": 5331.9284312865275,
    "output_throughput": 4740.5227794390075,
    "total_throughput": 10072.451210725534,
    "itl": 181.80492626519674,
    "ttft": 2040648.8038733564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4591445439122468,
    "arrivals": 576087,
    "finished_requests": 77827,
    "scheduler_time": 107.52224285048223
}
#Debug simulation 
Total elapsed time: 27.343109054956585. Arrivals time: 0.35655861301347613 Scheduler time: 26.866274846252054 Scheduler overhead time: 0.043464180547744036 Adapter cache time: 0.014936836902052164 Engine time: 0.04482491360977292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.327453351113945,
    "estimated_duration": 3600.1348352878194,
    "input_throughput": 5323.901708383563,
    "output_throughput": 4729.028988893105,
    "total_throughput": 10052.930697276668,
    "itl": 179.42267449550377,
    "ttft": 2043963.2889913023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6539737275987851,
    "arrivals": 576087,
    "finished_requests": 77710,
    "scheduler_time": 107.84389313625974
}
#Debug simulation 
Total elapsed time: 26.327610615175217. Arrivals time: 0.4106725677847862 Scheduler time: 25.794899598229676 Scheduler overhead time: 0.044522515032440424 Adapter cache time: 0.014739835634827614 Engine time: 0.045797405764460564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 31.974054554942995,
    "estimated_duration": 3600.117256956043,
    "input_throughput": 5394.717342184928,
    "output_throughput": 4764.90730040948,
    "total_throughput": 10159.624642594408,
    "itl": 180.39483992343114,
    "ttft": 2039327.587790305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4720946048316697,
    "arrivals": 564621,
    "finished_requests": 78500,
    "scheduler_time": 108.14649408145863
}
#Debug simulation 
Total elapsed time: 31.974191872868687. Arrivals time: 0.3635771735571325 Scheduler time: 31.48473336547613 Scheduler overhead time: 0.04600839037448168 Adapter cache time: 0.015621720347553492 Engine time: 0.046398666221648455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 31.97514686966315,
    "estimated_duration": 3600.1541885323886,
    "input_throughput": 5392.981517804047,
    "output_throughput": 4765.701717623988,
    "total_throughput": 10158.683235428036,
    "itl": 180.45191041994664,
    "ttft": 2039119.431734549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.57698777093087,
    "arrivals": 564621,
    "finished_requests": 78503,
    "scheduler_time": 108.1432163635342
}
#Debug simulation 
Total elapsed time: 31.975262668915093. Arrivals time: 0.41840659733861685 Scheduler time: 31.42955441121012 Scheduler overhead time: 0.04718320118263364 Adapter cache time: 0.01598019478842616 Engine time: 0.04641873016953468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 28.27270251326263,
    "estimated_duration": 3600.188980464894,
    "input_throughput": 5371.0127731969915,
    "output_throughput": 4747.171354819565,
    "total_throughput": 10118.184128016557,
    "itl": 178.44596684395168,
    "ttft": 2039099.556717299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5823313110321855,
    "arrivals": 564621,
    "finished_requests": 78198,
    "scheduler_time": 108.29766047823084
}
#Debug simulation 
Total elapsed time: 28.272827350068837. Arrivals time: 0.4050283753313124 Scheduler time: 27.743388643022627 Scheduler overhead time: 0.04600792285054922 Adapter cache time: 0.015029843430966139 Engine time: 0.0460100625641644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 32.11288570286706,
    "estimated_duration": 3600.1488539594548,
    "input_throughput": 5394.669995003137,
    "output_throughput": 4764.865480807475,
    "total_throughput": 10159.535475810613,
    "itl": 180.39620482839007,
    "ttft": 2039336.8916511682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5035894427983971,
    "arrivals": 564621,
    "finished_requests": 78500,
    "scheduler_time": 108.14659624685117
}
#Debug simulation 
Total elapsed time: 32.11303513078019. Arrivals time: 0.3845867458730936 Scheduler time: 31.599838951136917 Scheduler overhead time: 0.04804323194548488 Adapter cache time: 0.01566544733941555 Engine time: 0.04719022335484624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 28.222662561573088,
    "estimated_duration": 3600.016104353213,
    "input_throughput": 5371.115139351291,
    "output_throughput": 4747.125430726477,
    "total_throughput": 10118.240570077769,
    "itl": 178.44592921606545,
    "ttft": 2039083.775878009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6034579471498769,
    "arrivals": 564621,
    "finished_requests": 78194,
    "scheduler_time": 108.29183538697241
}
#Debug simulation 
Total elapsed time: 28.222818359732628. Arrivals time: 0.4046386005356908 Scheduler time: 27.693066905252635 Scheduler overhead time: 0.045805225148797035 Adapter cache time: 0.015368647873401642 Engine time: 0.04622590122744441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 32.75379444099963,
    "estimated_duration": 3600.0882712193447,
    "input_throughput": 5394.760777191145,
    "output_throughput": 4764.945664565577,
    "total_throughput": 10159.706441756724,
    "itl": 180.39380699341538,
    "ttft": 2039319.803424211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4382141918479323,
    "arrivals": 564621,
    "finished_requests": 78500,
    "scheduler_time": 108.14645586227122
}
#Debug simulation 
Total elapsed time: 32.7540249209851. Arrivals time: 0.37707152124494314 Scheduler time: 32.24998432165012 Scheduler overhead time: 0.046765044797211885 Adapter cache time: 0.016066789627075195 Engine time: 0.046560849994421005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 28.327368957921863,
    "estimated_duration": 3600.002129270631,
    "input_throughput": 5371.135989832745,
    "output_throughput": 4747.143858901667,
    "total_throughput": 10118.279848734412,
    "itl": 178.453369537997,
    "ttft": 2039063.7041977458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.62307553783059,
    "arrivals": 564621,
    "finished_requests": 78194,
    "scheduler_time": 108.29037218183889
}
#Debug simulation 
Total elapsed time: 28.327504307962954. Arrivals time: 0.3707233667373657 Scheduler time: 27.832438200712204 Scheduler overhead time: 0.04582167882472277 Adapter cache time: 0.015538057778030634 Engine time: 0.04564602207392454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 28.56421609921381,
    "estimated_duration": 3600.088887558607,
    "input_throughput": 5407.531760473256,
    "output_throughput": 4784.908522545354,
    "total_throughput": 10192.44028301861,
    "itl": 179.93910820024306,
    "ttft": 2034048.6075723397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5180019209906626,
    "arrivals": 558945,
    "finished_requests": 78824,
    "scheduler_time": 108.50668662866374
}
#Debug simulation 
Total elapsed time: 28.564336278941482. Arrivals time: 0.46308394940569997 Scheduler time: 27.97599098365754 Scheduler overhead time: 0.04562307754531503 Adapter cache time: 0.01718965359032154 Engine time: 0.045386528596282005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 29.451857951004058,
    "estimated_duration": 3600.0418565989057,
    "input_throughput": 5419.043660350849,
    "output_throughput": 4790.74152107047,
    "total_throughput": 10209.785181421319,
    "itl": 179.66931338640953,
    "ttft": 2030576.8227694507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6259244785388052,
    "arrivals": 558945,
    "finished_requests": 78972,
    "scheduler_time": 108.63809237775892
}
#Debug simulation 
Total elapsed time: 29.451982683036476. Arrivals time: 0.4166308999992907 Scheduler time: 28.908101591281593 Scheduler overhead time: 0.04653448658064008 Adapter cache time: 0.01548884017392993 Engine time: 0.04700850090011954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 27.171993070282042,
    "estimated_duration": 3600.0510566597095,
    "input_throughput": 5405.432782404957,
    "output_throughput": 4782.688003312811,
    "total_throughput": 10188.120785717769,
    "itl": 177.4185029851302,
    "ttft": 2032169.1015428426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6482604004256527,
    "arrivals": 558945,
    "finished_requests": 78766,
    "scheduler_time": 109.02630833221639
}
#Debug simulation 
Total elapsed time: 27.172143457923084. Arrivals time: 0.40704183652997017 Scheduler time: 26.641500351019204 Scheduler overhead time: 0.04514237632974982 Adapter cache time: 0.015453275293111801 Engine time: 0.045773040037602186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 28.43032520916313,
    "estimated_duration": 3600.1170971453644,
    "input_throughput": 5409.185166627289,
    "output_throughput": 4782.921092664874,
    "total_throughput": 10192.106259292163,
    "itl": 179.81273754989442,
    "ttft": 2031977.161043208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5780809340393114,
    "arrivals": 558945,
    "finished_requests": 78893,
    "scheduler_time": 108.53190104839393
}
#Debug simulation 
Total elapsed time: 28.430542928166687. Arrivals time: 0.428095287643373 Scheduler time: 27.8762419084087 Scheduler overhead time: 0.04687177576124668 Adapter cache time: 0.01557286037132144 Engine time: 0.04602931719273329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 27.652992160990834,
    "estimated_duration": 3600.035389407156,
    "input_throughput": 5401.4644014936275,
    "output_throughput": 4779.734124456381,
    "total_throughput": 10181.198525950009,
    "itl": 177.50832336877966,
    "ttft": 2031812.3285589877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6834131170623046,
    "arrivals": 558945,
    "finished_requests": 78745,
    "scheduler_time": 108.97988997018932
}
#Debug simulation 
Total elapsed time: 27.653134774882346. Arrivals time: 0.4290736489929259 Scheduler time: 27.099632573779672 Scheduler overhead time: 0.045665977988392115 Adapter cache time: 0.015276908874511719 Engine time: 0.046121690422296524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 28.74896179884672,
    "estimated_duration": 3600.0538260669723,
    "input_throughput": 5407.5844252773795,
    "output_throughput": 4784.955123523629,
    "total_throughput": 10192.539548801007,
    "itl": 179.93761922157793,
    "ttft": 2034034.4326668747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4830649462714633,
    "arrivals": 558945,
    "finished_requests": 78824,
    "scheduler_time": 108.50656211168122
}
#Debug simulation 
Total elapsed time: 28.749066093936563. Arrivals time: 0.7214715834707022 Scheduler time: 27.905208375304937 Scheduler overhead time: 0.044876601081341505 Adapter cache time: 0.015263939741998911 Engine time: 0.04533429443836212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 28.43562995409593,
    "estimated_duration": 3600.197499539323,
    "input_throughput": 5401.256181775649,
    "output_throughput": 4779.352244481514,
    "total_throughput": 10180.608426257162,
    "itl": 177.84044475121613,
    "ttft": 2032373.2515829145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.703725500814615,
    "arrivals": 558945,
    "finished_requests": 78752,
    "scheduler_time": 108.88661201420655
}
#Debug simulation 
Total elapsed time: 28.43572765775025. Arrivals time: 0.41495059756562114 Scheduler time: 27.895040195435286 Scheduler overhead time: 0.046385691966861486 Adapter cache time: 0.015350142028182745 Engine time: 0.04659932991489768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 25.164291058201343,
    "estimated_duration": 3600.171884757177,
    "input_throughput": 5441.164374106333,
    "output_throughput": 4788.850519330925,
    "total_throughput": 10230.014893437257,
    "itl": 178.96171066037834,
    "ttft": 2025594.6070074423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3374331440986238,
    "arrivals": 556042,
    "finished_requests": 79064,
    "scheduler_time": 108.71136603522557
}
#Debug simulation 
Total elapsed time: 25.164421797264367. Arrivals time: 0.3749719080515206 Scheduler time: 24.67024282598868 Scheduler overhead time: 0.04408579785376787 Adapter cache time: 0.013857599813491106 Engine time: 0.04394868994131684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.55931205395609,
    "estimated_duration": 3600.192740367132,
    "input_throughput": 5460.849853831755,
    "output_throughput": 4801.112675494523,
    "total_throughput": 10261.962529326278,
    "itl": 178.36497987386252,
    "ttft": 2027406.1455875642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6430475896713377,
    "arrivals": 556042,
    "finished_requests": 79374,
    "scheduler_time": 109.01202732986647
}
#Debug simulation 
Total elapsed time: 25.5595329310745. Arrivals time: 0.36134094279259443 Scheduler time: 25.07808049954474 Scheduler overhead time: 0.04379645548760891 Adapter cache time: 0.01398404547944665 Engine time: 0.045423589646816254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 24.198825638275594,
    "estimated_duration": 3600.062015016948,
    "input_throughput": 5432.926410271275,
    "output_throughput": 4782.817887074354,
    "total_throughput": 10215.74429734563,
    "itl": 176.566712587054,
    "ttft": 2029014.8915375392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6330510975793102,
    "arrivals": 556042,
    "finished_requests": 78988,
    "scheduler_time": 109.12093164096719
}
#Debug simulation 
Total elapsed time: 24.19896396296099. Arrivals time: 0.3650668691843748 Scheduler time: 23.714473229832947 Scheduler overhead time: 0.043309671338647604 Adapter cache time: 0.014429835136979818 Engine time: 0.044493379071354866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 25.400823585223407,
    "estimated_duration": 3600.0274146412385,
    "input_throughput": 5461.100634968146,
    "output_throughput": 4801.3331592149925,
    "total_throughput": 10262.433794183138,
    "itl": 178.3585105986009,
    "ttft": 2027353.1387229513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5780809340393116,
    "arrivals": 556042,
    "finished_requests": 79374,
    "scheduler_time": 109.01088790860092
}
#Debug simulation 
Total elapsed time: 25.400923524051905. Arrivals time: 0.3555034948512912 Scheduler time: 24.923983663786203 Scheduler overhead time: 0.04353719810023904 Adapter cache time: 0.014545475132763386 Engine time: 0.045848002191632986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 27.47628291277215,
    "estimated_duration": 3600.1900602121264,
    "input_throughput": 5422.504277135231,
    "output_throughput": 4770.662579682812,
    "total_throughput": 10193.166856818043,
    "itl": 177.0247299120164,
    "ttft": 2026729.6438662175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.147792402207856,
    "arrivals": 556042,
    "finished_requests": 78796,
    "scheduler_time": 108.87961766086063
}
#Debug simulation 
Total elapsed time: 27.476408838760108. Arrivals time: 0.38333422457799315 Scheduler time: 26.97067081183195 Scheduler overhead time: 0.045444723684340715 Adapter cache time: 0.012826521880924702 Engine time: 0.04648849926888943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 25.329295228701085,
    "estimated_duration": 3600.16021936563,
    "input_throughput": 5443.262467761238,
    "output_throughput": 4788.399668234104,
    "total_throughput": 10231.662135995342,
    "itl": 178.91648521370237,
    "ttft": 2025781.429853689,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3126320794620454,
    "arrivals": 556042,
    "finished_requests": 79079,
    "scheduler_time": 108.71651020038068
}
#Debug simulation 
Total elapsed time: 25.32946031494066. Arrivals time: 0.3626509942114353 Scheduler time: 24.846989002544433 Scheduler overhead time: 0.04423679178580642 Adapter cache time: 0.013713710010051727 Engine time: 0.044560976792126894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.053564897738397,
    "estimated_duration": 3600.088263941685,
    "input_throughput": 5459.221985430287,
    "output_throughput": 4802.446699201277,
    "total_throughput": 10261.668684631564,
    "itl": 175.83322603638555,
    "ttft": 2029075.5722225287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6343236106634134,
    "arrivals": 556042,
    "finished_requests": 79341,
    "scheduler_time": 109.5621606339413
}
#Debug simulation 
Total elapsed time: 25.053702706005424. Arrivals time: 0.3702394487336278 Scheduler time: 24.563288796227425 Scheduler overhead time: 0.043358052615076303 Adapter cache time: 0.014518733602017164 Engine time: 0.04325384879484773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 36.91412154305726,
    "estimated_duration": 3600.018547730454,
    "input_throughput": 5420.0004086926,
    "output_throughput": 4770.0095908744015,
    "total_throughput": 10190.009999567,
    "itl": 179.59628650321574,
    "ttft": 2023215.4837069407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8752994947647664,
    "arrivals": 554638,
    "finished_requests": 78654,
    "scheduler_time": 108.33219105468923
}
#Debug simulation 
Total elapsed time: 36.914273529779166. Arrivals time: 0.41085668792948127 Scheduler time: 36.374908494297415 Scheduler overhead time: 0.04901176551356912 Adapter cache time: 0.01177820423617959 Engine time: 0.049544818233698606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.92181787500158,
    "estimated_duration": 3600.19178435146,
    "input_throughput": 5528.686579008339,
    "output_throughput": 4873.348157801144,
    "total_throughput": 10402.034736809483,
    "itl": 176.09151136046378,
    "ttft": 2042746.588416305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7148277428699719,
    "arrivals": 554638,
    "finished_requests": 80309,
    "scheduler_time": 110.58422683635456
}
#Debug simulation 
Total elapsed time: 26.921913993079215. Arrivals time: 0.34925514878705144 Scheduler time: 26.452835999894887 Scheduler overhead time: 0.04479108098894358 Adapter cache time: 0.013767635449767113 Engine time: 0.04424465587362647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 35.22083782684058,
    "estimated_duration": 3600.1304052787304,
    "input_throughput": 5401.413229778398,
    "output_throughput": 4748.925476402661,
    "total_throughput": 10150.338706181059,
    "itl": 177.83624859658505,
    "ttft": 2028745.8791718525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0548903044872042,
    "arrivals": 554638,
    "finished_requests": 78374,
    "scheduler_time": 108.38946767329988
}
#Debug simulation 
Total elapsed time: 35.22095556370914. Arrivals time: 0.36485186591744423 Scheduler time: 34.72891979571432 Scheduler overhead time: 0.0488761062733829 Adapter cache time: 0.01245923200622201 Engine time: 0.04742251429706812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 36.61706827627495,
    "estimated_duration": 3600.1741653875497,
    "input_throughput": 5433.915999976098,
    "output_throughput": 4783.084708943885,
    "total_throughput": 10217.000708919983,
    "itl": 179.32837773533578,
    "ttft": 2019546.5440996971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1753333256882625,
    "arrivals": 554638,
    "finished_requests": 78809,
    "scheduler_time": 108.53169215821958
}
#Debug simulation 
Total elapsed time: 36.617200437001884. Arrivals time: 0.3634335659444332 Scheduler time: 36.129334397614 Scheduler overhead time: 0.04721831111237407 Adapter cache time: 0.012406418099999428 Engine time: 0.04703419003635645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 28.532044966239482,
    "estimated_duration": 3600.171608819205,
    "input_throughput": 5395.286700338912,
    "output_throughput": 4754.3755853388175,
    "total_throughput": 10149.662285677729,
    "itl": 177.877984831185,
    "ttft": 2029953.7823816456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.314346812330192,
    "arrivals": 554638,
    "finished_requests": 78416,
    "scheduler_time": 108.46434979090844
}
#Debug simulation 
Total elapsed time: 28.532269050832838. Arrivals time: 0.35063503682613373 Scheduler time: 28.060771318152547 Scheduler overhead time: 0.04537958977743983 Adapter cache time: 0.012936986982822418 Engine time: 0.04551249509677291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 35.186154352966696,
    "estimated_duration": 3600.192410994994,
    "input_throughput": 5437.923801019651,
    "output_throughput": 4779.453716820783,
    "total_throughput": 10217.377517840434,
    "itl": 179.17168605398527,
    "ttft": 2020439.969247042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1302390114730192,
    "arrivals": 554638,
    "finished_requests": 78843,
    "scheduler_time": 108.5395581040862
}
#Debug simulation 
Total elapsed time: 35.18631787830964. Arrivals time: 0.3683145474642515 Scheduler time: 34.69381284667179 Scheduler overhead time: 0.0466628666035831 Adapter cache time: 0.012158641126006842 Engine time: 0.04739388870075345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 28.639304304029793,
    "estimated_duration": 3600.18834951402,
    "input_throughput": 5395.26161252702,
    "output_throughput": 4754.353477731386,
    "total_throughput": 10149.615090258405,
    "itl": 177.87859089770237,
    "ttft": 2029961.4035335318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3317008348554396,
    "arrivals": 554638,
    "finished_requests": 78416,
    "scheduler_time": 108.46438978288138
}
#Debug simulation 
Total elapsed time: 28.63947235001251. Arrivals time: 0.3634138535708189 Scheduler time: 28.154546299483627 Scheduler overhead time: 0.04555714223533869 Adapter cache time: 0.01286562904715538 Engine time: 0.04585694940760732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 29.861553870141506,
    "estimated_duration": 3600.156974180139,
    "input_throughput": 5529.32723288636,
    "output_throughput": 4871.1781530008675,
    "total_throughput": 10400.505385887227,
    "itl": 176.2327609773302,
    "ttft": 2027933.448452203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.199711195621645,
    "arrivals": 553841,
    "finished_requests": 80343,
    "scheduler_time": 110.56557462137997
}
#Debug simulation 
Total elapsed time: 29.86169006023556. Arrivals time: 0.3816915932111442 Scheduler time: 29.354459504131228 Scheduler overhead time: 0.047591245267540216 Adapter cache time: 0.012931706849485636 Engine time: 0.04696224955841899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 30.09373333491385,
    "estimated_duration": 3600.0758457937372,
    "input_throughput": 5547.751174002431,
    "output_throughput": 4887.292310954292,
    "total_throughput": 10435.043484956723,
    "itl": 175.70194817630326,
    "ttft": 2029599.8587852807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2818630270753106,
    "arrivals": 553841,
    "finished_requests": 80591,
    "scheduler_time": 110.88572673042722
}
#Debug simulation 
Total elapsed time: 30.093849128112197. Arrivals time: 0.40901937801390886 Scheduler time: 29.558107849676162 Scheduler overhead time: 0.047841756138950586 Adapter cache time: 0.01281863171607256 Engine time: 0.04828979168087244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 28.000142083968967,
    "estimated_duration": 3600.1249541836373,
    "input_throughput": 5494.651505640762,
    "output_throughput": 4839.660351164372,
    "total_throughput": 10334.311856805132,
    "itl": 175.20207047855658,
    "ttft": 2022957.6314589018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2195526956208118,
    "arrivals": 553841,
    "finished_requests": 79799,
    "scheduler_time": 110.3210516432547
}
#Debug simulation 
Total elapsed time: 28.0003202278167. Arrivals time: 0.3629576717503369 Scheduler time: 27.512290323153138 Scheduler overhead time: 0.047057271003723145 Adapter cache time: 0.01253792131319642 Engine time: 0.04750181408599019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 32.28420356102288,
    "estimated_duration": 3600.187705219543,
    "input_throughput": 5566.050617568567,
    "output_throughput": 4901.400550426004,
    "total_throughput": 10467.45116799457,
    "itl": 175.18993743563067,
    "ttft": 2031642.9788063017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1770975900278429,
    "arrivals": 553841,
    "finished_requests": 80855,
    "scheduler_time": 111.20040266457762
}
#Debug simulation 
Total elapsed time: 32.284323289990425. Arrivals time: 0.3673187419772148 Scheduler time: 31.787908374797553 Scheduler overhead time: 0.04887513257563114 Adapter cache time: 0.01309395907446742 Engine time: 0.049030694644898176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 27.882860012818128,
    "estimated_duration": 3600.1725702885897,
    "input_throughput": 5494.578833040307,
    "output_throughput": 4839.596341517413,
    "total_throughput": 10334.17517455772,
    "itl": 175.19326790421945,
    "ttft": 2022986.049699443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2345173962041782,
    "arrivals": 553841,
    "finished_requests": 79799,
    "scheduler_time": 110.32332945830143
}
#Debug simulation 
Total elapsed time: 27.883019906003028. Arrivals time: 0.3656075019389391 Scheduler time: 27.396127033047378 Scheduler overhead time: 0.04558686772361398 Adapter cache time: 0.012366644572466612 Engine time: 0.04579368885606527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 29.909214545972645,
    "estimated_duration": 3600.1434076639284,
    "input_throughput": 5529.689722254078,
    "output_throughput": 4871.605104025681,
    "total_throughput": 10401.294826279758,
    "itl": 176.21411920059495,
    "ttft": 2028064.0855763927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1750897658965502,
    "arrivals": 553841,
    "finished_requests": 80347,
    "scheduler_time": 110.57703671189635
}
#Debug simulation 
Total elapsed time: 29.90934226103127. Arrivals time: 0.4057595054619014 Scheduler time: 29.379564677365124 Scheduler overhead time: 0.046598415821790695 Adapter cache time: 0.012561275158077478 Engine time: 0.046922562178224325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 35.71794998832047,
    "estimated_duration": 3600.1640484382892,
    "input_throughput": 5482.869039971291,
    "output_throughput": 4828.334422021813,
    "total_throughput": 10311.203461993106,
    "itl": 175.53007364635621,
    "ttft": 2018290.227359091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8667841094732324,
    "arrivals": 553841,
    "finished_requests": 79682,
    "scheduler_time": 110.06570845340569
}
#Debug simulation 
Total elapsed time: 35.71810313221067. Arrivals time: 0.38589785899966955 Scheduler time: 35.20041827112436 Scheduler overhead time: 0.05106392642483115 Adapter cache time: 0.011393181513994932 Engine time: 0.050761859863996506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 14.057221285067499,
    "estimated_duration": 3600.113235199422,
    "input_throughput": 5347.4812435819385,
    "output_throughput": 4732.483643408577,
    "total_throughput": 10079.964886990516,
    "itl": 181.8183646734783,
    "ttft": 2005526.3995143427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6312399675161784,
    "arrivals": 484157,
    "finished_requests": 78139,
    "scheduler_time": 107.06180326730961
}
#Debug simulation 
Total elapsed time: 14.057389606256038. Arrivals time: 0.3530311565846205 Scheduler time: 13.600031888112426 Scheduler overhead time: 0.03668386861681938 Adapter cache time: 0.014642626978456974 Engine time: 0.03736234735697508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.201105639804155,
    "estimated_duration": 3600.0720449635855,
    "input_throughput": 5348.421298106205,
    "output_throughput": 4731.681418384587,
    "total_throughput": 10080.102716490792,
    "itl": 181.77294588995773,
    "ttft": 2005914.5862827238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 534,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7413296002056502,
    "arrivals": 484157,
    "finished_requests": 78138,
    "scheduler_time": 107.06086291791894
}
#Debug simulation 
Total elapsed time: 14.201248767785728. Arrivals time: 0.3295221542939544 Scheduler time: 13.766405601520091 Scheduler overhead time: 0.03703412367030978 Adapter cache time: 0.014906701631844044 Engine time: 0.03745807381346822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.002911218907684,
    "estimated_duration": 3600.1233161818855,
    "input_throughput": 5335.55028897503,
    "output_throughput": 4720.308863759334,
    "total_throughput": 10055.859152734363,
    "itl": 179.88648903537828,
    "ttft": 2007462.427363429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7652465238608528,
    "arrivals": 484157,
    "finished_requests": 77953,
    "scheduler_time": 107.29694307294308
}
#Debug simulation 
Total elapsed time: 14.003007933031768. Arrivals time: 0.367633359041065 Scheduler time: 13.529390719719231 Scheduler overhead time: 0.03727464657276869 Adapter cache time: 0.015137713868170977 Engine time: 0.038024081848561764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 14.27728765271604,
    "estimated_duration": 3600.0050891737496,
    "input_throughput": 5347.587440332882,
    "output_throughput": 4732.6127541420565,
    "total_throughput": 10080.200194474939,
    "itl": 181.82258816878397,
    "ttft": 2005503.4396021862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6647923925914718,
    "arrivals": 484157,
    "finished_requests": 78138,
    "scheduler_time": 107.05738337155152
}
#Debug simulation 
Total elapsed time: 14.277412063907832. Arrivals time: 0.364722796715796 Scheduler time: 13.807529215235263 Scheduler overhead time: 0.03699822537600994 Adapter cache time: 0.014791409019380808 Engine time: 0.03787263156846166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 13.859329241327941,
    "estimated_duration": 3600.1058908673917,
    "input_throughput": 5336.207762314296,
    "output_throughput": 4720.686145124836,
    "total_throughput": 10056.893907439131,
    "itl": 179.86682935222188,
    "ttft": 2007641.4503434494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7903769830614369,
    "arrivals": 484157,
    "finished_requests": 77972,
    "scheduler_time": 107.29632235437792
}
#Debug simulation 
Total elapsed time: 13.859426067210734. Arrivals time: 0.3163732383400202 Scheduler time: 13.437710640951991 Scheduler overhead time: 0.03719404013827443 Adapter cache time: 0.014919481705874205 Engine time: 0.037567551247775555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 14.122688747011125,
    "estimated_duration": 3600.073343726399,
    "input_throughput": 5347.62466257774,
    "output_throughput": 4732.598859323362,
    "total_throughput": 10080.223521901104,
    "itl": 181.81712367435597,
    "ttft": 2005504.4150316175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.59369680718284,
    "arrivals": 484157,
    "finished_requests": 78140,
    "scheduler_time": 107.06160646406596
}
#Debug simulation 
Total elapsed time: 14.122832543216646. Arrivals time: 0.3706358717754483 Scheduler time: 13.648049673996866 Scheduler overhead time: 0.03685214230790734 Adapter cache time: 0.014688482042402029 Engine time: 0.03715500375255942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 13.998826280236244,
    "estimated_duration": 3600.129475296384,
    "input_throughput": 5336.172804845704,
    "output_throughput": 4720.655219935075,
    "total_throughput": 10056.82802478078,
    "itl": 179.8675659504142,
    "ttft": 2007651.981865048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8127611570432751,
    "arrivals": 484157,
    "finished_requests": 77972,
    "scheduler_time": 107.29646083690555
}
#Debug simulation 
Total elapsed time: 13.99892221391201. Arrivals time: 0.3623239444568753 Scheduler time: 13.529961834196001 Scheduler overhead time: 0.03767102351412177 Adapter cache time: 0.015097588300704956 Engine time: 0.0380179681815207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 12.523940594866872,
    "estimated_duration": 3600.0884827935433,
    "input_throughput": 5375.673984819068,
    "output_throughput": 4728.694331087715,
    "total_throughput": 10104.368315906782,
    "itl": 181.38903163599022,
    "ttft": 2004567.8047268447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6373609430040441,
    "arrivals": 472692,
    "finished_requests": 78046,
    "scheduler_time": 107.10451013229363
}
#Debug simulation 
Total elapsed time: 12.52408242970705. Arrivals time: 0.36004353780299425 Scheduler time: 12.062165741808712 Scheduler overhead time: 0.035840671975165606 Adapter cache time: 0.014199313707649708 Engine time: 0.036388800013810396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 12.430970095098019,
    "estimated_duration": 3600.09463679398,
    "input_throughput": 5374.188167794876,
    "output_throughput": 4727.080456739081,
    "total_throughput": 10101.268624533956,
    "itl": 181.40105168056195,
    "ttft": 2004251.2479476682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7803490832261808,
    "arrivals": 472692,
    "finished_requests": 78023,
    "scheduler_time": 107.09880021734915
}
#Debug simulation 
Total elapsed time: 12.431066330056638. Arrivals time: 0.34976485650986433 Scheduler time: 11.979285712819546 Scheduler overhead time: 0.0355755016207695 Adapter cache time: 0.01465247105807066 Engine time: 0.03620762377977371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 12.266468042042106,
    "estimated_duration": 3600.117666352695,
    "input_throughput": 5360.891167635864,
    "output_throughput": 4715.446986264389,
    "total_throughput": 10076.338153900253,
    "itl": 179.3566969191107,
    "ttft": 2006406.9721513952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.819001537859451,
    "arrivals": 472692,
    "finished_requests": 77802,
    "scheduler_time": 107.34581098196452
}
#Debug simulation 
Total elapsed time: 12.266587396152318. Arrivals time: 0.3631205898709595 Scheduler time: 11.800816235598177 Scheduler overhead time: 0.035549502819776535 Adapter cache time: 0.01480890717357397 Engine time: 0.03661443293094635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 12.45945022907108,
    "estimated_duration": 3600.0939741523553,
    "input_throughput": 5371.419507056911,
    "output_throughput": 4726.645782630304,
    "total_throughput": 10098.065289687214,
    "itl": 181.44566228997314,
    "ttft": 2004296.937742553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.713729100199408,
    "arrivals": 472692,
    "finished_requests": 77984,
    "scheduler_time": 107.09702190819357
}
#Debug simulation 
Total elapsed time: 12.45960728591308. Arrivals time: 0.35486189741641283 Scheduler time: 12.002593465615064 Scheduler overhead time: 0.035454601515084505 Adapter cache time: 0.014890908729285002 Engine time: 0.036271268501877785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 12.348286712076515,
    "estimated_duration": 3600.108633809112,
    "input_throughput": 5361.488211420302,
    "output_throughput": 4715.033829976377,
    "total_throughput": 10076.522041396678,
    "itl": 179.48767709700493,
    "ttft": 2006207.6930282502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.82599092556164,
    "arrivals": 472692,
    "finished_requests": 77816,
    "scheduler_time": 107.32815068846938
}
#Debug simulation 
Total elapsed time: 12.348407722078264. Arrivals time: 0.3549044937826693 Scheduler time: 11.891733489464968 Scheduler overhead time: 0.035745942033827305 Adapter cache time: 0.014518883544951677 Engine time: 0.0359836146235466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 12.534534541890025,
    "estimated_duration": 3600.049558343938,
    "input_throughput": 5375.732107672025,
    "output_throughput": 4728.745458668379,
    "total_throughput": 10104.477566340403,
    "itl": 181.3874909543247,
    "ttft": 2004551.1046853615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.599676907772644,
    "arrivals": 472692,
    "finished_requests": 78046,
    "scheduler_time": 107.10435333714315
}
#Debug simulation 
Total elapsed time: 12.534637422300875. Arrivals time: 0.3534402451477945 Scheduler time: 12.080047797877342 Scheduler overhead time: 0.035168048925697803 Adapter cache time: 0.01428051944822073 Engine time: 0.036208047065883875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 12.304516365285963,
    "estimated_duration": 3600.13851383552,
    "input_throughput": 5362.484506028653,
    "output_throughput": 4715.444401586044,
    "total_throughput": 10077.928907614698,
    "itl": 179.43939312017542,
    "ttft": 2006529.6756924265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 555,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8632414427027064,
    "arrivals": 472692,
    "finished_requests": 77819,
    "scheduler_time": 107.3357729191961
}
#Debug simulation 
Total elapsed time: 12.304663937073201. Arrivals time: 0.352455148473382 Scheduler time: 11.849307767581195 Scheduler overhead time: 0.035926108714193106 Adapter cache time: 0.014695438090711832 Engine time: 0.036532269325107336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 11.311063033062965,
    "estimated_duration": 3600.13155266303,
    "input_throughput": 5386.869261945332,
    "output_throughput": 4726.267846355172,
    "total_throughput": 10113.137108300503,
    "itl": 181.28068154543737,
    "ttft": 1999782.5431243896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6098165533086484,
    "arrivals": 466835,
    "finished_requests": 78286,
    "scheduler_time": 107.10078333672244
}
#Debug simulation 
Total elapsed time: 11.311161589808762. Arrivals time: 0.2978707286529243 Scheduler time: 10.914074883796275 Scheduler overhead time: 0.03499543573707342 Adapter cache time: 0.013993734959512949 Engine time: 0.03501320816576481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 11.452150254976004,
    "estimated_duration": 3600.000138567338,
    "input_throughput": 5386.5011815574635,
    "output_throughput": 4725.328707006605,
    "total_throughput": 10111.829888564069,
    "itl": 181.26771831490535,
    "ttft": 1999799.7378039642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6858743136236505,
    "arrivals": 466835,
    "finished_requests": 78269,
    "scheduler_time": 107.09589739371327
}
#Debug simulation 
Total elapsed time: 11.452324379235506. Arrivals time: 0.29920039093121886 Scheduler time: 11.053008338902146 Scheduler overhead time: 0.034881073981523514 Adapter cache time: 0.014039034955203533 Engine time: 0.035551125183701515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 11.427041363436729,
    "estimated_duration": 3600.0619527635695,
    "input_throughput": 5374.126960550839,
    "output_throughput": 4718.004085168997,
    "total_throughput": 10092.131045719836,
    "itl": 179.7879787000644,
    "ttft": 2000730.7087032225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7037099432945355,
    "arrivals": 466835,
    "finished_requests": 78114,
    "scheduler_time": 107.27522117405509
}
#Debug simulation 
Total elapsed time: 11.42713604401797. Arrivals time: 0.302607212215662 Scheduler time: 11.023706339299679 Scheduler overhead time: 0.03543897904455662 Adapter cache time: 0.014029819518327713 Engine time: 0.03581026429310441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 11.495219537988305,
    "estimated_duration": 3600.168011438214,
    "input_throughput": 5386.814709309249,
    "output_throughput": 4726.219983606456,
    "total_throughput": 10113.034692915704,
    "itl": 181.28178681745206,
    "ttft": 1999796.9323323101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.641819063934955,
    "arrivals": 466835,
    "finished_requests": 78286,
    "scheduler_time": 107.10099448223616
}
#Debug simulation 
Total elapsed time: 11.495335838291794. Arrivals time: 0.3326256941072643 Scheduler time: 11.062773086130619 Scheduler overhead time: 0.03504040837287903 Adapter cache time: 0.013774868566542864 Engine time: 0.03574441699311137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 11.40684745600447,
    "estimated_duration": 3600.087454265116,
    "input_throughput": 5374.088892501455,
    "output_throughput": 4717.970664817409,
    "total_throughput": 10092.059557318864,
    "itl": 179.7887121376664,
    "ttft": 2000742.2628237263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7263456248492037,
    "arrivals": 466835,
    "finished_requests": 78114,
    "scheduler_time": 107.27534334991171
}
#Debug simulation 
Total elapsed time: 11.406942979898304. Arrivals time: 0.2982736895792186 Scheduler time: 11.008405023254454 Scheduler overhead time: 0.0354034430347383 Adapter cache time: 0.014068112708628178 Engine time: 0.03543814457952976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 11.487403604667634,
    "estimated_duration": 3600.098448045519,
    "input_throughput": 5386.804910995811,
    "output_throughput": 4725.508273040677,
    "total_throughput": 10112.313184036488,
    "itl": 181.26003442802974,
    "ttft": 1999818.0514064343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5428659521695047,
    "arrivals": 466835,
    "finished_requests": 78274,
    "scheduler_time": 107.10218913262634
}
#Debug simulation 
Total elapsed time: 11.487519043963403. Arrivals time: 0.31137716909870505 Scheduler time: 11.076918279752135 Scheduler overhead time: 0.03442784072831273 Adapter cache time: 0.013720894232392311 Engine time: 0.03563812701031566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 11.487355220131576,
    "estimated_duration": 3600.1132237592296,
    "input_throughput": 5374.050424946833,
    "output_throughput": 4717.936893735857,
    "total_throughput": 10091.98731868269,
    "itl": 179.78734829361102,
    "ttft": 2000753.7726184987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.747723768539724,
    "arrivals": 466835,
    "finished_requests": 78114,
    "scheduler_time": 107.2760717836914
}
#Debug simulation 
Total elapsed time: 11.48754125693813. Arrivals time: 0.3037886512465775 Scheduler time: 11.082475624978542 Scheduler overhead time: 0.0352833797223866 Adapter cache time: 0.014379091560840607 Engine time: 0.03595372708514333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.540126263163984,
    "estimated_duration": 3600.096669620193,
    "input_throughput": 5397.596449000366,
    "output_throughput": 4726.320030121605,
    "total_throughput": 10123.916479121972,
    "itl": 181.09507511158338,
    "ttft": 2000802.8420131365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.530243871966394,
    "arrivals": 463927,
    "finished_requests": 78054,
    "scheduler_time": 107.10132972364912
}
#Debug simulation 
Total elapsed time: 10.540225805249065. Arrivals time: 0.3397815893404186 Scheduler time: 10.101696111261845 Scheduler overhead time: 0.03583762189373374 Adapter cache time: 0.013199402950704098 Engine time: 0.034460791386663914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.788842659909278,
    "estimated_duration": 3600.1217740588136,
    "input_throughput": 5398.655995485359,
    "output_throughput": 4725.272384556315,
    "total_throughput": 10123.928380041674,
    "itl": 181.0479546699359,
    "ttft": 2000985.9618272465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6051240095403103,
    "arrivals": 463927,
    "finished_requests": 78064,
    "scheduler_time": 107.1023749136658
}
#Debug simulation 
Total elapsed time: 10.78896554093808. Arrivals time: 0.29511343454942107 Scheduler time: 10.395924584474415 Scheduler overhead time: 0.034455185290426016 Adapter cache time: 0.013147465884685516 Engine time: 0.0349718201905489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.52001107018441,
    "estimated_duration": 3600.0947070255274,
    "input_throughput": 5388.317413468104,
    "output_throughput": 4718.018658468465,
    "total_throughput": 10106.336071936568,
    "itl": 179.70813725541475,
    "ttft": 2001956.4810233281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6414988677390019,
    "arrivals": 463927,
    "finished_requests": 77919,
    "scheduler_time": 107.2648691749409
}
#Debug simulation 
Total elapsed time: 10.520107372198254. Arrivals time: 0.3424263843335211 Scheduler time: 10.078830420970917 Scheduler overhead time: 0.03453916497528553 Adapter cache time: 0.013432624284178019 Engine time: 0.0355134760029614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.55639193719253,
    "estimated_duration": 3600.1295362061564,
    "input_throughput": 5397.547172837967,
    "output_throughput": 4726.276882228731,
    "total_throughput": 10123.824055066698,
    "itl": 181.09644445182536,
    "ttft": 2000817.0131681594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5616261843568582,
    "arrivals": 463927,
    "finished_requests": 78054,
    "scheduler_time": 107.10150090121803
}
#Debug simulation 
Total elapsed time: 10.55654569203034. Arrivals time: 0.29113563895225525 Scheduler time: 10.16668165801093 Scheduler overhead time: 0.034488923847675323 Adapter cache time: 0.013431440107524395 Engine time: 0.03539553843438625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 10.500868516974151,
    "estimated_duration": 3600.0340091530984,
    "input_throughput": 5389.099644801421,
    "output_throughput": 4718.058484118524,
    "total_throughput": 10107.158128919946,
    "itl": 179.69426527218346,
    "ttft": 2002058.8701936076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6674753017723598,
    "arrivals": 463927,
    "finished_requests": 77924,
    "scheduler_time": 107.26289344568153
}
#Debug simulation 
Total elapsed time: 10.500963394064456. Arrivals time: 0.29157787980511785 Scheduler time: 10.111035387497395 Scheduler overhead time: 0.03417155612260103 Adapter cache time: 0.013592465315014124 Engine time: 0.035161941312253475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.565457216929644,
    "estimated_duration": 3600.0587268756835,
    "input_throughput": 5397.653336856529,
    "output_throughput": 4726.369843073831,
    "total_throughput": 10124.02317993036,
    "itl": 181.09373378686044,
    "ttft": 2000787.022755976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 500,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4950251474510716,
    "arrivals": 463927,
    "finished_requests": 78054,
    "scheduler_time": 107.10115322474263
}
#Debug simulation 
Total elapsed time: 10.565578519832343. Arrivals time: 0.3352054785937071 Scheduler time: 10.131955137941986 Scheduler overhead time: 0.03467808151617646 Adapter cache time: 0.013486187905073166 Engine time: 0.03493022918701172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.489470524713397,
    "estimated_duration": 3600.0552104184,
    "input_throughput": 5389.06790758501,
    "output_throughput": 4718.030698764194,
    "total_throughput": 10107.098606349204,
    "itl": 179.69496437749456,
    "ttft": 2002068.3013293038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.687847415171562,
    "arrivals": 463927,
    "finished_requests": 77924,
    "scheduler_time": 107.26298741549108
}
#Debug simulation 
Total elapsed time: 10.489569587167352. Arrivals time: 0.3397186272777617 Scheduler time: 10.050947351846844 Scheduler overhead time: 0.03458334458991885 Adapter cache time: 0.013550783507525921 Engine time: 0.03537714388221502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.718768888153136,
    "estimated_duration": 3600.1126523053977,
    "input_throughput": 5380.358025073503,
    "output_throughput": 4728.592031446215,
    "total_throughput": 10108.950056519718,
    "itl": 181.05043272612755,
    "ttft": 2001411.4219208462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3649775337940195,
    "arrivals": 462544,
    "finished_requests": 78166,
    "scheduler_time": 107.08206724434962
}
#Debug simulation 
Total elapsed time: 9.718880069907755. Arrivals time: 0.2941535972058773 Scheduler time: 9.328597352840006 Scheduler overhead time: 0.03399376943707466 Adapter cache time: 0.012583020608872175 Engine time: 0.034322742372751236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.766665078233927,
    "estimated_duration": 3600.1138408407896,
    "input_throughput": 5380.229863919069,
    "output_throughput": 4728.442141714156,
    "total_throughput": 10108.672005633225,
    "itl": 181.05326852304978,
    "ttft": 2001415.3993113595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4549152411031594,
    "arrivals": 462544,
    "finished_requests": 78165,
    "scheduler_time": 107.08004661658023
}
#Debug simulation 
Total elapsed time: 9.766766692046076. Arrivals time: 0.2983567197807133 Scheduler time: 9.371728472411633 Scheduler overhead time: 0.033744275104254484 Adapter cache time: 0.01266987668350339 Engine time: 0.03502531908452511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.609059568028897,
    "estimated_duration": 3600.019011015105,
    "input_throughput": 5367.841097747724,
    "output_throughput": 4718.442027118708,
    "total_throughput": 10086.283124866433,
    "itl": 179.37421373572644,
    "ttft": 2003006.3898677698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.450135884750643,
    "arrivals": 462544,
    "finished_requests": 77996,
    "scheduler_time": 107.28633200720328
}
#Debug simulation 
Total elapsed time: 9.60922825615853. Arrivals time: 0.3103710375726223 Scheduler time: 9.202792300842702 Scheduler overhead time: 0.033734322525560856 Adapter cache time: 0.012389061506837606 Engine time: 0.03448123671114445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 9.702609054278582,
    "estimated_duration": 3600.1511173027034,
    "input_throughput": 5380.300539859634,
    "output_throughput": 4728.5415098781405,
    "total_throughput": 10108.842049737776,
    "itl": 181.05159178773042,
    "ttft": 2001424.2240335497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.393625943337099,
    "arrivals": 462544,
    "finished_requests": 78166,
    "scheduler_time": 107.08245477653637
}
#Debug simulation 
Total elapsed time: 9.702705349307507. Arrivals time: 0.28994501242414117 Scheduler time: 9.317761330865324 Scheduler overhead time: 0.03331915661692619 Adapter cache time: 0.012471785768866539 Engine time: 0.033981917425990105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.905969645362347,
    "estimated_duration": 3600.0397241003184,
    "input_throughput": 5367.810213491275,
    "output_throughput": 4718.414879226109,
    "total_throughput": 10086.225092717385,
    "itl": 179.3747722336802,
    "ttft": 2003016.1162479937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4691247064992834,
    "arrivals": 462544,
    "finished_requests": 77996,
    "scheduler_time": 107.28645619344984
}
#Debug simulation 
Total elapsed time: 9.90607697237283. Arrivals time: 0.5758577850647271 Scheduler time: 9.233297958504409 Scheduler overhead time: 0.034389890264719725 Adapter cache time: 0.01250215107575059 Engine time: 0.03475563041865826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.773446949664503,
    "estimated_duration": 3600.017348720806,
    "input_throughput": 5380.374071497889,
    "output_throughput": 4728.568879271862,
    "total_throughput": 10108.942950769751,
    "itl": 181.04749863827664,
    "ttft": 2001376.2639637794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.33356243152636,
    "arrivals": 462544,
    "finished_requests": 78165,
    "scheduler_time": 107.07991711599014
}
#Debug simulation 
Total elapsed time: 9.773542752955109. Arrivals time: 0.2974668857641518 Scheduler time: 9.379912238568068 Scheduler overhead time: 0.0337640754878521 Adapter cache time: 0.01258425461128354 Engine time: 0.034464198630303144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.642444086726755,
    "estimated_duration": 3600.058284458802,
    "input_throughput": 5367.782539361035,
    "output_throughput": 4718.390553100055,
    "total_throughput": 10086.17309246109,
    "itl": 179.37545556818475,
    "ttft": 2003024.4994225635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4869817441701891,
    "arrivals": 462544,
    "finished_requests": 77996,
    "scheduler_time": 107.28653893448094
}
#Debug simulation 
Total elapsed time: 9.642568669747561. Arrivals time: 0.309287341311574 Scheduler time: 9.236332467757165 Scheduler overhead time: 0.034218238200992346 Adapter cache time: 0.012338079512119293 Engine time: 0.03499267343431711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.8231968479231,
    "estimated_duration": 3600.175304586013,
    "input_throughput": 5343.87616500033,
    "output_throughput": 4730.472146260765,
    "total_throughput": 10074.348311261096,
    "itl": 181.85586853494405,
    "ttft": 1996922.9205191573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2639814382442351,
    "arrivals": 461815,
    "finished_requests": 77912,
    "scheduler_time": 106.99114983225587
}
#Debug simulation 
Total elapsed time: 9.82335346005857. Arrivals time: 0.28771613352000713 Scheduler time: 9.440065167378634 Scheduler overhead time: 0.03364519448950887 Adapter cache time: 0.011849611066281796 Engine time: 0.03480434184893966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.881472245324403,
    "estimated_duration": 3600.1316425728214,
    "input_throughput": 5343.790702678689,
    "output_throughput": 4730.166196875547,
    "total_throughput": 10073.956899554236,
    "itl": 181.85825858108387,
    "ttft": 1996940.314058194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3472544843657004,
    "arrivals": 461815,
    "finished_requests": 77909,
    "scheduler_time": 106.98779248743148
}
#Debug simulation 
Total elapsed time: 9.881595909129828. Arrivals time: 0.33467518351972103 Scheduler time: 9.437790263444185 Scheduler overhead time: 0.03419427806511521 Adapter cache time: 0.011831812560558319 Engine time: 0.04787747794762254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.541282712947577,
    "estimated_duration": 3600.019836096231,
    "input_throughput": 5330.902571028779,
    "output_throughput": 4718.988164915736,
    "total_throughput": 10049.890735944515,
    "itl": 179.65533784922607,
    "ttft": 1998959.0929725366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4251132389716887,
    "arrivals": 461815,
    "finished_requests": 77722,
    "scheduler_time": 107.25846407917503
}
#Debug simulation 
Total elapsed time: 9.541355757042766. Arrivals time: 0.6099357577040792 Scheduler time: 8.835931058973074 Scheduler overhead time: 0.03386929677799344 Adapter cache time: 0.012173665221780539 Engine time: 0.03415222326293588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 9.896789356134832,
    "estimated_duration": 3600.015506657358,
    "input_throughput": 5343.6417049941765,
    "output_throughput": 4729.894348652499,
    "total_throughput": 10073.536053646676,
    "itl": 181.85409470424833,
    "ttft": 1996958.4478861506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2916855210578064,
    "arrivals": 461815,
    "finished_requests": 77906,
    "scheduler_time": 106.98563020481849
}
#Debug simulation 
Total elapsed time: 9.896925921086222. Arrivals time: 0.34045882569625974 Scheduler time: 9.460777665954083 Scheduler overhead time: 0.033923450857400894 Adapter cache time: 0.011799964588135481 Engine time: 0.03466320736333728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.290227496996522,
    "estimated_duration": 3600.0356528556267,
    "input_throughput": 5330.879149704253,
    "output_throughput": 4718.967432037622,
    "total_throughput": 10049.846581741875,
    "itl": 179.65511763404143,
    "ttft": 1998965.2890036749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.443221784215425,
    "arrivals": 461815,
    "finished_requests": 77722,
    "scheduler_time": 107.25866905695457
}
#Debug simulation 
Total elapsed time: 9.290324097964913. Arrivals time: 0.3313306733034551 Scheduler time: 8.863088330253959 Scheduler overhead time: 0.033947918098419905 Adapter cache time: 0.011954769492149353 Engine time: 0.034642210230231285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.897522576153278,
    "estimated_duration": 3600.117667297434,
    "input_throughput": 5343.96171957413,
    "output_throughput": 4730.54788033765,
    "total_throughput": 10074.50959991178,
    "itl": 181.8541585801435,
    "ttft": 1996910.6190896684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2348907717945916,
    "arrivals": 461815,
    "finished_requests": 77912,
    "scheduler_time": 106.99014486039201
}
#Debug simulation 
Total elapsed time: 9.897696217056364. Arrivals time: 0.3424517414532602 Scheduler time: 9.459703804925084 Scheduler overhead time: 0.03372061438858509 Adapter cache time: 0.011951652355492115 Engine time: 0.03459350299090147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.251976137980819,
    "estimated_duration": 3600.0203284953227,
    "input_throughput": 5330.789620296705,
    "output_throughput": 4719.568071745147,
    "total_throughput": 10050.35769204185,
    "itl": 179.69719905846705,
    "ttft": 1998752.2743798625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4717641499266034,
    "arrivals": 461815,
    "finished_requests": 77716,
    "scheduler_time": 107.25291868984085
}
#Debug simulation 
Total elapsed time: 9.25209575612098. Arrivals time: 0.28239764692261815 Scheduler time: 8.874326155986637 Scheduler overhead time: 0.03350504534319043 Adapter cache time: 0.012008384335786104 Engine time: 0.03445722768083215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.061203211080283,
    "estimated_duration": 3600.142091309731,
    "input_throughput": 5353.093714417501,
    "output_throughput": 4726.260399852682,
    "total_throughput": 10079.354114270183,
    "itl": 181.85200923499497,
    "ttft": 1972997.079016625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5501657829620816,
    "arrivals": 403594,
    "finished_requests": 77624,
    "scheduler_time": 106.69796226611113
}
#Debug simulation 
Total elapsed time: 7.061298680957407. Arrivals time: 0.27315526036545634 Scheduler time: 6.683195790741593 Scheduler overhead time: 0.0341939190402627 Adapter cache time: 0.02209888119250536 Engine time: 0.033468852285295725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.013785214163363,
    "estimated_duration": 3600.2018174830987,
    "input_throughput": 5352.509936087899,
    "output_throughput": 4725.82840144624,
    "total_throughput": 10078.33833753414,
    "itl": 181.86249732175006,
    "ttft": 1973146.627792326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7939599816966876,
    "arrivals": 403594,
    "finished_requests": 77618,
    "scheduler_time": 106.6932516730995
}
#Debug simulation 
Total elapsed time: 7.013905085157603. Arrivals time: 0.26608373410999775 Scheduler time: 6.6444245292805135 Scheduler overhead time: 0.03280423069372773 Adapter cache time: 0.022265552077442408 Engine time: 0.03335810313001275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.989739635027945,
    "estimated_duration": 3600.024166903957,
    "input_throughput": 5336.54362007302,
    "output_throughput": 4712.54336456031,
    "total_throughput": 10049.08698463333,
    "itl": 179.5243360793898,
    "ttft": 1975465.155257588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8357986352964657,
    "arrivals": 403594,
    "finished_requests": 77398,
    "scheduler_time": 106.96819342227722
}
#Debug simulation 
Total elapsed time: 6.9898367607966065. Arrivals time: 0.28660764591768384 Scheduler time: 6.598691378720105 Scheduler overhead time: 0.0334517820738256 Adapter cache time: 0.022381381131708622 Engine time: 0.03359602391719818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.020262413658202,
    "estimated_duration": 3600.023467327245,
    "input_throughput": 5353.013160857891,
    "output_throughput": 4726.263357564207,
    "total_throughput": 10079.276518422097,
    "itl": 181.8545344003411,
    "ttft": 1972976.6505321404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.622498777138493,
    "arrivals": 403594,
    "finished_requests": 77620,
    "scheduler_time": 106.69239206491716
}
#Debug simulation 
Total elapsed time: 7.0204084208235145. Arrivals time: 0.27046261029317975 Scheduler time: 6.646652614697814 Scheduler overhead time: 0.03286624141037464 Adapter cache time: 0.022020974662154913 Engine time: 0.03330990858376026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.314605331048369,
    "estimated_duration": 3600.187411891139,
    "input_throughput": 5338.107382000121,
    "output_throughput": 4713.770995350935,
    "total_throughput": 10051.878377351055,
    "itl": 179.5287969487381,
    "ttft": 1975380.4775511716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8798816461861128,
    "arrivals": 403594,
    "finished_requests": 77413,
    "scheduler_time": 106.97254242783795
}
#Debug simulation 
Total elapsed time: 7.3146857800893486. Arrivals time: 0.5538450488820672 Scheduler time: 6.655822028405964 Scheduler overhead time: 0.033212408889085054 Adapter cache time: 0.022874000947922468 Engine time: 0.0336895901709795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.0754812262021005,
    "estimated_duration": 3600.154570552056,
    "input_throughput": 5353.127656695244,
    "output_throughput": 4726.375678195054,
    "total_throughput": 10079.503334890298,
    "itl": 181.84636007234354,
    "ttft": 1973013.2916216285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4744384426762416,
    "arrivals": 403594,
    "finished_requests": 77625,
    "scheduler_time": 106.70024316781648
}
#Debug simulation 
Total elapsed time: 7.075581901241094. Arrivals time: 0.27244223933666945 Scheduler time: 6.698898912407458 Scheduler overhead time: 0.03316378267481923 Adapter cache time: 0.022401390131562948 Engine time: 0.03357701143249869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.027412751223892,
    "estimated_duration": 3600.039077548505,
    "input_throughput": 5337.9326129665005,
    "output_throughput": 4713.735777433755,
    "total_throughput": 10051.668390400255,
    "itl": 179.53214545917336,
    "ttft": 1975355.5787216981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9274165774509897,
    "arrivals": 403594,
    "finished_requests": 77406,
    "scheduler_time": 106.96670759462245
}
#Debug simulation 
Total elapsed time: 7.027525802142918. Arrivals time: 0.2957530962303281 Scheduler time: 6.627359052654356 Scheduler overhead time: 0.03310413798317313 Adapter cache time: 0.022374799009412527 Engine time: 0.03372065257281065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.44265843834728,
    "estimated_duration": 3600.1582763860015,
    "input_throughput": 5382.338917457752,
    "output_throughput": 4725.626679135454,
    "total_throughput": 10107.965596593207,
    "itl": 181.3412728823928,
    "ttft": 1965077.9724330693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.055146260711004,
    "arrivals": 397773,
    "finished_requests": 78149,
    "scheduler_time": 106.73438457126727
}
#Debug simulation 
Total elapsed time: 6.442754339426756. Arrivals time: 0.26237253565341234 Scheduler time: 6.075326585676521 Scheduler overhead time: 0.03276539267972112 Adapter cache time: 0.024036585353314877 Engine time: 0.03327165078371763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.468376171775162,
    "estimated_duration": 3600.1197295001443,
    "input_throughput": 5381.599628824018,
    "output_throughput": 4724.812305717878,
    "total_throughput": 10106.411934541895,
    "itl": 181.35329843318286,
    "ttft": 1965185.742626692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.328735236704793,
    "arrivals": 397773,
    "finished_requests": 78138,
    "scheduler_time": 106.72554817872353
}
#Debug simulation 
Total elapsed time: 6.4685214180499315. Arrivals time: 0.26451464323326945 Scheduler time: 6.099309337791055 Scheduler overhead time: 0.03222843864932656 Adapter cache time: 0.024313491769135 Engine time: 0.033103812020272017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.433557132724673,
    "estimated_duration": 3600.12779695397,
    "input_throughput": 5368.162490329255,
    "output_throughput": 4713.231573155998,
    "total_throughput": 10081.394063485253,
    "itl": 179.50736063785357,
    "ttft": 1966894.117742517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.420038174688757,
    "arrivals": 397773,
    "finished_requests": 77939,
    "scheduler_time": 106.93447297348595
}
#Debug simulation 
Total elapsed time: 6.433679627720267. Arrivals time: 0.26489570504054427 Scheduler time: 6.062118095811456 Scheduler overhead time: 0.032928076572716236 Adapter cache time: 0.02501375414431095 Engine time: 0.033587262500077486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.477637694682926,
    "estimated_duration": 3600.0701769453945,
    "input_throughput": 5382.345912056899,
    "output_throughput": 4725.210110885575,
    "total_throughput": 10107.556022942474,
    "itl": 181.34407636196215,
    "ttft": 1965102.3124181472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.144050152769721,
    "arrivals": 397773,
    "finished_requests": 78144,
    "scheduler_time": 106.72926667381472
}
#Debug simulation 
Total elapsed time: 6.477735214866698. Arrivals time: 0.2657969770953059 Scheduler time: 6.1064061457291245 Scheduler overhead time: 0.032775456085801125 Adapter cache time: 0.024409249890595675 Engine time: 0.033320562448352575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.4393983841873705,
    "estimated_duration": 3600.0799598454273,
    "input_throughput": 5368.0060486294715,
    "output_throughput": 4713.082261853016,
    "total_throughput": 10081.088310482488,
    "itl": 179.53651038574304,
    "ttft": 1966712.8185825997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.474504150096304,
    "arrivals": 397773,
    "finished_requests": 77936,
    "scheduler_time": 106.92937002288622
}
#Debug simulation 
Total elapsed time: 6.439496320206672. Arrivals time: 0.2686696392484009 Scheduler time: 6.064369678962976 Scheduler overhead time: 0.03299536881968379 Adapter cache time: 0.024696543347090483 Engine time: 0.033655885607004166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.494207396637648,
    "estimated_duration": 3600.0239862161666,
    "input_throughput": 5382.415526727133,
    "output_throughput": 4725.47795935116,
    "total_throughput": 10107.893486078292,
    "itl": 181.33581624033292,
    "ttft": 1965039.0136504238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.961816640745279,
    "arrivals": 397773,
    "finished_requests": 78146,
    "scheduler_time": 106.73278817684383
}
#Debug simulation 
Total elapsed time: 6.494330883957446. Arrivals time: 0.26962918415665627 Scheduler time: 6.1193928006105125 Scheduler overhead time: 0.032761487644165754 Adapter cache time: 0.024317088071256876 Engine time: 0.033140435349196196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.41510718408972,
    "estimated_duration": 3600.143312083032,
    "input_throughput": 5368.056025752532,
    "output_throughput": 4713.081821785172,
    "total_throughput": 10081.137847537704,
    "itl": 179.58903193195823,
    "ttft": 1966768.83972809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5284270964935525,
    "arrivals": 397773,
    "finished_requests": 77937,
    "scheduler_time": 106.9236909119805
}
#Debug simulation 
Total elapsed time: 6.415251946076751. Arrivals time: 0.2662498289719224 Scheduler time: 6.042323553469032 Scheduler overhead time: 0.03293694229796529 Adapter cache time: 0.024972677696496248 Engine time: 0.0335925598628819 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.128451032098383,
    "estimated_duration": 3600.147138422189,
    "input_throughput": 5360.721453306546,
    "output_throughput": 4726.401823525848,
    "total_throughput": 10087.123276832395,
    "itl": 181.76382714644697,
    "ttft": 1963626.6833605806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.416283814495081,
    "arrivals": 394957,
    "finished_requests": 77958,
    "scheduler_time": 106.69596751502499
}
#Debug simulation 
Total elapsed time: 6.128545554354787. Arrivals time: 0.26386867789551616 Scheduler time: 5.759339509997517 Scheduler overhead time: 0.03233225317671895 Adapter cache time: 0.025229549501091242 Engine time: 0.03288915613666177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.049760529305786,
    "estimated_duration": 3600.107612435944,
    "input_throughput": 5360.235603330415,
    "output_throughput": 4726.020672611972,
    "total_throughput": 10086.256275942387,
    "itl": 181.7767407338148,
    "ttft": 1963644.031967828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.677940652000197,
    "arrivals": 394957,
    "finished_requests": 77951,
    "scheduler_time": 106.68702433725011
}
#Debug simulation 
Total elapsed time: 6.049872640054673. Arrivals time: 0.261611161287874 Scheduler time: 5.683341515250504 Scheduler overhead time: 0.03211601451039314 Adapter cache time: 0.025089941918849945 Engine time: 0.03277211729437113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.076525209005922,
    "estimated_duration": 3600.117501087932,
    "input_throughput": 5351.179786264832,
    "output_throughput": 4718.232667369019,
    "total_throughput": 10069.412453633851,
    "itl": 180.16359655297074,
    "ttft": 1965184.111613677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.782833372894621,
    "arrivals": 394957,
    "finished_requests": 77819,
    "scheduler_time": 106.86998796243421
}
#Debug simulation 
Total elapsed time: 6.0766213200986385. Arrivals time: 0.26282413955777884 Scheduler time: 5.707846328616142 Scheduler overhead time: 0.032522720750421286 Adapter cache time: 0.025325675029307604 Engine time: 0.0331009509973228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.084834353998303,
    "estimated_duration": 3600.0337591859975,
    "input_throughput": 5360.681952150251,
    "output_throughput": 4726.210124164821,
    "total_throughput": 10086.892076315073,
    "itl": 181.76675052022188,
    "ttft": 1963626.9315347907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.499736254797233,
    "arrivals": 394957,
    "finished_requests": 77954,
    "scheduler_time": 106.69013719063352
}
#Debug simulation 
Total elapsed time: 6.084931769873947. Arrivals time: 0.26328314701095223 Scheduler time: 5.716447952203453 Scheduler overhead time: 0.03236739942803979 Adapter cache time: 0.025285083334892988 Engine time: 0.032691434025764465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.124848478008062,
    "estimated_duration": 3600.0156982293606,
    "input_throughput": 5350.800000531758,
    "output_throughput": 4718.15414814834,
    "total_throughput": 10068.954148680097,
    "itl": 180.16147859394434,
    "ttft": 1965135.713249284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.834662262592431,
    "arrivals": 394957,
    "finished_requests": 77813,
    "scheduler_time": 106.86658619036363
}
#Debug simulation 
Total elapsed time: 6.124958774074912. Arrivals time: 0.26642331667244434 Scheduler time: 5.750993350055069 Scheduler overhead time: 0.03229699470102787 Adapter cache time: 0.02728110458701849 Engine time: 0.03294364223256707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.047008546069264,
    "estimated_duration": 3600.0429706582054,
    "input_throughput": 5360.876566557049,
    "output_throughput": 4726.538582646131,
    "total_throughput": 10087.41514920318,
    "itl": 181.75957792135426,
    "ttft": 1963586.8848939936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.311652525248821,
    "arrivals": 394957,
    "finished_requests": 77958,
    "scheduler_time": 106.69574849675584
}
#Debug simulation 
Total elapsed time: 6.047101273201406. Arrivals time: 0.25717877922579646 Scheduler time: 5.685251327697188 Scheduler overhead time: 0.03218164620921016 Adapter cache time: 0.02480723476037383 Engine time: 0.03286140505224466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.976384178735316,
    "estimated_duration": 3600.0180038092417,
    "input_throughput": 5350.244632004514,
    "output_throughput": 4717.876127849491,
    "total_throughput": 10068.120759854004,
    "itl": 180.16554686128407,
    "ttft": 1965084.346278398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.925133445449144,
    "arrivals": 394957,
    "finished_requests": 77809,
    "scheduler_time": 106.86421574871552
}
#Debug simulation 
Total elapsed time: 5.976484284736216. Arrivals time: 0.2427406939677894 Scheduler time: 5.629768193233758 Scheduler overhead time: 0.032140163239091635 Adapter cache time: 0.02459238190203905 Engine time: 0.03246748587116599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.950788395944983,
    "estimated_duration": 3600.0000800706744,
    "input_throughput": 5364.5876584594025,
    "output_throughput": 4726.723783757789,
    "total_throughput": 10091.311442217191,
    "itl": 181.37288187766308,
    "ttft": 1959605.181202841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.119416503333594,
    "arrivals": 393528,
    "finished_requests": 77973,
    "scheduler_time": 106.66113969983164
}
#Debug simulation 
Total elapsed time: 5.950889469124377. Arrivals time: 0.24289526231586933 Scheduler time: 5.60538715403527 Scheduler overhead time: 0.0316119478084147 Adapter cache time: 0.02348809828981757 Engine time: 0.03278890997171402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.977608615066856,
    "estimated_duration": 3600.0222461534036,
    "input_throughput": 5364.288795891262,
    "output_throughput": 4726.622458564363,
    "total_throughput": 10090.911254455625,
    "itl": 181.3878914751335,
    "ttft": 1959635.9934036334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4381413116608375,
    "arrivals": 393528,
    "finished_requests": 77972,
    "scheduler_time": 106.65289406345013
}
#Debug simulation 
Total elapsed time: 5.977705440949649. Arrivals time: 0.24217929039150476 Scheduler time: 5.633264370262623 Scheduler overhead time: 0.03164539812132716 Adapter cache time: 0.023639723658561707 Engine time: 0.03234404604882002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.935814866796136,
    "estimated_duration": 3600.0032728361334,
    "input_throughput": 5350.035413947439,
    "output_throughput": 4713.186548475767,
    "total_throughput": 10063.221962423206,
    "itl": 179.21605485221417,
    "ttft": 1961376.2095879698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.566127079427185,
    "arrivals": 393528,
    "finished_requests": 77753,
    "scheduler_time": 106.89639542931977
}
#Debug simulation 
Total elapsed time: 5.935943542048335. Arrivals time: 0.24349680915474892 Scheduler time: 5.588262090925127 Scheduler overhead time: 0.03217967040836811 Adapter cache time: 0.0243343198671937 Engine time: 0.03272561263293028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.975262003950775,
    "estimated_duration": 3600.1631627231964,
    "input_throughput": 5364.30909575552,
    "output_throughput": 4726.636052552808,
    "total_throughput": 10090.945148308327,
    "itl": 181.38010204958238,
    "ttft": 1959620.6535823438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.203015021909896,
    "arrivals": 393528,
    "finished_requests": 77975,
    "scheduler_time": 106.66321521096167
}
#Debug simulation 
Total elapsed time: 5.975355119910091. Arrivals time: 0.24630377581343055 Scheduler time: 5.626558030024171 Scheduler overhead time: 0.031481248792260885 Adapter cache time: 0.023776736110448837 Engine time: 0.03244095342233777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.895662015769631,
    "estimated_duration": 3600.10415820348,
    "input_throughput": 5350.041041482382,
    "output_throughput": 4713.076415118816,
    "total_throughput": 10063.117456601198,
    "itl": 179.21928707456402,
    "ttft": 1961439.656091463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.615592075157881,
    "arrivals": 393528,
    "finished_requests": 77755,
    "scheduler_time": 106.8976061883741
}
#Debug simulation 
Total elapsed time: 5.895758220925927. Arrivals time: 0.24298158707097173 Scheduler time: 5.548938779626042 Scheduler overhead time: 0.032005189917981625 Adapter cache time: 0.024137289728969336 Engine time: 0.03285190276801586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.974029059056193,
    "estimated_duration": 3600.1030408769116,
    "input_throughput": 5364.550342230139,
    "output_throughput": 4726.688599406064,
    "total_throughput": 10091.238941636202,
    "itl": 181.36951727908965,
    "ttft": 1959607.315589718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1360,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.066468401066851,
    "arrivals": 393528,
    "finished_requests": 77975,
    "scheduler_time": 106.66560233704276
}
#Debug simulation 
Total elapsed time: 5.974130830727518. Arrivals time: 0.2465265248902142 Scheduler time: 5.62485485849902 Scheduler overhead time: 0.031717315316200256 Adapter cache time: 0.02392094023525715 Engine time: 0.03244808502495289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.915905843023211,
    "estimated_duration": 3600.0870029161847,
    "input_throughput": 5349.91098392863,
    "output_throughput": 4713.076930156354,
    "total_throughput": 10062.987914084984,
    "itl": 179.22728324537675,
    "ttft": 1961400.627546835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.681066040210421,
    "arrivals": 393528,
    "finished_requests": 77753,
    "scheduler_time": 106.89538423375247
}
#Debug simulation 
Total elapsed time: 5.9160329550504684. Arrivals time: 0.24236209923401475 Scheduler time: 5.569364784285426 Scheduler overhead time: 0.03208457492291927 Adapter cache time: 0.024285897612571716 Engine time: 0.03294132323935628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.857814573217183,
    "estimated_duration": 3600.005705600108,
    "input_throughput": 5359.265672825889,
    "output_throughput": 4727.126674695982,
    "total_throughput": 10086.39234752187,
    "itl": 181.56694373495384,
    "ttft": 1958581.5487592134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.293864304737767,
    "arrivals": 392778,
    "finished_requests": 78176,
    "scheduler_time": 106.61633116125445
}
#Debug simulation 
Total elapsed time: 5.857941260095686. Arrivals time: 0.24584586825221777 Scheduler time: 5.509442361537367 Scheduler overhead time: 0.031540971249341965 Adapter cache time: 0.02425345266237855 Engine time: 0.03213708708062768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.874285156838596,
    "estimated_duration": 3600.002831795234,
    "input_throughput": 5358.803840268007,
    "output_throughput": 4726.756004109688,
    "total_throughput": 10085.559844377694,
    "itl": 181.58280660908528,
    "ttft": 1958659.6850843353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.563333774849261,
    "arrivals": 392778,
    "finished_requests": 78167,
    "scheduler_time": 106.60861172870936
}
#Debug simulation 
Total elapsed time: 5.8744016997516155. Arrivals time: 0.2443466573022306 Scheduler time: 5.5275258789770305 Scheduler overhead time: 0.03164517227560282 Adapter cache time: 0.02388833835721016 Engine time: 0.03233752865344286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.83398356474936,
    "estimated_duration": 3600.002390300826,
    "input_throughput": 5347.379505042875,
    "output_throughput": 4716.541590568539,
    "total_throughput": 10063.921095611415,
    "itl": 179.71292871915583,
    "ttft": 1960489.2778124409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.704409543406159,
    "arrivals": 392778,
    "finished_requests": 77992,
    "scheduler_time": 106.81460126084434
}
#Debug simulation 
Total elapsed time: 5.834078412968665. Arrivals time: 0.23995529161766171 Scheduler time: 5.490384936332703 Scheduler overhead time: 0.031897460110485554 Adapter cache time: 0.024502120446413755 Engine time: 0.03250753181055188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.848316358868033,
    "estimated_duration": 3600.1601299238423,
    "input_throughput": 5358.902466487823,
    "output_throughput": 4726.755862484367,
    "total_throughput": 10085.65832897219,
    "itl": 181.57267012856295,
    "ttft": 1958694.3255491636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.369380881655886,
    "arrivals": 392778,
    "finished_requests": 78174,
    "scheduler_time": 106.61882974173979
}
#Debug simulation 
Total elapsed time: 5.848410555161536. Arrivals time: 0.24366513453423977 Scheduler time: 5.502409448847175 Scheduler overhead time: 0.03162979707121849 Adapter cache time: 0.023934238124638796 Engine time: 0.032128119841217995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.822554830927402,
    "estimated_duration": 3600.1092221020194,
    "input_throughput": 5347.5985900114565,
    "output_throughput": 4716.543013682717,
    "total_throughput": 10064.141603694174,
    "itl": 179.71741351912885,
    "ttft": 1960529.0647870142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.767246422748975,
    "arrivals": 392778,
    "finished_requests": 77997,
    "scheduler_time": 106.81565816481246
}
#Debug simulation 
Total elapsed time: 5.822650162968785. Arrivals time: 0.24355321656912565 Scheduler time: 5.475439365487546 Scheduler overhead time: 0.0320256007835269 Adapter cache time: 0.02447497844696045 Engine time: 0.03235210990533233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.835453181993216,
    "estimated_duration": 3600.1074350172553,
    "input_throughput": 5359.368671148427,
    "output_throughput": 4727.229480560885,
    "total_throughput": 10086.598151709311,
    "itl": 181.5625850004796,
    "ttft": 1958624.315785581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.19504056374764,
    "arrivals": 392778,
    "finished_requests": 78180,
    "scheduler_time": 106.62214857258924
}
#Debug simulation 
Total elapsed time: 5.835572422016412. Arrivals time: 0.24329071259126067 Scheduler time: 5.490180664230138 Scheduler overhead time: 0.031496938318014145 Adapter cache time: 0.023834924213588238 Engine time: 0.032036236487329006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.795961453113705,
    "estimated_duration": 3600.163879656771,
    "input_throughput": 5347.517402967618,
    "output_throughput": 4716.471407301278,
    "total_throughput": 10063.988810268896,
    "itl": 179.71953901072172,
    "ttft": 1960548.576647169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.823709872849266,
    "arrivals": 392778,
    "finished_requests": 77997,
    "scheduler_time": 106.81586668323602
}
#Debug simulation 
Total elapsed time: 5.79605021700263. Arrivals time: 0.24273077631369233 Scheduler time: 5.4498887890949845 Scheduler overhead time: 0.031939898151904345 Adapter cache time: 0.024408421479165554 Engine time: 0.03233276028186083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.8745583621785045,
    "estimated_duration": 3600.085921574688,
    "input_throughput": 5366.498306115272,
    "output_throughput": 4725.728043890255,
    "total_throughput": 10092.226350005527,
    "itl": 181.7971269369746,
    "ttft": 1958083.8967897487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.016918904571967,
    "arrivals": 386275,
    "finished_requests": 77698,
    "scheduler_time": 106.59477863926693
}
#Debug simulation 
Total elapsed time: 5.874655534047633. Arrivals time: 0.2761129504069686 Scheduler time: 5.489189125597477 Scheduler overhead time: 0.0317349536344409 Adapter cache time: 0.030676376540213823 Engine time: 0.03228391194716096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.905414987821132,
    "estimated_duration": 3600.1121025589496,
    "input_throughput": 5365.917907464294,
    "output_throughput": 4725.406741614496,
    "total_throughput": 10091.324649078791,
    "itl": 181.81597163709964,
    "ttft": 1958202.9477665417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1965,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.413568863868502,
    "arrivals": 386275,
    "finished_requests": 77692,
    "scheduler_time": 106.58395238432753
}
#Debug simulation 
Total elapsed time: 5.905520061030984. Arrivals time: 0.24369613640010357 Scheduler time: 5.552631594706327 Scheduler overhead time: 0.03146189171820879 Adapter cache time: 0.030792398378252983 Engine time: 0.03217864455655217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.866484865080565,
    "estimated_duration": 3600.052183137641,
    "input_throughput": 5349.478013181259,
    "output_throughput": 4713.606397007944,
    "total_throughput": 10063.084410189203,
    "itl": 179.82777735165158,
    "ttft": 1959652.3970639093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.528469793684733,
    "arrivals": 386275,
    "finished_requests": 77485,
    "scheduler_time": 106.83185192691958
}
#Debug simulation 
Total elapsed time: 5.866576272062957. Arrivals time: 0.2763140541501343 Scheduler time: 5.479473405517638 Scheduler overhead time: 0.03193588741123676 Adapter cache time: 0.03135197050869465 Engine time: 0.032683227211236954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.894519199151546,
    "estimated_duration": 3600.00161933211,
    "input_throughput": 5366.265641731594,
    "output_throughput": 4725.692318759635,
    "total_throughput": 10091.957960491229,
    "itl": 181.80175119253317,
    "ttft": 1958106.406814221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1962,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.130431254310354,
    "arrivals": 386275,
    "finished_requests": 77695,
    "scheduler_time": 106.58902311240543
}
#Debug simulation 
Total elapsed time: 5.894632305018604. Arrivals time: 0.24358778726309538 Scheduler time: 5.541116562206298 Scheduler overhead time: 0.03188010957092047 Adapter cache time: 0.030992007348686457 Engine time: 0.03232011431828141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.903367126360536,
    "estimated_duration": 3600.0770213257706,
    "input_throughput": 5349.601101841887,
    "output_throughput": 4713.660541004417,
    "total_throughput": 10063.261642846302,
    "itl": 179.8504598029051,
    "ttft": 1959642.5398401918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1996,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6117791122383,
    "arrivals": 386275,
    "finished_requests": 77487,
    "scheduler_time": 106.82898273280531
}
#Debug simulation 
Total elapsed time: 5.903455380350351. Arrivals time: 0.26424694852903485 Scheduler time: 5.5287791774608195 Scheduler overhead time: 0.03182712895795703 Adapter cache time: 0.03126497799530625 Engine time: 0.03243558667600155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.875797502230853,
    "estimated_duration": 3600.111252042018,
    "input_throughput": 5366.478046766348,
    "output_throughput": 4725.772568930998,
    "total_throughput": 10092.250615697347,
    "itl": 181.79054063870464,
    "ttft": 1958070.9085547458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.878438879777505,
    "arrivals": 386275,
    "finished_requests": 77699,
    "scheduler_time": 106.59947146427514
}
#Debug simulation 
Total elapsed time: 5.875890363007784. Arrivals time: 0.24074686877429485 Scheduler time: 5.525651245377958 Scheduler overhead time: 0.03151084017008543 Adapter cache time: 0.031037449836730957 Engine time: 0.032284441869705915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.835036087781191,
    "estimated_duration": 3600.0345397803594,
    "input_throughput": 5349.439508759534,
    "output_throughput": 4713.50922122966,
    "total_throughput": 10062.948729989193,
    "itl": 179.8457524089557,
    "ttft": 1959682.0812045876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1995,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.691315817199441,
    "arrivals": 386275,
    "finished_requests": 77483,
    "scheduler_time": 106.82634692254138
}
#Debug simulation 
Total elapsed time: 5.8351464699953794. Arrivals time: 0.2773427376523614 Scheduler time: 5.446813582908362 Scheduler overhead time: 0.03202188201248646 Adapter cache time: 0.03169535519555211 Engine time: 0.03242808347567916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.739080394152552,
    "estimated_duration": 3600.1902377227784,
    "input_throughput": 5446.038599449633,
    "output_throughput": 4771.62007162878,
    "total_throughput": 10217.658671078412,
    "itl": 179.49039077844287,
    "ttft": 1945421.551651338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2002,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.12709646335355,
    "arrivals": 383398,
    "finished_requests": 78726,
    "scheduler_time": 107.73826426490963
}
#Debug simulation 
Total elapsed time: 5.739168380852789. Arrivals time: 0.25367664033547044 Scheduler time: 5.375969264656305 Scheduler overhead time: 0.031035275664180517 Adapter cache time: 0.031853693537414074 Engine time: 0.032016501761972904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.688838765956461,
    "estimated_duration": 3600.0178169441992,
    "input_throughput": 5445.606382204155,
    "output_throughput": 4771.09083159469,
    "total_throughput": 10216.697213798845,
    "itl": 179.50983832930848,
    "ttft": 1945497.4523605793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2002,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.525426510735189,
    "arrivals": 383398,
    "finished_requests": 78714,
    "scheduler_time": 107.72120448160761
}
#Debug simulation 
Total elapsed time: 5.688925108872354. Arrivals time: 0.2440095911733806 Scheduler time: 5.335811210330576 Scheduler overhead time: 0.03109097760170698 Adapter cache time: 0.031306915916502476 Engine time: 0.03214755607768893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.772777955047786,
    "estimated_duration": 3600.1360729710887,
    "input_throughput": 5439.740221773909,
    "output_throughput": 4766.292065688207,
    "total_throughput": 10206.032287462116,
    "itl": 177.2900812276054,
    "ttft": 1946454.524400302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.522062091287182,
    "arrivals": 383398,
    "finished_requests": 78636,
    "scheduler_time": 108.11087658815805
}
#Debug simulation 
Total elapsed time: 5.772874996997416. Arrivals time: 0.26798035204410553 Scheduler time: 5.394339614547789 Scheduler overhead time: 0.03138163918629289 Adapter cache time: 0.03188712988048792 Engine time: 0.032532745972275734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.691814189776778,
    "estimated_duration": 3600.143120698172,
    "input_throughput": 5445.640726693668,
    "output_throughput": 4771.406142506006,
    "total_throughput": 10217.046869199674,
    "itl": 179.49749029312343,
    "ttft": 1945485.7429522886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2002,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.244721526966705,
    "arrivals": 383398,
    "finished_requests": 78721,
    "scheduler_time": 107.73290422821441
}
#Debug simulation 
Total elapsed time: 5.691919975914061. Arrivals time: 0.24295279430225492 Scheduler time: 5.339813770260662 Scheduler overhead time: 0.0310073746368289 Adapter cache time: 0.03146124258637428 Engine time: 0.032065675128251314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.694006015080959,
    "estimated_duration": 3600.120707196954,
    "input_throughput": 5439.515114271602,
    "output_throughput": 4766.193246159199,
    "total_throughput": 10205.7083604308,
    "itl": 177.29679507674325,
    "ttft": 1946431.7696122322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1998,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.611584201324607,
    "arrivals": 383398,
    "finished_requests": 78634,
    "scheduler_time": 108.10765802082949
}
#Debug simulation 
Total elapsed time: 5.694096620194614. Arrivals time: 0.24288801150396466 Scheduler time: 5.3406810611486435 Scheduler overhead time: 0.03143421187996864 Adapter cache time: 0.03205533651635051 Engine time: 0.0322906244546175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.723770288750529,
    "estimated_duration": 3600.049466431456,
    "input_throughput": 5446.022112422352,
    "output_throughput": 4771.656656436965,
    "total_throughput": 10217.678768859318,
    "itl": 179.48447213168998,
    "ttft": 1945391.7727352476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1998,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.974120489214371,
    "arrivals": 383398,
    "finished_requests": 78724,
    "scheduler_time": 107.73654472152229
}
#Debug simulation 
Total elapsed time: 5.723857888951898. Arrivals time: 0.2448807219043374 Scheduler time: 5.368836482055485 Scheduler overhead time: 0.03127790242433548 Adapter cache time: 0.03200046019628644 Engine time: 0.03227853495627642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.729439262766391,
    "estimated_duration": 3600.0014245449797,
    "input_throughput": 5439.686458589435,
    "output_throughput": 4766.208947311381,
    "total_throughput": 10205.895405900816,
    "itl": 177.29973350258228,
    "ttft": 1946431.0679298656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1998,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.691186348125134,
    "arrivals": 383398,
    "finished_requests": 78632,
    "scheduler_time": 108.1018132738713
}
#Debug simulation 
Total elapsed time: 5.72952985111624. Arrivals time: 0.2587257605046034 Scheduler time: 5.360308718401939 Scheduler overhead time: 0.031193553004413843 Adapter cache time: 0.03222740301862359 Engine time: 0.03236003406345844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.743136055301875,
    "estimated_duration": 3600.16116527289,
    "input_throughput": 5477.029803610299,
    "output_throughput": 4832.69948240809,
    "total_throughput": 10309.729286018388,
    "itl": 177.78479344228984,
    "ttft": 1943840.9293489133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1719,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.260978431820551,
    "arrivals": 381994,
    "finished_requests": 79504,
    "scheduler_time": 108.92369719444964
}
#Debug simulation 
Total elapsed time: 5.74324954720214. Arrivals time: 0.2455605990253389 Scheduler time: 5.390237728599459 Scheduler overhead time: 0.031142601743340492 Adapter cache time: 0.02956340229138732 Engine time: 0.03209153702482581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.782005215995014,
    "estimated_duration": 3600.0372613331174,
    "input_throughput": 5476.674425502738,
    "output_throughput": 4831.858044035513,
    "total_throughput": 10308.53246953825,
    "itl": 177.80011493853274,
    "ttft": 1943939.8200164668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.588682945771004,
    "arrivals": 381994,
    "finished_requests": 79494,
    "scheduler_time": 108.91005715938962
}
#Debug simulation 
Total elapsed time: 5.782093174289912. Arrivals time: 0.25437994953244925 Scheduler time: 5.407958853058517 Scheduler overhead time: 0.04392453748732805 Adapter cache time: 0.029004281386733055 Engine time: 0.03223115112632513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.7629895568825305,
    "estimated_duration": 3600.1710614134076,
    "input_throughput": 5469.4509411087365,
    "output_throughput": 4825.78597061974,
    "total_throughput": 10295.236911728476,
    "itl": 175.55130252055898,
    "ttft": 1944877.5573199007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.610208881292433,
    "arrivals": 381994,
    "finished_requests": 79396,
    "scheduler_time": 109.33141240621632
}
#Debug simulation 
Total elapsed time: 5.763078015763313. Arrivals time: 0.24673471320420504 Scheduler time: 5.407889243680984 Scheduler overhead time: 0.03128270339220762 Adapter cache time: 0.029615591280162334 Engine time: 0.03268345911055803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.759806673973799,
    "estimated_duration": 3600.080542835607,
    "input_throughput": 5476.964963808693,
    "output_throughput": 4832.431328410537,
    "total_throughput": 10309.39629221923,
    "itl": 177.78836228284774,
    "ttft": 1943870.109504269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.359980504389251,
    "arrivals": 381994,
    "finished_requests": 79500,
    "scheduler_time": 108.91839927028154
}
#Debug simulation 
Total elapsed time: 5.759896105155349. Arrivals time: 0.2452589776366949 Scheduler time: 5.407369019463658 Scheduler overhead time: 0.03118889592587948 Adapter cache time: 0.029244222678244114 Engine time: 0.03217193391174078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.948315981309861,
    "estimated_duration": 3600.055038664225,
    "input_throughput": 5469.464435550956,
    "output_throughput": 4825.704833236677,
    "total_throughput": 10295.169268787633,
    "itl": 175.5530085794583,
    "ttft": 1944896.5451038135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.68440361527717,
    "arrivals": 381994,
    "finished_requests": 79391,
    "scheduler_time": 109.32571256251583
}
#Debug simulation 
Total elapsed time: 5.948384057264775. Arrivals time: 0.46757774613797665 Scheduler time: 5.3724763030186296 Scheduler overhead time: 0.031502434983849525 Adapter cache time: 0.029510909225791693 Engine time: 0.032611486967653036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.781692288815975,
    "estimated_duration": 3600.052742551205,
    "input_throughput": 5477.13670051031,
    "output_throughput": 4832.7386413985405,
    "total_throughput": 10309.87534190885,
    "itl": 177.7819004975296,
    "ttft": 1943814.5735695409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.136906406641792,
    "arrivals": 381994,
    "finished_requests": 79503,
    "scheduler_time": 108.92279058642814
}
#Debug simulation 
Total elapsed time: 5.781784015707672. Arrivals time: 0.25748441042378545 Scheduler time: 5.417121620383114 Scheduler overhead time: 0.030998427886515856 Adapter cache time: 0.029269051738083363 Engine time: 0.032244413159787655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.768561439123005,
    "estimated_duration": 3600.1214093935178,
    "input_throughput": 5469.363602189481,
    "output_throughput": 4825.61586802892,
    "total_throughput": 10294.9794702184,
    "itl": 175.5560579809681,
    "ttft": 1944923.373979872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.750172845572119,
    "arrivals": 381994,
    "finished_requests": 79391,
    "scheduler_time": 109.32583557655707
}
#Debug simulation 
Total elapsed time: 5.768650987185538. Arrivals time: 0.2540703732520342 Scheduler time: 5.405633412767202 Scheduler overhead time: 0.0313662001863122 Adapter cache time: 0.029895222280174494 Engine time: 0.03274846402928233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.8785186028108,
    "estimated_duration": 3600.198614977641,
    "input_throughput": 5532.900300869571,
    "output_throughput": 4896.908722384332,
    "total_throughput": 10429.809023253902,
    "itl": 175.84396014453156,
    "ttft": 1934650.673417184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.664183321753643,
    "arrivals": 381208,
    "finished_requests": 80216,
    "scheduler_time": 110.21501766459505
}
#Debug simulation 
Total elapsed time: 5.878623676020652. Arrivals time: 0.2652426543645561 Scheduler time: 5.506348142400384 Scheduler overhead time: 0.031530431006103754 Adapter cache time: 0.028161081485450268 Engine time: 0.0325003070756793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.858087845146656,
    "estimated_duration": 3600.1433832990606,
    "input_throughput": 5532.359097805825,
    "output_throughput": 4896.6602501941525,
    "total_throughput": 10429.019347999976,
    "itl": 175.8590424974188,
    "ttft": 1934716.1960578011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1522,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.965172201739543,
    "arrivals": 381208,
    "finished_requests": 80208,
    "scheduler_time": 110.20505264213395
}
#Debug simulation 
Total elapsed time: 5.85819489788264. Arrivals time: 0.2502658781595528 Scheduler time: 5.5012326603755355 Scheduler overhead time: 0.03128100140020251 Adapter cache time: 0.02817089483141899 Engine time: 0.03239934565499425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.836108827963471,
    "estimated_duration": 3600.0281818285157,
    "input_throughput": 5526.745901720763,
    "output_throughput": 4891.851705187256,
    "total_throughput": 10418.597606908019,
    "itl": 173.59981327847484,
    "ttft": 1935986.7062625263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.969812138453069,
    "arrivals": 381208,
    "finished_requests": 80115,
    "scheduler_time": 110.60728497296486
}
#Debug simulation 
Total elapsed time: 5.8362220181152225. Arrivals time: 0.24826201098039746 Scheduler time: 5.480531064327806 Scheduler overhead time: 0.03149970807135105 Adapter cache time: 0.028281196020543575 Engine time: 0.03272965596988797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.847713470924646,
    "estimated_duration": 3600.086507760432,
    "input_throughput": 5532.662328270209,
    "output_throughput": 4896.92427168007,
    "total_throughput": 10429.586599950278,
    "itl": 175.84793670437463,
    "ttft": 1934683.8601968496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7511981379426045,
    "arrivals": 381208,
    "finished_requests": 80212,
    "scheduler_time": 110.20939617354641
}
#Debug simulation 
Total elapsed time: 5.847824160940945. Arrivals time: 0.24999460484832525 Scheduler time: 5.490720640402287 Scheduler overhead time: 0.03152367752045393 Adapter cache time: 0.02787566650658846 Engine time: 0.032848086673766375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.843294733669609,
    "estimated_duration": 3600.12687378698,
    "input_throughput": 5526.574672928394,
    "output_throughput": 4891.573440987014,
    "total_throughput": 10418.148113915408,
    "itl": 173.61165745075243,
    "ttft": 1936012.806239858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.030248853769102,
    "arrivals": 381208,
    "finished_requests": 80114,
    "scheduler_time": 110.60609215849973
}
#Debug simulation 
Total elapsed time: 5.843381579965353. Arrivals time: 0.24567556427791715 Scheduler time: 5.489718561060727 Scheduler overhead time: 0.03175581246614456 Adapter cache time: 0.02825371688231826 Engine time: 0.03301794035360217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.873112931847572,
    "estimated_duration": 3600.0367254733196,
    "input_throughput": 5533.149109022229,
    "output_throughput": 4897.128930728364,
    "total_throughput": 10430.278039750592,
    "itl": 175.83834472552454,
    "ttft": 1934608.3087877424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.547866498546084,
    "arrivals": 381208,
    "finished_requests": 80216,
    "scheduler_time": 110.21342071138058
}
#Debug simulation 
Total elapsed time: 5.873201441951096. Arrivals time: 0.24866987997666 Scheduler time: 5.517787614837289 Scheduler overhead time: 0.031305032316595316 Adapter cache time: 0.02799263969063759 Engine time: 0.03264719480648637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.845250231679529,
    "estimated_duration": 3600.187276999807,
    "input_throughput": 5526.4819491780745,
    "output_throughput": 4891.49137115873,
    "total_throughput": 10417.973320336805,
    "itl": 173.61426810415378,
    "ttft": 1936036.5830762344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.090736425034653,
    "arrivals": 381208,
    "finished_requests": 80114,
    "scheduler_time": 110.60617650843903
}
#Debug simulation 
Total elapsed time: 5.845335884951055. Arrivals time: 0.24906850885599852 Scheduler time: 5.488857166841626 Scheduler overhead time: 0.031754450872540474 Adapter cache time: 0.027974527794867754 Engine time: 0.032746825367212296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.966778532136232,
    "estimated_duration": 3600.1753760945567,
    "input_throughput": 5715.2815767327165,
    "output_throughput": 5010.011212166646,
    "total_throughput": 10725.292788899364,
    "itl": 170.98454886667963,
    "ttft": 1904896.5607578268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.449949179678343,
    "arrivals": 377644,
    "finished_requests": 82776,
    "scheduler_time": 112.95932764623853
}
#Debug simulation 
Total elapsed time: 5.966866101138294. Arrivals time: 0.25451842602342367 Scheduler time: 5.605660586617887 Scheduler overhead time: 0.03201851015910506 Adapter cache time: 0.025984766893088818 Engine time: 0.03350707795470953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.99472735915333,
    "estimated_duration": 3600.023352518905,
    "input_throughput": 5714.866817629056,
    "output_throughput": 5009.75833597882,
    "total_throughput": 10724.625153607874,
    "itl": 171.00051265739603,
    "ttft": 1904938.9933667872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.742236206401075,
    "arrivals": 377644,
    "finished_requests": 82765,
    "scheduler_time": 112.94505166769028
}
#Debug simulation 
Total elapsed time: 5.9948258539661765. Arrivals time: 0.257013319991529 Scheduler time: 5.6317462576553226 Scheduler overhead time: 0.03194032609462738 Adapter cache time: 0.025573989376425743 Engine time: 0.03329516388475895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.979095086921006,
    "estimated_duration": 3600.0459248022034,
    "input_throughput": 5709.192724014835,
    "output_throughput": 5006.336690271593,
    "total_throughput": 10715.52941428643,
    "itl": 169.40358862329452,
    "ttft": 1905604.03686153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.74429595313961,
    "arrivals": 377644,
    "finished_requests": 82676,
    "scheduler_time": 113.24101904654482
}
#Debug simulation 
Total elapsed time: 5.97921680053696. Arrivals time: 0.2551278406754136 Scheduler time: 5.616655057761818 Scheduler overhead time: 0.03241908038035035 Adapter cache time: 0.025922929868102074 Engine time: 0.03368084505200386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.226760892663151,
    "estimated_duration": 3600.174559015595,
    "input_throughput": 5715.059829106157,
    "output_throughput": 5009.796526347231,
    "total_throughput": 10724.856355453387,
    "itl": 170.98968305620448,
    "ttft": 1904944.7300072755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5304539483831325,
    "arrivals": 377644,
    "finished_requests": 82773,
    "scheduler_time": 112.95649064380211
}
#Debug simulation 
Total elapsed time: 6.226827908772975. Arrivals time: 0.4742886791937053 Scheduler time: 5.645896361209452 Scheduler overhead time: 0.03212603693827987 Adapter cache time: 0.025707609485834837 Engine time: 0.03356635291129351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.984628985635936,
    "estimated_duration": 3600.102084777651,
    "input_throughput": 5709.103663172767,
    "output_throughput": 5006.258593667945,
    "total_throughput": 10715.362256840712,
    "itl": 169.39630411790827,
    "ttft": 1905640.3377083614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.801880345232754,
    "arrivals": 377644,
    "finished_requests": 82676,
    "scheduler_time": 113.24270805778838
}
#Debug simulation 
Total elapsed time: 5.984720348846167. Arrivals time: 0.25683252373710275 Scheduler time: 5.619905257131904 Scheduler overhead time: 0.0323077323846519 Adapter cache time: 0.026382847223430872 Engine time: 0.033963616006076336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.951250599231571,
    "estimated_duration": 3600.0703117128496,
    "input_throughput": 5715.448371398696,
    "output_throughput": 5010.157424236071,
    "total_throughput": 10725.605795634767,
    "itl": 170.97993431396952,
    "ttft": 1904854.41947025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.347533128787646,
    "arrivals": 377644,
    "finished_requests": 82776,
    "scheduler_time": 112.95905846524494
}
#Debug simulation 
Total elapsed time: 5.95134026510641. Arrivals time: 0.25421159574761987 Scheduler time: 5.590733237564564 Scheduler overhead time: 0.03205747436732054 Adapter cache time: 0.02568171825259924 Engine time: 0.033379106782376766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.158070320729166,
    "estimated_duration": 3600.1584012490675,
    "input_throughput": 5709.014356943032,
    "output_throughput": 5006.1802818861925,
    "total_throughput": 10715.194638829224,
    "itl": 169.39877616262172,
    "ttft": 1905662.13022494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.858972564265111,
    "arrivals": 377644,
    "finished_requests": 82676,
    "scheduler_time": 113.24278250846257
}
#Debug simulation 
Total elapsed time: 6.158135007601231. Arrivals time: 0.25022919988259673 Scheduler time: 5.8006686004810035 Scheduler overhead time: 0.03244425589218736 Adapter cache time: 0.025783715769648552 Engine time: 0.03368580900132656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.011414031963795,
    "estimated_duration": 3600.010059157078,
    "input_throughput": 5808.032937801212,
    "output_throughput": 5083.387462613063,
    "total_throughput": 10891.420400414274,
    "itl": 168.36715191606248,
    "ttft": 1893787.9209343211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.608315050096806,
    "arrivals": 376253,
    "finished_requests": 84249,
    "scheduler_time": 114.64952589608056
}
#Debug simulation 
Total elapsed time: 6.011515410151333. Arrivals time: 0.24888109415769577 Scheduler time: 5.657377487514168 Scheduler overhead time: 0.03247826360166073 Adapter cache time: 0.023796388413757086 Engine time: 0.03362949192523956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.089422530028969,
    "estimated_duration": 3600.1792741639792,
    "input_throughput": 5807.759949636233,
    "output_throughput": 5083.148534110046,
    "total_throughput": 10890.908483746278,
    "itl": 168.37949085993742,
    "ttft": 1893830.6316233096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8473722916864843,
    "arrivals": 376253,
    "finished_requests": 84249,
    "scheduler_time": 114.64750210767717
}
#Debug simulation 
Total elapsed time: 6.089514933060855. Arrivals time: 0.2647693185135722 Scheduler time: 5.719160619191825 Scheduler overhead time: 0.03258980670943856 Adapter cache time: 0.023683174047619104 Engine time: 0.03389231767505407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.225086033344269,
    "estimated_duration": 3600.050929875421,
    "input_throughput": 5801.237651024763,
    "output_throughput": 5076.675401542846,
    "total_throughput": 10877.91305256761,
    "itl": 166.80730003293985,
    "ttft": 1894625.1926918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8418939643911436,
    "arrivals": 376253,
    "finished_requests": 84149,
    "scheduler_time": 114.92367220399086
}
#Debug simulation 
Total elapsed time: 6.225149710197002. Arrivals time: 0.467015263158828 Scheduler time: 5.651976705528796 Scheduler overhead time: 0.03287205845117569 Adapter cache time: 0.023668203968554735 Engine time: 0.03414529003202915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.054728956893086,
    "estimated_duration": 3600.081124184937,
    "input_throughput": 5807.985508863741,
    "output_throughput": 5083.287672897427,
    "total_throughput": 10891.273181761168,
    "itl": 168.3688431632126,
    "ttft": 1893822.296798184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.675223779557227,
    "arrivals": 376253,
    "finished_requests": 84250,
    "scheduler_time": 114.64974780471483
}
#Debug simulation 
Total elapsed time: 6.05481246067211. Arrivals time: 0.2696042051538825 Scheduler time: 5.679483168758452 Scheduler overhead time: 0.03256382839754224 Adapter cache time: 0.023796553257852793 Engine time: 0.033910212107002735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.0666120001114905,
    "estimated_duration": 3600.1009622349143,
    "input_throughput": 5801.157028394812,
    "output_throughput": 5076.604848507977,
    "total_throughput": 10877.76187690279,
    "itl": 166.80918239641102,
    "ttft": 1894644.1712455954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8938302781805465,
    "arrivals": 376253,
    "finished_requests": 84149,
    "scheduler_time": 114.92376504338561
}
#Debug simulation 
Total elapsed time: 6.066703314892948. Arrivals time: 0.2707313662394881 Scheduler time: 5.69007432134822 Scheduler overhead time: 0.03287653438746929 Adapter cache time: 0.023474627640098333 Engine time: 0.033957079984247684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.26230323780328,
    "estimated_duration": 3600.092151854419,
    "input_throughput": 5808.320486804467,
    "output_throughput": 5083.825143357082,
    "total_throughput": 10892.145630161549,
    "itl": 168.36366704268156,
    "ttft": 1893769.738233099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5252692976895768,
    "arrivals": 376253,
    "finished_requests": 84258,
    "scheduler_time": 114.65501349043697
}
#Debug simulation 
Total elapsed time: 6.262365913018584. Arrivals time: 0.4678761619143188 Scheduler time: 5.688510160893202 Scheduler overhead time: 0.03277591150254011 Adapter cache time: 0.023812018800526857 Engine time: 0.03388582030311227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.037916133180261,
    "estimated_duration": 3600.145508323673,
    "input_throughput": 5801.085248280567,
    "output_throughput": 5076.5420335774,
    "total_throughput": 10877.627281857965,
    "itl": 166.8115312997849,
    "ttft": 1894665.7242138444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9398561640084395,
    "arrivals": 376253,
    "finished_requests": 84149,
    "scheduler_time": 114.92393829296446
}
#Debug simulation 
Total elapsed time: 6.038001334294677. Arrivals time: 0.25003958167508245 Scheduler time: 5.681748577859253 Scheduler overhead time: 0.03296643029898405 Adapter cache time: 0.02364176558330655 Engine time: 0.034028909634798765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.102442651987076,
    "estimated_duration": 3600.1755897393086,
    "input_throughput": 5822.93820883275,
    "output_throughput": 5141.022858093491,
    "total_throughput": 10963.961066926242,
    "itl": 167.05611759518945,
    "ttft": 1894461.7453426258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1026,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1400604252750783,
    "arrivals": 375490,
    "finished_requests": 84565,
    "scheduler_time": 115.73573364121515
}
#Debug simulation 
Total elapsed time: 6.102529712021351. Arrivals time: 0.25054135732352734 Scheduler time: 5.747849611565471 Scheduler overhead time: 0.03280977997928858 Adapter cache time: 0.02198086678981781 Engine time: 0.03383489791303873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.1299545099027455,
    "estimated_duration": 3600.0779597151572,
    "input_throughput": 5822.73640586899,
    "output_throughput": 5140.801173501342,
    "total_throughput": 10963.537579370332,
    "itl": 167.06848367265522,
    "ttft": 1894414.3607818324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1028,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3491649477882355,
    "arrivals": 375490,
    "finished_requests": 84556,
    "scheduler_time": 115.72635130348648
}
#Debug simulation 
Total elapsed time: 6.13006646791473. Arrivals time: 0.2510894536972046 Scheduler time: 5.774858834221959 Scheduler overhead time: 0.03269497398287058 Adapter cache time: 0.021966891828924417 Engine time: 0.0339740258641541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.156256689224392,
    "estimated_duration": 3600.055149338502,
    "input_throughput": 5814.805365925154,
    "output_throughput": 5134.26023581786,
    "total_throughput": 10949.065601743014,
    "itl": 165.10281934761628,
    "ttft": 1895958.4766285117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1024,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3428563699684664,
    "arrivals": 375490,
    "finished_requests": 84434,
    "scheduler_time": 116.08040729185291
}
#Debug simulation 
Total elapsed time: 6.15634696232155. Arrivals time: 0.2657187692821026 Scheduler time: 5.785459817387164 Scheduler overhead time: 0.03308379463851452 Adapter cache time: 0.02194141037762165 Engine time: 0.03437767969444394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.092811048962176,
    "estimated_duration": 3600.07249925161,
    "input_throughput": 5822.868846212896,
    "output_throughput": 5140.959523411665,
    "total_throughput": 10963.828369624562,
    "itl": 167.06188924803504,
    "ttft": 1894389.3049142116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1026,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1960905325598636,
    "arrivals": 375490,
    "finished_requests": 84558,
    "scheduler_time": 115.73052303436408
}
#Debug simulation 
Total elapsed time: 6.092906621750444. Arrivals time: 0.2780297789722681 Scheduler time: 5.710842886008322 Scheduler overhead time: 0.03264869889244437 Adapter cache time: 0.021756495349109173 Engine time: 0.034159330651164055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.104646757245064,
    "estimated_duration": 3600.100316201615,
    "input_throughput": 5814.73241336969,
    "output_throughput": 5134.195821382459,
    "total_throughput": 10948.92823475215,
    "itl": 165.10470881321856,
    "ttft": 1895976.6622569053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1024,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3878762255050288,
    "arrivals": 375490,
    "finished_requests": 84434,
    "scheduler_time": 116.08052341415393
}
#Debug simulation 
Total elapsed time: 6.104730972088873. Arrivals time: 0.24007772328332067 Scheduler time: 5.759223067667335 Scheduler overhead time: 0.032983371522277594 Adapter cache time: 0.02206406556069851 Engine time: 0.03468328574672341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.090819988865405,
    "estimated_duration": 3600.10230792813,
    "input_throughput": 5823.056737536055,
    "output_throughput": 5141.127506082389,
    "total_throughput": 10964.184243618445,
    "itl": 167.05316014691024,
    "ttft": 1894435.6230646018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1026,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.06779160256956,
    "arrivals": 375490,
    "finished_requests": 84565,
    "scheduler_time": 115.73531953920227
}
#Debug simulation 
Total elapsed time: 6.090903182979673. Arrivals time: 0.28228427236899734 Scheduler time: 5.704837683588266 Scheduler overhead time: 0.03265665518119931 Adapter cache time: 0.02171979658305645 Engine time: 0.0338886184617877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.13281852286309,
    "estimated_duration": 3600.1396267277028,
    "input_throughput": 5814.6689213349555,
    "output_throughput": 5134.139760240475,
    "total_throughput": 10948.80868157543,
    "itl": 165.1061368816185,
    "ttft": 1895992.6500776005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1024,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4272371606529175,
    "arrivals": 375490,
    "finished_requests": 84434,
    "scheduler_time": 116.08060966687391
}
#Debug simulation 
Total elapsed time: 6.132905044127256. Arrivals time: 0.25298734987154603 Scheduler time: 5.774522609543055 Scheduler overhead time: 0.03315269062295556 Adapter cache time: 0.022120135370641947 Engine time: 0.03447968838736415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.227424204815179,
    "estimated_duration": 3600.0368263676455,
    "input_throughput": 5961.685403550428,
    "output_throughput": 5249.1032484983225,
    "total_throughput": 11210.788652048752,
    "itl": 163.5934228042523,
    "ttft": 1877084.8407885986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.494297511305245,
    "arrivals": 373265,
    "finished_requests": 86665,
    "scheduler_time": 118.15602188442617
}
#Debug simulation 
Total elapsed time: 6.2275121179409325. Arrivals time: 0.25602093944326043 Scheduler time: 5.8690539873205125 Scheduler overhead time: 0.0334242177195847 Adapter cache time: 0.018651851452887058 Engine time: 0.03459870256483555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.2648434760048985,
    "estimated_duration": 3600.016610052755,
    "input_throughput": 5961.571104997068,
    "output_throughput": 5248.880782170362,
    "total_throughput": 11210.45188716743,
    "itl": 163.60114423673082,
    "ttft": 1877140.4633312842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6571238669846267,
    "arrivals": 373265,
    "finished_requests": 86659,
    "scheduler_time": 118.15023960938375
}
#Debug simulation 
Total elapsed time: 6.264937708154321. Arrivals time: 0.2895543552003801 Scheduler time: 5.872482194099575 Scheduler overhead time: 0.0333642172627151 Adapter cache time: 0.01858813688158989 Engine time: 0.03489812696352601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.236055688932538,
    "estimated_duration": 3600.0875706404313,
    "input_throughput": 5953.024636061144,
    "output_throughput": 5241.813880833732,
    "total_throughput": 11194.838516894875,
    "itl": 161.94389176864362,
    "ttft": 1878597.7577632663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6490584434196225,
    "arrivals": 373265,
    "finished_requests": 86534,
    "scheduler_time": 118.45754726114006
}
#Debug simulation 
Total elapsed time: 6.236162692774087. Arrivals time: 0.29959072126075625 Scheduler time: 5.833065134007484 Scheduler overhead time: 0.033610386308282614 Adapter cache time: 0.018544372636824846 Engine time: 0.03528166515752673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.20900595607236,
    "estimated_duration": 3600.109498692388,
    "input_throughput": 5961.6572795342845,
    "output_throughput": 5249.030343900288,
    "total_throughput": 11210.687623434573,
    "itl": 163.59633621301813,
    "ttft": 1877099.932969092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5427171778212996,
    "arrivals": 373265,
    "finished_requests": 86666,
    "scheduler_time": 118.15650246702846
}
#Debug simulation 
Total elapsed time: 6.209114572033286. Arrivals time: 0.25836772471666336 Scheduler time: 5.848039076663554 Scheduler overhead time: 0.03346179611980915 Adapter cache time: 0.018763051368296146 Engine time: 0.03464221814647317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.216879558749497,
    "estimated_duration": 3600.064130514893,
    "input_throughput": 5953.0633963831115,
    "output_throughput": 5241.848010441139,
    "total_throughput": 11194.91140682425,
    "itl": 161.95776213116244,
    "ttft": 1878588.9238037274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6841437498293823,
    "arrivals": 373265,
    "finished_requests": 86534,
    "scheduler_time": 118.45299743519875
}
#Debug simulation 
Total elapsed time: 6.216973506845534. Arrivals time: 0.28683280060067773 Scheduler time: 5.82589796371758 Scheduler overhead time: 0.03359944513067603 Adapter cache time: 0.019380968529731035 Engine time: 0.03522666357457638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.240099136251956,
    "estimated_duration": 3600.16613373136,
    "input_throughput": 5962.134024563231,
    "output_throughput": 5249.201647373239,
    "total_throughput": 11211.335671936471,
    "itl": 163.59228429775507,
    "ttft": 1877026.0063809375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4368909903452236,
    "arrivals": 373265,
    "finished_requests": 86672,
    "scheduler_time": 118.16180713803845
}
#Debug simulation 
Total elapsed time: 6.240182827226818. Arrivals time: 0.28702792478725314 Scheduler time: 5.850081825628877 Scheduler overhead time: 0.033385803923010826 Adapter cache time: 0.018898725975304842 Engine time: 0.0349226756952703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.269301878288388,
    "estimated_duration": 3600.133195634387,
    "input_throughput": 5952.949192543285,
    "output_throughput": 5241.747450589728,
    "total_throughput": 11194.696643133013,
    "itl": 161.95240121996727,
    "ttft": 1878610.823319289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.716462472938021,
    "arrivals": 373265,
    "finished_requests": 86534,
    "scheduler_time": 118.45563362596812
}
#Debug simulation 
Total elapsed time: 6.269396001007408. Arrivals time: 0.25494141690433025 Scheduler time: 5.911322564817965 Scheduler overhead time: 0.03367580194026232 Adapter cache time: 0.018358084373176098 Engine time: 0.035181183367967606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.301382947247475,
    "estimated_duration": 3600.0275153212265,
    "input_throughput": 6026.7053259075055,
    "output_throughput": 5336.839209765225,
    "total_throughput": 11363.54453567273,
    "itl": 161.40968402561649,
    "ttft": 1863347.186876368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.903623376726203,
    "arrivals": 372565,
    "finished_requests": 87665,
    "scheduler_time": 119.95926759456191
}
#Debug simulation 
Total elapsed time: 6.301469148136675. Arrivals time: 0.28735026251524687 Scheduler time: 5.913307876791805 Scheduler overhead time: 0.03347518015652895 Adapter cache time: 0.01603434793651104 Engine time: 0.03522562189027667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.344750164076686,
    "estimated_duration": 3600.157586343842,
    "input_throughput": 6026.4875855153305,
    "output_throughput": 5336.64639372401,
    "total_throughput": 11363.133979239341,
    "itl": 161.4129814456142,
    "ttft": 1863377.3237293027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.027335363989701,
    "arrivals": 372565,
    "finished_requests": 87665,
    "scheduler_time": 119.9608238819206
}
#Debug simulation 
Total elapsed time: 6.344837743323296. Arrivals time: 0.25747483409941196 Scheduler time: 5.98584746196866 Scheduler overhead time: 0.0337815135717392 Adapter cache time: 0.01642622798681259 Engine time: 0.035268913488835096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.29868577234447,
    "estimated_duration": 3600.085881105812,
    "input_throughput": 6013.771258520616,
    "output_throughput": 5323.443282445604,
    "total_throughput": 11337.21454096622,
    "itl": 159.30644148639362,
    "ttft": 1865276.6982565993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0313401966914655,
    "arrivals": 372565,
    "finished_requests": 87461,
    "scheduler_time": 120.32844302949678
}
#Debug simulation 
Total elapsed time: 6.298772081267089. Arrivals time: 0.25533663388341665 Scheduler time: 5.941235846374184 Scheduler overhead time: 0.03417850099503994 Adapter cache time: 0.016372306272387505 Engine time: 0.03550092130899429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.324936219025403,
    "estimated_duration": 3600.0671396113353,
    "input_throughput": 6026.638992722325,
    "output_throughput": 5336.780469620413,
    "total_throughput": 11363.41946234274,
    "itl": 161.41092727601065,
    "ttft": 1863360.3740717294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9398959658434476,
    "arrivals": 372565,
    "finished_requests": 87665,
    "scheduler_time": 119.95970017094378
}
#Debug simulation 
Total elapsed time: 6.325038338080049. Arrivals time: 0.2899041995406151 Scheduler time: 5.933544042985886 Scheduler overhead time: 0.03368900436908007 Adapter cache time: 0.01637091301381588 Engine time: 0.03542388742789626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.275741785299033,
    "estimated_duration": 3600.107403452171,
    "input_throughput": 6013.735306685449,
    "output_throughput": 5323.411457564481,
    "total_throughput": 11337.14676424993,
    "itl": 159.29765114326926,
    "ttft": 1865285.3985308073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.054870740640913,
    "arrivals": 372565,
    "finished_requests": 87461,
    "scheduler_time": 120.3308000367194
}
#Debug simulation 
Total elapsed time: 6.275846905075014. Arrivals time: 0.27337876288220286 Scheduler time: 5.900536257773638 Scheduler overhead time: 0.034135712776333094 Adapter cache time: 0.016133436933159828 Engine time: 0.035481421276926994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.28791406005621,
    "estimated_duration": 3600.151588900653,
    "input_throughput": 6026.6476186424625,
    "output_throughput": 5336.77305678869,
    "total_throughput": 11363.420675431153,
    "itl": 161.40711292676417,
    "ttft": 1863369.7992015295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.859811283429124,
    "arrivals": 372565,
    "finished_requests": 87667,
    "scheduler_time": 119.96494637275293
}
#Debug simulation 
Total elapsed time: 6.288004463072866. Arrivals time: 0.2879423778504133 Scheduler time: 5.898492733482271 Scheduler overhead time: 0.034274786710739136 Adapter cache time: 0.01606530323624611 Engine time: 0.035184505861252546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.339131481014192,
    "estimated_duration": 3600.1321995267763,
    "input_throughput": 6013.69388680944,
    "output_throughput": 5323.374792325445,
    "total_throughput": 11337.068679134885,
    "itl": 159.2984419335547,
    "ttft": 1865296.1770846709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0793927289918037,
    "arrivals": 372565,
    "finished_requests": 87461,
    "scheduler_time": 120.33086520154397
}
#Debug simulation 
Total elapsed time: 6.339221911970526. Arrivals time: 0.2569958493113518 Scheduler time: 5.979682430624962 Scheduler overhead time: 0.034185264725238085 Adapter cache time: 0.01615050807595253 Engine time: 0.035910471342504025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.63422332610935,
    "estimated_duration": 3600.0083405146797,
    "input_throughput": 6161.444336217518,
    "output_throughput": 5419.694665821411,
    "total_throughput": 11581.13900203893,
    "itl": 158.35411161226628,
    "ttft": 1847769.0707875763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.484336555807401,
    "arrivals": 371093,
    "finished_requests": 89478,
    "scheduler_time": 121.88374542250506
}
#Debug simulation 
Total elapsed time: 6.634314429946244. Arrivals time: 0.4799217046238482 Scheduler time: 6.054269260261208 Scheduler overhead time: 0.034428040497004986 Adapter cache time: 0.013685607351362705 Engine time: 0.03567255847156048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.417989328037947,
    "estimated_duration": 3600.0231043674885,
    "input_throughput": 6161.41906786378,
    "output_throughput": 5419.672439415637,
    "total_throughput": 11581.091507279416,
    "itl": 158.3576364567523,
    "ttft": 1847777.0493236885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5858280387497572,
    "arrivals": 371093,
    "finished_requests": 89478,
    "scheduler_time": 121.88228954131694
}
#Debug simulation 
Total elapsed time: 6.4180735549889505. Arrivals time: 0.2634453773498535 Scheduler time: 6.0545624922961 Scheduler overhead time: 0.03426181944087148 Adapter cache time: 0.013628719840198755 Engine time: 0.035875477362424135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.418864362873137,
    "estimated_duration": 3600.149512564456,
    "input_throughput": 6151.943113113547,
    "output_throughput": 5411.835239620808,
    "total_throughput": 11563.778352734356,
    "itl": 157.01123749211607,
    "ttft": 1849167.1491704513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.588072809837768,
    "arrivals": 371093,
    "finished_requests": 89348,
    "scheduler_time": 122.12725175582565
}
#Debug simulation 
Total elapsed time: 6.418968569952995. Arrivals time: 0.2630763165652752 Scheduler time: 6.054546819999814 Scheduler overhead time: 0.03475579293444753 Adapter cache time: 0.013697715476155281 Engine time: 0.03631692519411445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.391378507018089,
    "estimated_duration": 3600.0872964266196,
    "input_throughput": 6161.309205478628,
    "output_throughput": 5419.575802888504,
    "total_throughput": 11580.885008367133,
    "itl": 158.3550597200435,
    "ttft": 1847789.9350134814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5135066673858033,
    "arrivals": 371093,
    "finished_requests": 89478,
    "scheduler_time": 121.88579595136684
}
#Debug simulation 
Total elapsed time: 6.391466177999973. Arrivals time: 0.26250462839379907 Scheduler time: 6.028841397259384 Scheduler overhead time: 0.03433993738144636 Adapter cache time: 0.013615761417895555 Engine time: 0.03581827133893967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.37502905074507,
    "estimated_duration": 3600.168908986404,
    "input_throughput": 6151.909968645207,
    "output_throughput": 5411.806082588882,
    "total_throughput": 11563.716051234089,
    "itl": 157.0117132016978,
    "ttft": 1849174.1470038076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6103312300331922,
    "arrivals": 371093,
    "finished_requests": 89348,
    "scheduler_time": 122.12742830114671
}
#Debug simulation 
Total elapsed time: 6.375141827855259. Arrivals time: 0.2650997703894973 Scheduler time: 6.009498585015535 Scheduler overhead time: 0.0342851746827364 Adapter cache time: 0.013827702961862087 Engine time: 0.03594385599717498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.39800257794559,
    "estimated_duration": 3600.1501532160732,
    "input_throughput": 6161.269129340316,
    "output_throughput": 5419.727836231986,
    "total_throughput": 11580.996965572302,
    "itl": 158.35304450641794,
    "ttft": 1847783.7084756999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4501743930275406,
    "arrivals": 371093,
    "finished_requests": 89480,
    "scheduler_time": 121.88955890126482
}
#Debug simulation 
Total elapsed time: 6.398087413981557. Arrivals time: 0.27906385669484735 Scheduler time: 6.019121490884572 Scheduler overhead time: 0.03435972984880209 Adapter cache time: 0.013502675574272871 Engine time: 0.03576895175501704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.3658802271820605,
    "estimated_duration": 3600.0129053483984,
    "input_throughput": 6151.832669015588,
    "output_throughput": 5411.627544739204,
    "total_throughput": 11563.460213754792,
    "itl": 157.01230228401104,
    "ttft": 1849162.4616700194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.629823066927491,
    "arrivals": 371093,
    "finished_requests": 89343,
    "scheduler_time": 122.12156181601401
}
#Debug simulation 
Total elapsed time: 6.365967703983188. Arrivals time: 0.2623794241808355 Scheduler time: 6.002638895530254 Scheduler overhead time: 0.034640766214579344 Adapter cache time: 0.013735380489379168 Engine time: 0.03611694695428014 
