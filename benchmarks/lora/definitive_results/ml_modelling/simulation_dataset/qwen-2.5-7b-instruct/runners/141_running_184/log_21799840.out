INFO 06-01 00:47:13 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:14 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.7133815260604024,
    "estimated_duration": 3599.8840770404463,
    "input_throughput": 2724.2227222112338,
    "output_throughput": 2387.0707545295913,
    "total_throughput": 5111.293476740825,
    "itl": 42.11641004644166,
    "ttft": 21695.492423502794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.94902059063859,
    "arrivals": 39791,
    "finished_requests": 39617,
    "scheduler_time": 20.362562012539478
}
#Debug simulation 
Total elapsed time: 3.7135031558573246. Arrivals time: 0.10505985328927636 Scheduler time: 3.2596983849070966 Scheduler overhead time: 0.09203632595017552 Adapter cache time: 0.12410435453057289 Engine time: 0.0898857256397605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.652659466955811,
    "estimated_duration": 3599.899839862208,
    "input_throughput": 2725.0899848301046,
    "output_throughput": 2387.9450491403227,
    "total_throughput": 5113.035033970427,
    "itl": 42.10534485258484,
    "ttft": 20350.582752648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.42853043522546,
    "arrivals": 39791,
    "finished_requests": 39627,
    "scheduler_time": 20.30240536054971
}
#Debug simulation 
Total elapsed time: 3.6527484441176057. Arrivals time: 0.10379127413034439 Scheduler time: 3.2030183225870132 Scheduler overhead time: 0.09055141871795058 Adapter cache time: 0.12649668054655194 Engine time: 0.08651739824563265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.637169284746051,
    "estimated_duration": 3599.9249579646025,
    "input_throughput": 2724.8765223001224,
    "output_throughput": 2387.8058849482613,
    "total_throughput": 5112.682407248383,
    "itl": 42.14263820717027,
    "ttft": 20459.871702709897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.654732655457344,
    "arrivals": 39791,
    "finished_requests": 39626,
    "scheduler_time": 20.318024269653343
}
#Debug simulation 
Total elapsed time: 3.6372448527254164. Arrivals time: 0.10360659100115299 Scheduler time: 3.186845249030739 Scheduler overhead time: 0.09055701643228531 Adapter cache time: 0.12664759159088135 Engine time: 0.08735973527655005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.651101815979928,
    "estimated_duration": 3599.892144042766,
    "input_throughput": 2725.0483090816347,
    "output_throughput": 2387.782649050911,
    "total_throughput": 5112.830958132546,
    "itl": 42.07474508506531,
    "ttft": 20327.862444805996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 36.82545943201584,
    "arrivals": 39791,
    "finished_requests": 39627,
    "scheduler_time": 20.289768650513242
}
#Debug simulation 
Total elapsed time: 3.6512009780853987. Arrivals time: 0.10371578531339765 Scheduler time: 3.198563065379858 Scheduler overhead time: 0.09056543605402112 Adapter cache time: 0.12798823649063706 Engine time: 0.08776633534580469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.6440214556641877,
    "estimated_duration": 3599.9051096826493,
    "input_throughput": 2724.891546062098,
    "output_throughput": 2387.8190502520706,
    "total_throughput": 5112.7105963141685,
    "itl": 42.1515388718789,
    "ttft": 20459.7573820042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.15077280633116,
    "arrivals": 39791,
    "finished_requests": 39626,
    "scheduler_time": 20.320954357723817
}
#Debug simulation 
Total elapsed time: 3.644094554707408. Arrivals time: 0.10361574729904532 Scheduler time: 3.1936384555883706 Scheduler overhead time: 0.09022816922515631 Adapter cache time: 0.12675115605816245 Engine time: 0.08795375796034932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.2413762626238167,
    "estimated_duration": 3600.0105237645053,
    "input_throughput": 2538.2967465452207,
    "output_throughput": 2240.2709510878008,
    "total_throughput": 4778.567697633021,
    "itl": 40.77288105903024,
    "ttft": 16038.711865177504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13963,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.733590368527466,
    "arrivals": 36981,
    "finished_requests": 36840,
    "scheduler_time": 17.208956319362525
}
#Debug simulation 
Total elapsed time: 3.2414488517679274. Arrivals time: 0.09800218744203448 Scheduler time: 2.7771712532266974 Scheduler overhead time: 0.09282191889360547 Adapter cache time: 0.1412667161785066 Engine time: 0.08884854335337877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.4051984627731144,
    "estimated_duration": 3600.018374420033,
    "input_throughput": 2537.970935071113,
    "output_throughput": 2240.144955176572,
    "total_throughput": 4778.115890247685,
    "itl": 40.82210463450878,
    "ttft": 16252.091375972925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.51880939561219,
    "arrivals": 36981,
    "finished_requests": 36838,
    "scheduler_time": 17.23024076054503
}
#Debug simulation 
Total elapsed time: 3.405276109930128. Arrivals time: 0.1012017298489809 Scheduler time: 2.932401588652283 Scheduler overhead time: 0.09341758443042636 Adapter cache time: 0.14313669688999653 Engine time: 0.09135204320773482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.3013449371792376,
    "estimated_duration": 3600.0238917514644,
    "input_throughput": 2538.28732107505,
    "output_throughput": 2240.2626322783262,
    "total_throughput": 4778.549953353377,
    "itl": 40.82471883068119,
    "ttft": 16060.105800960699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.58328100926222,
    "arrivals": 36981,
    "finished_requests": 36840,
    "scheduler_time": 17.230908956776688
}
#Debug simulation 
Total elapsed time: 3.3014150951057673. Arrivals time: 0.10009954869747162 Scheduler time: 2.8320143055170774 Scheduler overhead time: 0.09208648419007659 Adapter cache time: 0.14363987604156137 Engine time: 0.09034832520410419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.2807462150231004,
    "estimated_duration": 3600.035410080136,
    "input_throughput": 2538.339199223762,
    "output_throughput": 2240.2999085557526,
    "total_throughput": 4778.639107779514,
    "itl": 40.78788560087666,
    "ttft": 15945.985201185928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.53102832386076,
    "arrivals": 36981,
    "finished_requests": 36841,
    "scheduler_time": 17.215101730223893
}
#Debug simulation 
Total elapsed time: 3.2808184530586004. Arrivals time: 0.1004819581285119 Scheduler time: 2.8107091384008527 Scheduler overhead time: 0.09268031781539321 Adapter cache time: 0.1433144067414105 Engine time: 0.09034272190183401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.311228051315993,
    "estimated_duration": 3600.023483894568,
    "input_throughput": 2537.9673329563157,
    "output_throughput": 2240.141775762978,
    "total_throughput": 4778.1091087192935,
    "itl": 40.832877102729746,
    "ttft": 16261.061792551704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13928,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.17440993465961,
    "arrivals": 36981,
    "finished_requests": 36838,
    "scheduler_time": 17.23586180446141
}
#Debug simulation 
Total elapsed time: 3.311300101224333. Arrivals time: 0.10036776214838028 Scheduler time: 2.842016512528062 Scheduler overhead time: 0.09240883868187666 Adapter cache time: 0.14338798774406314 Engine time: 0.08979983534663916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.283987004775554,
    "estimated_duration": 3600.016921391245,
    "input_throughput": 2538.2922357122184,
    "output_throughput": 2240.266969879475,
    "total_throughput": 4778.559205591693,
    "itl": 40.75675596407757,
    "ttft": 16029.848926547811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13979,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.79791307244241,
    "arrivals": 36981,
    "finished_requests": 36840,
    "scheduler_time": 17.201559946233644
}
#Debug simulation 
Total elapsed time: 3.2840602891519666. Arrivals time: 0.10007187444716692 Scheduler time: 2.812918938230723 Scheduler overhead time: 0.0925184884108603 Adapter cache time: 0.14405512437224388 Engine time: 0.09110707463696599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.2876363238319755,
    "estimated_duration": 3600.03421088265,
    "input_throughput": 2538.019770028746,
    "output_throughput": 2240.1795448556877,
    "total_throughput": 4778.199314884434,
    "itl": 40.842559853147016,
    "ttft": 16165.3429565171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13925,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.713302954920685,
    "arrivals": 36981,
    "finished_requests": 36839,
    "scheduler_time": 17.239752958650033
}
#Debug simulation 
Total elapsed time: 3.2877092007547617. Arrivals time: 0.1000483869574964 Scheduler time: 2.818987669888884 Scheduler overhead time: 0.09172502532601357 Adapter cache time: 0.1436197878792882 Engine time: 0.08997739432379603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.123529575765133,
    "estimated_duration": 3600.012753129366,
    "input_throughput": 2471.2951342370698,
    "output_throughput": 2185.158647885855,
    "total_throughput": 4656.453782122925,
    "itl": 40.12042721812019,
    "ttft": 10731.125784439888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.976148392563125,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.938916999114836
}
#Debug simulation 
Total elapsed time: 3.1236299029551446. Arrivals time: 0.09795347834005952 Scheduler time: 2.65107664372772 Scheduler overhead time: 0.0922047640196979 Adapter cache time: 0.14788799546658993 Engine time: 0.09065261390060186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.119037225842476,
    "estimated_duration": 3600.0089120900784,
    "input_throughput": 2471.297770992126,
    "output_throughput": 2185.16097934681,
    "total_throughput": 4656.458750338937,
    "itl": 40.17450026881524,
    "ttft": 10746.330601333959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.86352031724435,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.96305341992521
}
#Debug simulation 
Total elapsed time: 3.119111024774611. Arrivals time: 0.09747242974117398 Scheduler time: 2.64826532965526 Scheduler overhead time: 0.09265454951673746 Adapter cache time: 0.14710564399138093 Engine time: 0.09016699809581041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.135319573804736,
    "estimated_duration": 3599.999141965669,
    "input_throughput": 2471.30447790669,
    "output_throughput": 2185.166909707841,
    "total_throughput": 4656.4713876145315,
    "itl": 40.17742855570989,
    "ttft": 10745.503033484594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.939042935511694,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.963460568219556
}
#Debug simulation 
Total elapsed time: 3.1353944316506386. Arrivals time: 0.09766914276406169 Scheduler time: 2.661507482174784 Scheduler overhead time: 0.0924123665317893 Adapter cache time: 0.14754239795729518 Engine time: 0.0923922797665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.114141213707626,
    "estimated_duration": 3600.0227703722308,
    "input_throughput": 2471.288257735134,
    "output_throughput": 2185.15256757296,
    "total_throughput": 4656.440825308094,
    "itl": 40.135413889168845,
    "ttft": 10734.875633893666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.78698207056754,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.94571124434237
}
#Debug simulation 
Total elapsed time: 3.1142118396237493. Arrivals time: 0.09827311988919973 Scheduler time: 2.640243532601744 Scheduler overhead time: 0.09224616875872016 Adapter cache time: 0.147103073541075 Engine time: 0.09232144383713603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.119416526053101,
    "estimated_duration": 3600.0059693728062,
    "input_throughput": 2471.29979108062,
    "output_throughput": 2185.1627655413363,
    "total_throughput": 4656.462556621956,
    "itl": 40.1882757049803,
    "ttft": 10749.82645016997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 47.553250245662205,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.968730382890215
}
#Debug simulation 
Total elapsed time: 3.1195169310085475. Arrivals time: 0.09793875040486455 Scheduler time: 2.6479482064023614 Scheduler overhead time: 0.09284634608775377 Adapter cache time: 0.14703766722232103 Engine time: 0.08999056927859783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.135588607750833,
    "estimated_duration": 3600.006410088866,
    "input_throughput": 2471.299488541851,
    "output_throughput": 2185.1624980317224,
    "total_throughput": 4656.461986573574,
    "itl": 40.10463597638584,
    "ttft": 10727.615860352756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.95805258686545,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.93084169329559
}
#Debug simulation 
Total elapsed time: 3.1356614166870713. Arrivals time: 0.09787876624614 Scheduler time: 2.662801625672728 Scheduler overhead time: 0.09286386659368873 Adapter cache time: 0.1479852320626378 Engine time: 0.09045324102044106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.1367820468731225,
    "estimated_duration": 3600.01557335591,
    "input_throughput": 2471.2931982420737,
    "output_throughput": 2185.156936048143,
    "total_throughput": 4656.450134290217,
    "itl": 40.199194504842154,
    "ttft": 10752.842820492595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.108114579097865,
    "arrivals": 35945,
    "finished_requests": 35849,
    "scheduler_time": 15.973499914170922
}
#Debug simulation 
Total elapsed time: 3.1368557140231133. Arrivals time: 0.09762913314625621 Scheduler time: 2.664434488862753 Scheduler overhead time: 0.09240063279867172 Adapter cache time: 0.14731440506875515 Engine time: 0.09113031160086393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.8549982979893684,
    "estimated_duration": 3600.032726818746,
    "input_throughput": 2329.4777121125394,
    "output_throughput": 2081.285246154164,
    "total_throughput": 4410.762958266703,
    "itl": 37.918593553892784,
    "ttft": 8287.02044302228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 40.680003092350304,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.293728732966487
}
#Debug simulation 
Total elapsed time: 2.855069264769554. Arrivals time: 0.09348828392103314 Scheduler time: 2.3847042564302683 Scheduler overhead time: 0.09443327505141497 Adapter cache time: 0.1431863703764975 Engine time: 0.09397561009973288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.8265967005863786,
    "estimated_duration": 3600.0305031192843,
    "input_throughput": 2329.4791510054406,
    "output_throughput": 2081.286531741294,
    "total_throughput": 4410.7656827467345,
    "itl": 37.97360437206991,
    "ttft": 8290.321786214487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.35894579882154,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.319176019646878
}
#Debug simulation 
Total elapsed time: 2.8266687686555088. Arrivals time: 0.09121095575392246 Scheduler time: 2.3617578842677176 Scheduler overhead time: 0.09410062665119767 Adapter cache time: 0.14196482207626104 Engine time: 0.09305926412343979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.8837592843919992,
    "estimated_duration": 3600.0080950969123,
    "input_throughput": 2329.4936507008724,
    "output_throughput": 2081.2994865774867,
    "total_throughput": 4410.79313727836,
    "itl": 37.97650630495744,
    "ttft": 8079.2364755196995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.43731542159662,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.319819611875658
}
#Debug simulation 
Total elapsed time: 2.8838301552459598. Arrivals time: 0.092103804461658 Scheduler time: 2.416366752702743 Scheduler overhead time: 0.09405264863744378 Adapter cache time: 0.14337046770378947 Engine time: 0.09270804096013308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.8947802647016943,
    "estimated_duration": 3600.018661503475,
    "input_throughput": 2329.357647412269,
    "output_throughput": 2081.1875449753884,
    "total_throughput": 4410.545192387657,
    "itl": 38.06664186609832,
    "ttft": 8312.326337661874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.082187859111826,
    "arrivals": 34024,
    "finished_requests": 33947,
    "scheduler_time": 13.371175203949727
}
#Debug simulation 
Total elapsed time: 2.89485318493098. Arrivals time: 0.09189674258232117 Scheduler time: 2.4279941390268505 Scheduler overhead time: 0.09432194894179702 Adapter cache time: 0.14361008629202843 Engine time: 0.09226666390895844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.8385033141821623,
    "estimated_duration": 3600.0155955881282,
    "input_throughput": 2329.488797292269,
    "output_throughput": 2081.295150271684,
    "total_throughput": 4410.783947563953,
    "itl": 37.987228813980806,
    "ttft": 8079.884621429658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.041163419786976,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.325417460105596
}
#Debug simulation 
Total elapsed time: 2.838578260038048. Arrivals time: 0.09236582461744547 Scheduler time: 2.372767307795584 Scheduler overhead time: 0.09406300261616707 Adapter cache time: 0.14249345008283854 Engine time: 0.0919622560031712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.852122575044632,
    "estimated_duration": 3600.035259835005,
    "input_throughput": 2329.4760730716707,
    "output_throughput": 2081.2837817436825,
    "total_throughput": 4410.759854815354,
    "itl": 37.900077974494344,
    "ttft": 8285.980753919344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.740758469547956,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.284579900058231
}
#Debug simulation 
Total elapsed time: 2.852197214961052. Arrivals time: 0.0948372702114284 Scheduler time: 2.380762870889157 Scheduler overhead time: 0.09437037538737059 Adapter cache time: 0.14270265866070986 Engine time: 0.094577566254884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.1-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.8845895659178495,
    "estimated_duration": 3600.017210234042,
    "input_throughput": 2329.4877524918284,
    "output_throughput": 2081.294216788727,
    "total_throughput": 4410.781969280556,
    "itl": 37.99895458670299,
    "ttft": 8186.371615788444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.544987687916034,
    "arrivals": 34024,
    "finished_requests": 33948,
    "scheduler_time": 13.330253773703374
}
#Debug simulation 
Total elapsed time: 2.8846620600670576. Arrivals time: 0.09197757765650749 Scheduler time: 2.4183962354436517 Scheduler overhead time: 0.09425213374197483 Adapter cache time: 0.14410101296380162 Engine time: 0.09106755023822188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.4196725017391145,
    "estimated_duration": 3599.7661543131157,
    "input_throughput": 1856.8986743738842,
    "output_throughput": 1665.6801422547207,
    "total_throughput": 3522.578816628605,
    "itl": 33.930370161221404,
    "ttft": 8177.852707988849,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 56.08955888303884,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.337326088455117
}
#Debug simulation 
Total elapsed time: 2.419746644794941. Arrivals time: 0.07776506571099162 Scheduler time: 1.917779900599271 Scheduler overhead time: 0.10193407721817493 Adapter cache time: 0.17488865228369832 Engine time: 0.09873828431591392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.441243678331375,
    "estimated_duration": 3599.7687443390982,
    "input_throughput": 1856.8973383392233,
    "output_throughput": 1665.6789438013886,
    "total_throughput": 3522.576282140612,
    "itl": 34.00145542528521,
    "ttft": 8182.099324198949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 59.779336695651615,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.37244876339873
}
#Debug simulation 
Total elapsed time: 2.4413164942525327. Arrivals time: 0.07820454565808177 Scheduler time: 1.9359625224024057 Scheduler overhead time: 0.10182198137044907 Adapter cache time: 0.17604486783966422 Engine time: 0.10030806716531515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.428154378198087,
    "estimated_duration": 3599.754843689534,
    "input_throughput": 1856.9045088495213,
    "output_throughput": 1665.6853759114322,
    "total_throughput": 3522.5898847609537,
    "itl": 34.00027692413925,
    "ttft": 8182.685227311314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 59.898697707893604,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.373601264080286
}
#Debug simulation 
Total elapsed time: 2.4282260914333165. Arrivals time: 0.07776123471558094 Scheduler time: 1.925740520004183 Scheduler overhead time: 0.10103020770475268 Adapter cache time: 0.176453092135489 Engine time: 0.098565558437258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.3916887869127095,
    "estimated_duration": 3599.754633746788,
    "input_throughput": 1856.9046171468003,
    "output_throughput": 1665.6854730565426,
    "total_throughput": 3522.590090203343,
    "itl": 33.95304988774965,
    "ttft": 8179.269664509029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 57.251728163538004,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.348403679677947
}
#Debug simulation 
Total elapsed time: 2.3917610151693225. Arrivals time: 0.07834498956799507 Scheduler time: 1.8868632740341127 Scheduler overhead time: 0.10158087406307459 Adapter cache time: 0.17828012024983764 Engine time: 0.09825587645173073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.427600882947445,
    "estimated_duration": 3599.776530046245,
    "input_throughput": 1856.8933221846767,
    "output_throughput": 1665.6753412198536,
    "total_throughput": 3522.5686634045305,
    "itl": 34.01463288329565,
    "ttft": 8183.388625174256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 60.67305325457054,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.380922597278849
}
#Debug simulation 
Total elapsed time: 2.4276727931573987. Arrivals time: 0.0773560511879623 Scheduler time: 1.9236595234833658 Scheduler overhead time: 0.10535123152658343 Adapter cache time: 0.1750925867818296 Engine time: 0.09727160912007093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.4323455770500004,
    "estimated_duration": 3599.773819110533,
    "input_throughput": 1856.8947205832078,
    "output_throughput": 1665.6765956149889,
    "total_throughput": 3522.5713161981967,
    "itl": 33.90426063311924,
    "ttft": 8176.057893470916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.77174130203405,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.324918418937652
}
#Debug simulation 
Total elapsed time: 2.432416836731136. Arrivals time: 0.07723456062376499 Scheduler time: 1.927850453183055 Scheduler overhead time: 0.10476535186171532 Adapter cache time: 0.1755429534241557 Engine time: 0.09801720874384046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.4202102976851165,
    "estimated_duration": 3599.7613666903812,
    "input_throughput": 1856.9011440182312,
    "output_throughput": 1665.6823575816009,
    "total_throughput": 3522.583501599832,
    "itl": 34.02967491219232,
    "ttft": 8184.476741917182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 18322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 61.43095102720455,
    "arrivals": 27120,
    "finished_requests": 27060,
    "scheduler_time": 5.388133961927566
}
#Debug simulation 
Total elapsed time: 2.420282101724297. Arrivals time: 0.07778872596099973 Scheduler time: 1.9153283494524658 Scheduler overhead time: 0.10190410818904638 Adapter cache time: 0.17695072293281555 Engine time: 0.09958445047959685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.271288793068379,
    "estimated_duration": 3600.028991839162,
    "input_throughput": 1727.0080363502163,
    "output_throughput": 1530.4982300115216,
    "total_throughput": 3257.506266361738,
    "itl": 31.113374730130147,
    "ttft": 6061.933539109684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.66253462078543,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.459523103838301
}
#Debug simulation 
Total elapsed time: 2.27136093005538. Arrivals time: 0.07396984426304698 Scheduler time: 1.7649988285265863 Scheduler overhead time: 0.10927498200908303 Adapter cache time: 0.16367332777008414 Engine time: 0.10666529415175319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.2401447859592736,
    "estimated_duration": 3600.0143784505995,
    "input_throughput": 1727.015046722074,
    "output_throughput": 1530.5044426992995,
    "total_throughput": 3257.5194894213737,
    "itl": 31.16637742847548,
    "ttft": 6062.462862650878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 52.96517413696392,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.481145922357322
}
#Debug simulation 
Total elapsed time: 2.240219875704497. Arrivals time: 0.07304115267470479 Scheduler time: 1.7416791217401624 Scheduler overhead time: 0.1094961310736835 Adapter cache time: 0.16127811325713992 Engine time: 0.10245779016986489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.2570020998828113,
    "estimated_duration": 3600.01239653961,
    "input_throughput": 1727.0159974938276,
    "output_throughput": 1530.5052852862798,
    "total_throughput": 3257.5212827801074,
    "itl": 31.168469417770538,
    "ttft": 6062.345431875885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 53.05950149252455,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.481959378268147
}
#Debug simulation 
Total elapsed time: 2.257075314875692. Arrivals time: 0.07356592919677496 Scheduler time: 1.754267669748515 Scheduler overhead time: 0.10875908471643925 Adapter cache time: 0.1608010958880186 Engine time: 0.1068763448856771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.2615344738587737,
    "estimated_duration": 3600.0280441917403,
    "input_throughput": 1727.0084909563175,
    "output_throughput": 1530.4986328896891,
    "total_throughput": 3257.5071238460064,
    "itl": 31.12901262508463,
    "ttft": 6062.089354139468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 50.6923072096672,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.4663194983201477
}
#Debug simulation 
Total elapsed time: 2.261605700943619. Arrivals time: 0.07362343231216073 Scheduler time: 1.7604337525554001 Scheduler overhead time: 0.10859175818040967 Adapter cache time: 0.1599732106551528 Engine time: 0.10673548048362136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.254104482010007,
    "estimated_duration": 3600.0215612053225,
    "input_throughput": 1727.0116009856324,
    "output_throughput": 1530.5013890403623,
    "total_throughput": 3257.512990025995,
    "itl": 31.17957450194274,
    "ttft": 6062.484146761664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 53.74657819923809,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.4864120999217962
}
#Debug simulation 
Total elapsed time: 2.2541769882664084. Arrivals time: 0.07334983302280307 Scheduler time: 1.7526790238916874 Scheduler overhead time: 0.10855253133922815 Adapter cache time: 0.1623113239184022 Engine time: 0.10508793126791716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.263100998941809,
    "estimated_duration": 3600.007773115816,
    "input_throughput": 1727.0182154687209,
    "output_throughput": 1530.5072508860783,
    "total_throughput": 3257.5254663547994,
    "itl": 31.09407196303514,
    "ttft": 6061.827687742919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.51655608509335,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.452106868680682
}
#Debug simulation 
Total elapsed time: 2.263174946885556. Arrivals time: 0.07361504528671503 Scheduler time: 1.7556293425150216 Scheduler overhead time: 0.11249433364719152 Adapter cache time: 0.16212563822045922 Engine time: 0.1065817167982459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.2367688389495015,
    "estimated_duration": 3600.0290607844695,
    "input_throughput": 1727.0080032757332,
    "output_throughput": 1530.4982007004608,
    "total_throughput": 3257.506203976194,
    "itl": 31.19225032979806,
    "ttft": 6062.42793100066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 54.423844335193834,
    "arrivals": 25185,
    "finished_requests": 25143,
    "scheduler_time": 2.491116360412855
}
#Debug simulation 
Total elapsed time: 2.236847361084074. Arrivals time: 0.07331794453784823 Scheduler time: 1.735750642605126 Scheduler overhead time: 0.1085470519028604 Adapter cache time: 0.16169810248538852 Engine time: 0.10498133627697825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.1854257751256227,
    "estimated_duration": 3599.8688394543187,
    "input_throughput": 1634.7109471055148,
    "output_throughput": 1468.5326704295567,
    "total_throughput": 3103.2436175350717,
    "itl": 29.872937128988113,
    "ttft": 5855.962275231545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.05728005843566,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5475123132461952
}
#Debug simulation 
Total elapsed time: 2.1854983922094107. Arrivals time: 0.07053642254322767 Scheduler time: 1.6880696029402316 Scheduler overhead time: 0.11163509031757712 Adapter cache time: 0.15385899180546403 Engine time: 0.10730618983507156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.1765735428780317,
    "estimated_duration": 3599.8629760445265,
    "input_throughput": 1634.7136097013522,
    "output_throughput": 1468.5350623563877,
    "total_throughput": 3103.24867205774,
    "itl": 29.91835992386082,
    "ttft": 5856.1559685348475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.10413421811837,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5617815781233864
}
#Debug simulation 
Total elapsed time: 2.1766418647021055. Arrivals time: 0.07037818059325218 Scheduler time: 1.673641995061189 Scheduler overhead time: 0.11425478896126151 Adapter cache time: 0.15372741175815463 Engine time: 0.11033945623785257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.1851498018950224,
    "estimated_duration": 3599.8606971909326,
    "input_throughput": 1634.7146445394467,
    "output_throughput": 1468.5359919969171,
    "total_throughput": 3103.250636536364,
    "itl": 29.920269715381245,
    "ttft": 5856.066944276484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.190495066206786,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5623294972338782
}
#Debug simulation 
Total elapsed time: 2.1852254127152264. Arrivals time: 0.07105246698483825 Scheduler time: 1.6774427313357592 Scheduler overhead time: 0.1150590730831027 Adapter cache time: 0.15647992445155978 Engine time: 0.11078499909490347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.1928544589318335,
    "estimated_duration": 3599.885805572846,
    "input_throughput": 1634.7032427778822,
    "output_throughput": 1468.5257492935282,
    "total_throughput": 3103.22899207141,
    "itl": 29.885884680367724,
    "ttft": 5856.004444600889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.967459414927994,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5516635375165537
}
#Debug simulation 
Total elapsed time: 2.192927877884358. Arrivals time: 0.07153174374252558 Scheduler time: 1.6865032743662596 Scheduler overhead time: 0.11289122654125094 Adapter cache time: 0.15535361832007766 Engine time: 0.10987004218623042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.18299391400069,
    "estimated_duration": 3599.879853149102,
    "input_throughput": 1634.705945769869,
    "output_throughput": 1468.5281775099952,
    "total_throughput": 3103.2341232798644,
    "itl": 29.929243721902985,
    "ttft": 5856.195401528917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.84041980617385,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5656334865986894
}
#Debug simulation 
Total elapsed time: 2.183063083793968. Arrivals time: 0.07096489146351814 Scheduler time: 1.6790729146450758 Scheduler overhead time: 0.11538893077522516 Adapter cache time: 0.15366853075101972 Engine time: 0.10934190824627876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.1928291651420295,
    "estimated_duration": 3599.8770575025633,
    "input_throughput": 1634.70721527434,
    "output_throughput": 1468.5293179616415,
    "total_throughput": 3103.2365332359814,
    "itl": 29.855076115023913,
    "ttft": 5855.784948307523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15045,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.98530668681086,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.542478616280544
}
#Debug simulation 
Total elapsed time: 2.1929068779572845. Arrivals time: 0.0714194537140429 Scheduler time: 1.6916857240721583 Scheduler overhead time: 0.1116382060572505 Adapter cache time: 0.15479967929422855 Engine time: 0.10964440880343318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.1897537307813764,
    "estimated_duration": 3599.86121408874,
    "input_throughput": 1634.7144098136155,
    "output_throughput": 1468.5357811323895,
    "total_throughput": 3103.250190946005,
    "itl": 29.93629523271325,
    "ttft": 5856.256552145977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 15037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 50.457447910049886,
    "arrivals": 24188,
    "finished_requests": 24149,
    "scheduler_time": 1.5685085267416254
}
#Debug simulation 
Total elapsed time: 2.189848619978875. Arrivals time: 0.07086665835231543 Scheduler time: 1.6877198559232056 Scheduler overhead time: 0.1116390647366643 Adapter cache time: 0.15566209610551596 Engine time: 0.10960564482957125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.0038611441850662,
    "estimated_duration": 3599.9525108599287,
    "input_throughput": 1450.992751777231,
    "output_throughput": 1297.4426706768672,
    "total_throughput": 2748.4354224540984,
    "itl": 27.277636944619523,
    "ttft": 6655.397200827308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.70904898525926,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.19859638913435904
}
#Debug simulation 
Total elapsed time: 2.003935490269214. Arrivals time: 0.06560392724350095 Scheduler time: 1.5015414184890687 Scheduler overhead time: 0.11868523480370641 Adapter cache time: 0.13699670508503914 Engine time: 0.12334166280925274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9747326308861375,
    "estimated_duration": 3599.9680158333763,
    "input_throughput": 1450.9865023872392,
    "output_throughput": 1297.4370826232873,
    "total_throughput": 2748.4235850105265,
    "itl": 27.311475892405998,
    "ttft": 6655.506429900794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12649,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.29829234014699,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.2014124518671463
}
#Debug simulation 
Total elapsed time: 1.9748112838715315. Arrivals time: 0.06389089254662395 Scheduler time: 1.4802384707145393 Scheduler overhead time: 0.118953219614923 Adapter cache time: 0.13773504691198468 Engine time: 0.11565778264775872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9966548001393676,
    "estimated_duration": 3599.9569830426894,
    "input_throughput": 1450.9909492265892,
    "output_throughput": 1297.441058879623,
    "total_throughput": 2748.432008106212,
    "itl": 27.310504177279853,
    "ttft": 6655.683387901997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.351216759818776,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.20152082769825086
}
#Debug simulation 
Total elapsed time: 1.9967329064384103. Arrivals time: 0.06335250334814191 Scheduler time: 1.502929789479822 Scheduler overhead time: 0.12069160584360361 Adapter cache time: 0.13707866054028273 Engine time: 0.11427991511300206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.9820249816402793,
    "estimated_duration": 3599.9668718166563,
    "input_throughput": 1450.9869634894878,
    "output_throughput": 1297.4374949297803,
    "total_throughput": 2748.424458419268,
    "itl": 27.287366262804937,
    "ttft": 6655.537921248028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.47991329002142,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.19938124888438355
}
#Debug simulation 
Total elapsed time: 1.982099249958992. Arrivals time: 0.06449825409799814 Scheduler time: 1.4874727367423475 Scheduler overhead time: 0.11852258816361427 Adapter cache time: 0.13708638865500689 Engine time: 0.11640829592943192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.033770287875086,
    "estimated_duration": 3599.950079670732,
    "input_throughput": 1450.9937316902365,
    "output_throughput": 1297.4435468914075,
    "total_throughput": 2748.437278581644,
    "itl": 27.316509425459014,
    "ttft": 6655.660699439657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12649,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.925581414516465,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.20217054081089594
}
#Debug simulation 
Total elapsed time: 2.0338453110307455. Arrivals time: 0.06435402017086744 Scheduler time: 1.537614403758198 Scheduler overhead time: 0.11778099276125431 Adapter cache time: 0.13911390071734786 Engine time: 0.11735016852617264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.012656817212701,
    "estimated_duration": 3599.9451538797634,
    "input_throughput": 1450.9957170793227,
    "output_throughput": 1297.4453221783724,
    "total_throughput": 2748.441039257695,
    "itl": 27.265536794862992,
    "ttft": 6655.297495859297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.8121760293344,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.19764852771066377
}
#Debug simulation 
Total elapsed time: 2.0127311423420906. Arrivals time: 0.06416134536266327 Scheduler time: 1.518253639806062 Scheduler overhead time: 0.1188175706192851 Adapter cache time: 0.1373591492883861 Engine time: 0.11608519731089473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9970240090042353,
    "estimated_duration": 3599.9527305754436,
    "input_throughput": 1450.9926632189515,
    "output_throughput": 1297.4425914901929,
    "total_throughput": 2748.4352547091444,
    "itl": 27.32373440962161,
    "ttft": 6655.627249977219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.430134777389256,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.20263631934565896
}
#Debug simulation 
Total elapsed time: 1.9971260260790586. Arrivals time: 0.06458680424839258 Scheduler time: 1.501017123926431 Scheduler overhead time: 0.11981137562543154 Adapter cache time: 0.13684593373909593 Engine time: 0.11668709432706237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.0032030162401497,
    "estimated_duration": 3600.01863962286,
    "input_throughput": 1394.8711111468194,
    "output_throughput": 1245.3616074803645,
    "total_throughput": 2640.2327186271837,
    "itl": 26.768979465415867,
    "ttft": 6209.887196970968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.82529003821184,
    "arrivals": 20441,
    "finished_requests": 20406,
    "scheduler_time": 0.13718037847386252
}
#Debug simulation 
Total elapsed time: 2.0032744710333645. Arrivals time: 0.06277065072208643 Scheduler time: 1.5112378522753716 Scheduler overhead time: 0.12111502094194293 Adapter cache time: 0.13019360648468137 Engine time: 0.11882105702534318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.955813033040613,
    "estimated_duration": 3600.023800628085,
    "input_throughput": 1394.9638330512826,
    "output_throughput": 1245.5231543796197,
    "total_throughput": 2640.4869874309024,
    "itl": 26.584514699243982,
    "ttft": 6033.576343095373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.28563114548013,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.1277538881214482
}
#Debug simulation 
Total elapsed time: 1.95588942989707. Arrivals time: 0.0626117018982768 Scheduler time: 1.4598652147687972 Scheduler overhead time: 0.12093637092038989 Adapter cache time: 0.13199233962222934 Engine time: 0.12098412308841944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9490582309663296,
    "estimated_duration": 3600.0026642133566,
    "input_throughput": 1394.877301041462,
    "output_throughput": 1245.3671339100688,
    "total_throughput": 2640.244434951531,
    "itl": 26.58476941277027,
    "ttft": 6209.86131115921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.34939016061367,
    "arrivals": 20441,
    "finished_requests": 20406,
    "scheduler_time": 0.12772115429110656
}
#Debug simulation 
Total elapsed time: 1.9491354688070714. Arrivals time: 0.0627451203763485 Scheduler time: 1.4559927792288363 Scheduler overhead time: 0.12063522124662995 Adapter cache time: 0.131289922632277 Engine time: 0.11948006832972169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.9509109170176089,
    "estimated_duration": 3600.0140052880756,
    "input_throughput": 1394.9676286323624,
    "output_throughput": 1245.5265433449874,
    "total_throughput": 2640.4941719773497,
    "itl": 26.5659649517452,
    "ttft": 6033.460341065056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.631228701114814,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.12652872803692863
}
#Debug simulation 
Total elapsed time: 1.9509840472601354. Arrivals time: 0.0635174042545259 Scheduler time: 1.4522255538031459 Scheduler overhead time: 0.12366463197395205 Adapter cache time: 0.13112175790593028 Engine time: 0.12050736322999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.9374674456194043,
    "estimated_duration": 3600.0201661186693,
    "input_throughput": 1394.9652413792785,
    "output_throughput": 1245.5244118352516,
    "total_throughput": 2640.48965321453,
    "itl": 26.592557747242605,
    "ttft": 6033.685135684489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.8487621902691,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.12814969556748973
}
#Debug simulation 
Total elapsed time: 1.9375427598133683. Arrivals time: 0.061236982233822346 Scheduler time: 1.443898015189916 Scheduler overhead time: 0.12525029992684722 Adapter cache time: 0.13077492266893387 Engine time: 0.11686200462281704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.9297249037772417,
    "estimated_duration": 3600.014071984507,
    "input_throughput": 1394.8728809362306,
    "output_throughput": 1245.3631875745884,
    "total_throughput": 2640.236068510819,
    "itl": 26.759092860035818,
    "ttft": 6209.793780905238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.02677235598501,
    "arrivals": 20441,
    "finished_requests": 20406,
    "scheduler_time": 0.1366147723440741
}
#Debug simulation 
Total elapsed time: 1.9297971660271287. Arrivals time: 0.06261568097397685 Scheduler time: 1.4379844833165407 Scheduler overhead time: 0.1200684062205255 Adapter cache time: 0.13086006231606007 Engine time: 0.11939809983596206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9273627898655832,
    "estimated_duration": 3600.011776691275,
    "input_throughput": 1394.8737702784,
    "output_throughput": 1245.363981592462,
    "total_throughput": 2640.237751870862,
    "itl": 26.595517405324067,
    "ttft": 6209.865084588686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.312236047237,
    "arrivals": 20441,
    "finished_requests": 20406,
    "scheduler_time": 0.1284947136030167
}
#Debug simulation 
Total elapsed time: 1.9274341869167984. Arrivals time: 0.06187784764915705 Scheduler time: 1.4387609749101102 Scheduler overhead time: 0.12096401024609804 Adapter cache time: 0.1294843005016446 Engine time: 0.11711436044424772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.8144710739143193,
    "estimated_duration": 3599.8624491478436,
    "input_throughput": 1286.3260931250722,
    "output_throughput": 1128.3914475569939,
    "total_throughput": 2414.717540682066,
    "itl": 24.9597731585343,
    "ttft": 4504.1873474750955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.330155553321372,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0011095714554604371
}
#Debug simulation 
Total elapsed time: 1.8145440509542823. Arrivals time: 0.05796665232628584 Scheduler time: 1.3281700280494988 Scheduler overhead time: 0.12558104237541556 Adapter cache time: 0.11632772395387292 Engine time: 0.12546358490362763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.7910198532044888,
    "estimated_duration": 3599.8679228170736,
    "input_throughput": 1286.3241372412158,
    "output_throughput": 1128.3897318158392,
    "total_throughput": 2414.713869057055,
    "itl": 24.97782742978033,
    "ttft": 4504.271718535753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.145217499196104,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0011421801637981968
}
#Debug simulation 
Total elapsed time: 1.7911042552441359. Arrivals time: 0.05715280398726463 Scheduler time: 1.3110073707066476 Scheduler overhead time: 0.12510043755173683 Adapter cache time: 0.11487571662291884 Engine time: 0.12207852769643068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.7983502568677068,
    "estimated_duration": 3599.849822871936,
    "input_throughput": 1286.3306048433267,
    "output_throughput": 1128.395405328137,
    "total_throughput": 2414.7260101714637,
    "itl": 24.978963746415115,
    "ttft": 4504.4059975618175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.206871208605456,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.001138010233753065
}
#Debug simulation 
Total elapsed time: 1.7984202499501407. Arrivals time: 0.057532789185643196 Scheduler time: 1.318782763555646 Scheduler overhead time: 0.12443931214511395 Adapter cache time: 0.1146603231318295 Engine time: 0.12227918300777674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.7937827953137457,
    "estimated_duration": 3599.847250447193,
    "input_throughput": 1286.3315240458498,
    "output_throughput": 1128.3962116713117,
    "total_throughput": 2414.7277357171615,
    "itl": 24.965661045473777,
    "ttft": 4504.259044133574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8929,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.83160249653242,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0011129073994965426
}
#Debug simulation 
Total elapsed time: 1.793896920979023. Arrivals time: 0.057511752005666494 Scheduler time: 1.3142714644782245 Scheduler overhead time: 0.12431825138628483 Adapter cache time: 0.11439778935164213 Engine time: 0.12275106413289905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.799655127339065,
    "estimated_duration": 3599.8715129215275,
    "input_throughput": 1286.322854407093,
    "output_throughput": 1128.3886064876192,
    "total_throughput": 2414.711460894712,
    "itl": 24.982766564552495,
    "ttft": 4504.280380169768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8934,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.60527378985574,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.001164072176022496
}
#Debug simulation 
Total elapsed time: 1.7997556123882532. Arrivals time: 0.05752786621451378 Scheduler time: 1.3162475628778338 Scheduler overhead time: 0.12616363866254687 Adapter cache time: 0.11479050340130925 Engine time: 0.1234069773927331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.7879447750747204,
    "estimated_duration": 3599.8576082454533,
    "input_throughput": 1286.3278229098962,
    "output_throughput": 1128.3929649594718,
    "total_throughput": 2414.720787869368,
    "itl": 24.953212085168417,
    "ttft": 4504.122996669291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.707129234064514,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0010836346351948886
}
#Debug simulation 
Total elapsed time: 1.7880157749168575. Arrivals time: 0.05691437469795346 Scheduler time: 1.3088485044427216 Scheduler overhead time: 0.12437700340524316 Adapter cache time: 0.11436670087277889 Engine time: 0.12296782294288278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.776774327736348,
    "estimated_duration": 3599.85638306892,
    "input_throughput": 1286.328260699212,
    "output_throughput": 1128.3933489971762,
    "total_throughput": 2414.721609696388,
    "itl": 24.987128878890893,
    "ttft": 4504.370656785805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8932,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.94790200188506,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0011616953399992995
}
#Debug simulation 
Total elapsed time: 1.7768448079004884. Arrivals time: 0.05608436558395624 Scheduler time: 1.299235642887652 Scheduler overhead time: 0.12654139567166567 Adapter cache time: 0.11459977691993117 Engine time: 0.11951907211914659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4649517498910427,
    "estimated_duration": 3599.620496178267,
    "input_throughput": 924.0435216799792,
    "output_throughput": 829.5760631351051,
    "total_throughput": 1753.6195848150842,
    "itl": 22.70644176399417,
    "ttft": 5834.868986624786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.69207259937189,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00011638219320147351
}
#Debug simulation 
Total elapsed time: 1.4650200679898262. Arrivals time: 0.0458968305028975 Scheduler time: 0.9859531777910888 Scheduler overhead time: 0.13348523760214448 Adapter cache time: 0.10110210720449686 Engine time: 0.13261767011135817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4740407029166818,
    "estimated_duration": 3599.6393539933842,
    "input_throughput": 924.0386807944964,
    "output_throughput": 829.5717171463862,
    "total_throughput": 1753.6103979408826,
    "itl": 22.722989748401755,
    "ttft": 5834.9635824770885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.61567830852043,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00011805016521952632
}
#Debug simulation 
Total elapsed time: 1.474111310672015. Arrivals time: 0.04592388914898038 Scheduler time: 0.9963792469352484 Scheduler overhead time: 0.1333819436840713 Adapter cache time: 0.10109583614394069 Engine time: 0.13115596724674106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.463337177876383,
    "estimated_duration": 3599.631339853634,
    "input_throughput": 924.0407380538164,
    "output_throughput": 829.5735640865438,
    "total_throughput": 1753.6143021403602,
    "itl": 22.72417872055451,
    "ttft": 5835.006100527475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 30.662361596283972,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00012126098725174922
}
#Debug simulation 
Total elapsed time: 1.4634111728519201. Arrivals time: 0.04540541581809521 Scheduler time: 0.9906447604298592 Scheduler overhead time: 0.1323057720437646 Adapter cache time: 0.10095289908349514 Engine time: 0.12909362139180303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.463459141086787,
    "estimated_duration": 3599.622653306032,
    "input_throughput": 924.0429679330649,
    "output_throughput": 829.5755659992295,
    "total_throughput": 1753.6185339322944,
    "itl": 22.712796273273245,
    "ttft": 5834.86978687699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.312519008828396,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00011638219320147351
}
#Debug simulation 
Total elapsed time: 1.4635298219509423. Arrivals time: 0.04568735556676984 Scheduler time: 0.9874766217544675 Scheduler overhead time: 0.13285553036257625 Adapter cache time: 0.10037815244868398 Engine time: 0.13177386624738574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.460537894628942,
    "estimated_duration": 3599.631738884069,
    "input_throughput": 924.040635621011,
    "output_throughput": 829.57347212572,
    "total_throughput": 1753.614107746731,
    "itl": 22.726228969867574,
    "ttft": 5835.1642439262405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.0787140534067,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00012292895926980203
}
#Debug simulation 
Total elapsed time: 1.4606075300835073. Arrivals time: 0.04536172188818455 Scheduler time: 0.9876036141067743 Scheduler overhead time: 0.13277806341648102 Adapter cache time: 0.10012596659362316 Engine time: 0.12976263323798776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4573007808066905,
    "estimated_duration": 3599.6401025915393,
    "input_throughput": 924.0384886270485,
    "output_throughput": 829.5715446247342,
    "total_throughput": 1753.6100332517829,
    "itl": 22.700437959433355,
    "ttft": 5834.668954373368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.034711565000638,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00011638219320147351
}
#Debug simulation 
Total elapsed time: 1.4573737988248467. Arrivals time: 0.04524017730727792 Scheduler time: 0.9823352796956897 Scheduler overhead time: 0.1333343326114118 Adapter cache time: 0.10021285805851221 Engine time: 0.1310600838623941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4596704519353807,
    "estimated_duration": 3599.6198722730646,
    "input_throughput": 924.043681840102,
    "output_throughput": 829.5762069216269,
    "total_throughput": 1753.6198887617288,
    "itl": 22.728990926455303,
    "ttft": 5835.091626379503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 9374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 31.457502787781102,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 0.00011888415122855272
}
#Debug simulation 
Total elapsed time: 1.459741062950343. Arrivals time: 0.04533756338059902 Scheduler time: 0.9861987549811602 Scheduler overhead time: 0.13259710557758808 Adapter cache time: 0.1000969847664237 Engine time: 0.13044942077249289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3853528327308595,
    "estimated_duration": 3599.964199965546,
    "input_throughput": 865.2580489633235,
    "output_throughput": 760.4639512876826,
    "total_throughput": 1625.722000251006,
    "itl": 21.946693719056437,
    "ttft": 5430.514000045125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.454076566289984,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3854232667945325. Arrivals time: 0.04288907581940293 Scheduler time: 0.9154131556861103 Scheduler overhead time: 0.136018390301615 Adapter cache time: 0.09229375654831529 Engine time: 0.13233102299273014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.383144340943545,
    "estimated_duration": 3599.97314659884,
    "input_throughput": 865.3914551955297,
    "output_throughput": 760.4817837562262,
    "total_throughput": 1625.8732389517559,
    "itl": 21.95929217400209,
    "ttft": 5146.878852662031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.15134905907728,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3832134529948235. Arrivals time: 0.042702626436948776 Scheduler time: 0.9149542921222746 Scheduler overhead time: 0.1355214654468 Adapter cache time: 0.09211127134039998 Engine time: 0.1316054416820407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3909764187410474,
    "estimated_duration": 3599.9580236271822,
    "input_throughput": 865.2595334602113,
    "output_throughput": 760.4652559925279,
    "total_throughput": 1625.7247894527393,
    "itl": 21.959666295952257,
    "ttft": 5430.576676995547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.19680247826576,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3910474348813295. Arrivals time: 0.04334311420097947 Scheduler time: 0.9180415635928512 Scheduler overhead time: 0.1364965084940195 Adapter cache time: 0.09344259928911924 Engine time: 0.13279407005757093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.3884618747979403,
    "estimated_duration": 3599.974725058965,
    "input_throughput": 865.3910757523366,
    "output_throughput": 760.4814503121709,
    "total_throughput": 1625.8725260645074,
    "itl": 21.950042149140003,
    "ttft": 5146.711041675479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 26.00835912421375,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3885304578579962. Arrivals time: 0.04294734774157405 Scheduler time: 0.9176108897663653 Scheduler overhead time: 0.13624286651611328 Adapter cache time: 0.09278869582340121 Engine time: 0.13237504614517093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.3978508422151208,
    "estimated_duration": 3599.9635915126396,
    "input_throughput": 865.2581952061288,
    "output_throughput": 760.4640798185661,
    "total_throughput": 1625.722275024695,
    "itl": 21.962663876962985,
    "ttft": 5430.772705127152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.55709791842775,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3979214751161635. Arrivals time: 0.04320546193048358 Scheduler time: 0.924046125728637 Scheduler overhead time: 0.1354618943296373 Adapter cache time: 0.09321814589202404 Engine time: 0.13486656825989485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3921552249230444,
    "estimated_duration": 3599.961542534617,
    "input_throughput": 865.2586876822303,
    "output_throughput": 760.4645126493529,
    "total_throughput": 1625.723200331583,
    "itl": 21.94143933778499,
    "ttft": 5430.37479772105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.868248302700287,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3922239318490028. Arrivals time: 0.04260996077209711 Scheduler time: 0.9173107696697116 Scheduler overhead time: 0.14041123259812593 Adapter cache time: 0.09229378262534738 Engine time: 0.13237720215693116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3909301082603633,
    "estimated_duration": 3599.967040813141,
    "input_throughput": 865.2573661608924,
    "output_throughput": 760.4633511815808,
    "total_throughput": 1625.720717342473,
    "itl": 21.963819638813057,
    "ttft": 5430.655165127088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 8315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.896888393124136,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.390998611226678. Arrivals time: 0.04352266527712345 Scheduler time: 0.9184150486253202 Scheduler overhead time: 0.13681684248149395 Adapter cache time: 0.09274344798177481 Engine time: 0.1326259863562882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.2643922180868685,
    "estimated_duration": 3599.792259525768,
    "input_throughput": 725.3421341441466,
    "output_throughput": 655.0728014262291,
    "total_throughput": 1380.4149355703757,
    "itl": 21.2470830701209,
    "ttft": 2724.866510714691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.516730343058512,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2644484690390527. Arrivals time: 0.038445466198027134 Scheduler time: 0.8022496588528156 Scheduler overhead time: 0.1389501322992146 Adapter cache time: 0.08036461193114519 Engine time: 0.13599620340391994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.2819536556489766,
    "estimated_duration": 3599.7895474166967,
    "input_throughput": 725.342680622477,
    "output_throughput": 655.073294963105,
    "total_throughput": 1380.415975585582,
    "itl": 21.256605573727448,
    "ttft": 2725.0571626716032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.810209763738598,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2820170829072595. Arrivals time: 0.039493422489613295 Scheduler time: 0.8147437353618443 Scheduler overhead time: 0.1402777717448771 Adapter cache time: 0.08074695616960526 Engine time: 0.13798002060502768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.2742048134095967,
    "estimated_duration": 3599.7868718032,
    "input_throughput": 725.3432197479127,
    "output_throughput": 655.0737818594163,
    "total_throughput": 1380.417001607329,
    "itl": 21.255818767466863,
    "ttft": 2724.8673917908986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.846880262772977,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2743191001936793. Arrivals time: 0.03888665931299329 Scheduler time: 0.8092993055470288 Scheduler overhead time: 0.1392438947223127 Adapter cache time: 0.08045617630705237 Engine time: 0.13803822873160243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.272702518850565,
    "estimated_duration": 3599.7950290696253,
    "input_throughput": 725.3415760938032,
    "output_throughput": 655.072297438408,
    "total_throughput": 1380.413873532211,
    "itl": 21.249607550826116,
    "ttft": 2724.845760787085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.875492504334666,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2727710106410086. Arrivals time: 0.03845948725938797 Scheduler time: 0.8096929965540767 Scheduler overhead time: 0.13904024567455053 Adapter cache time: 0.08018066454678774 Engine time: 0.13680486427620053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.277879502158612,
    "estimated_duration": 3599.78793370059,
    "input_throughput": 725.343005779733,
    "output_throughput": 655.0735886199388,
    "total_throughput": 1380.4165943996718,
    "itl": 21.257660492896715,
    "ttft": 2725.0393082697924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.12908634537722,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2779360502026975. Arrivals time: 0.038330595940351486 Scheduler time: 0.808255230076611 Scheduler overhead time: 0.14543919544667006 Adapter cache time: 0.07988181710243225 Engine time: 0.13513319240882993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.2592322560958564,
    "estimated_duration": 3599.7942810392265,
    "input_throughput": 725.3417268184019,
    "output_throughput": 655.0724335611843,
    "total_throughput": 1380.4141603795863,
    "itl": 21.24323787469859,
    "ttft": 2724.604041836924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.064560680297095,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2592916167341173. Arrivals time: 0.03827269887551665 Scheduler time: 0.8001849418506026 Scheduler overhead time: 0.13922617677599192 Adapter cache time: 0.07879517646506429 Engine time: 0.13399119349196553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.2609374490566552,
    "estimated_duration": 3599.7773872759344,
    "input_throughput": 725.3451308487405,
    "output_throughput": 655.0755078175733,
    "total_throughput": 1380.4206386663138,
    "itl": 21.261223941694087,
    "ttft": 2724.943065006201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.379713641701475,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2610081918537617. Arrivals time: 0.038232375867664814 Scheduler time: 0.8026342913508415 Scheduler overhead time: 0.1387520208954811 Adapter cache time: 0.07935116486623883 Engine time: 0.1334871551953256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 0.9997041099704802,
    "estimated_duration": 3599.5022968600465,
    "input_throughput": 468.4580980739856,
    "output_throughput": 410.2544958196526,
    "total_throughput": 878.7125938936382,
    "itl": 19.638818392980717,
    "ttft": 3215.4876411254904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.249630935749547,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9997824560850859. Arrivals time: 0.02961398847401142 Scheduler time: 0.5494145704433322 Scheduler overhead time: 0.1440902454778552 Adapter cache time: 0.06008522631600499 Engine time: 0.1436968371272087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.001657699700445,
    "estimated_duration": 3599.494157649014,
    "input_throughput": 468.4591573559716,
    "output_throughput": 410.25542349108986,
    "total_throughput": 878.7145808470615,
    "itl": 19.643016679628243,
    "ttft": 3215.587090998522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.202880262507596,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0017127767205238. Arrivals time: 0.02991324057802558 Scheduler time: 0.5520430151373148 Scheduler overhead time: 0.14438707334920764 Adapter cache time: 0.0606938898563385 Engine time: 0.14216784201562405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0032232929952443,
    "estimated_duration": 3599.500666320603,
    "input_throughput": 468.4583102810324,
    "output_throughput": 410.2546816610232,
    "total_throughput": 878.7129919420556,
    "itl": 19.643604705417236,
    "ttft": 3215.5567441402327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.225879461652243,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0032808571122587. Arrivals time: 0.02988586761057377 Scheduler time: 0.5525141349062324 Scheduler overhead time: 0.14471853571012616 Adapter cache time: 0.06027126591652632 Engine time: 0.1421519946306944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 0.9984520617872477,
    "estimated_duration": 3599.4961627268567,
    "input_throughput": 468.4588964035955,
    "output_throughput": 410.25519496075606,
    "total_throughput": 878.7140913643516,
    "itl": 19.640942709544184,
    "ttft": 3215.358262659922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.544094726116127,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9985106699168682. Arrivals time: 0.029807656537741423 Scheduler time: 0.5491323252208531 Scheduler overhead time: 0.14446311397477984 Adapter cache time: 0.059930103830993176 Engine time: 0.1426801891066134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.0011646943166852,
    "estimated_duration": 3599.5035870666316,
    "input_throughput": 468.45793015979734,
    "output_throughput": 410.254348767972,
    "total_throughput": 878.7122789277693,
    "itl": 19.646009785412375,
    "ttft": 3215.714200886861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.431095055191507,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0012243050150573. Arrivals time: 0.029679948464035988 Scheduler time: 0.5517219426110387 Scheduler overhead time: 0.14486774755641818 Adapter cache time: 0.060110696125775576 Engine time: 0.14200907852500677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0042605390772223,
    "estimated_duration": 3599.5074644247456,
    "input_throughput": 468.45742554099195,
    "output_throughput": 410.25390684555794,
    "total_throughput": 878.7113323865499,
    "itl": 19.6358614883181,
    "ttft": 3215.3715281362965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.927654273655635,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0043434239923954. Arrivals time: 0.029535826295614243 Scheduler time: 0.5550575647503138 Scheduler overhead time: 0.14437864301726222 Adapter cache time: 0.0606774496845901 Engine time: 0.1420467128045857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 0.9988766247406602,
    "estimated_duration": 3599.502738248736,
    "input_throughput": 468.45804062935474,
    "output_throughput": 410.25444551223313,
    "total_throughput": 878.7124861415879,
    "itl": 19.645465997418263,
    "ttft": 3215.628404699598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.61823127526682,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9989331769756973. Arrivals time: 0.02970271836966276 Scheduler time: 0.5467954007908702 Scheduler overhead time: 0.14497050968930125 Adapter cache time: 0.05997681571170688 Engine time: 0.14475781749933958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 44.860918930731714,
    "estimated_duration": 3600.011569827108,
    "input_throughput": 5313.550978648291,
    "output_throughput": 4706.372930021915,
    "total_throughput": 10019.923908670205,
    "itl": 182.71707867728787,
    "ttft": 2165950.079498037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.90056288898227,
    "arrivals": 1724092,
    "finished_requests": 77671,
    "scheduler_time": 122.71811196964813
}
#Debug simulation 
Total elapsed time: 44.86106230970472. Arrivals time: 0.3932900121435523 Scheduler time: 44.33892275951803 Scheduler overhead time: 0.04856202518567443 Adapter cache time: 0.01714875688776374 Engine time: 0.045697275549173355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 49.78354157600552,
    "estimated_duration": 3600.149920880305,
    "input_throughput": 5326.951494095184,
    "output_throughput": 4724.402142631073,
    "total_throughput": 10051.353636726257,
    "itl": 181.51148411805602,
    "ttft": 2167497.957603748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7059874750510842,
    "arrivals": 1724092,
    "finished_requests": 77875,
    "scheduler_time": 123.39833071633709
}
#Debug simulation 
Total elapsed time: 49.78369058808312. Arrivals time: 0.40009313402697444 Scheduler time: 49.25329075101763 Scheduler overhead time: 0.049985292833298445 Adapter cache time: 0.015342739410698414 Engine time: 0.0473507852293551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 46.351480084937066,
    "estimated_duration": 3600.054983567312,
    "input_throughput": 5305.773408236297,
    "output_throughput": 4705.878403893894,
    "total_throughput": 10011.65181213019,
    "itl": 179.85177964883,
    "ttft": 2169649.614695838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7128404998406872,
    "arrivals": 1724092,
    "finished_requests": 77629,
    "scheduler_time": 123.58197153023882
}
#Debug simulation 
Total elapsed time: 46.35163448192179. Arrivals time: 0.39970362186431885 Scheduler time: 45.82290426455438 Scheduler overhead time: 0.04858323885127902 Adapter cache time: 0.015357178635895252 Engine time: 0.04720574989914894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 50.06071647675708,
    "estimated_duration": 3600.080579870646,
    "input_throughput": 5327.054096297221,
    "output_throughput": 4724.493139153883,
    "total_throughput": 10051.547235451104,
    "itl": 181.50845110920127,
    "ttft": 2167468.4365375843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6369348662346532,
    "arrivals": 1724092,
    "finished_requests": 77875,
    "scheduler_time": 123.3980423154776
}
#Debug simulation 
Total elapsed time: 50.06085334671661. Arrivals time: 0.5154951820150018 Scheduler time: 49.413975360803306 Scheduler overhead time: 0.050738786812871695 Adapter cache time: 0.015524755232036114 Engine time: 0.0469156363978982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 46.91533771296963,
    "estimated_duration": 3600.0760712105416,
    "input_throughput": 5305.742329377273,
    "output_throughput": 4705.850838952792,
    "total_throughput": 10011.593168330066,
    "itl": 179.8526909148667,
    "ttft": 2169658.7764492095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7338413821719634,
    "arrivals": 1724092,
    "finished_requests": 77629,
    "scheduler_time": 123.58205829114733
}
#Debug simulation 
Total elapsed time: 46.915450715925545. Arrivals time: 0.8024539635516703 Scheduler time: 45.98484610021114 Scheduler overhead time: 0.04901275737211108 Adapter cache time: 0.015271537005901337 Engine time: 0.04598343279212713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 45.11514385929331,
    "estimated_duration": 3600.181729912843,
    "input_throughput": 5313.791756968651,
    "output_throughput": 4706.754344984198,
    "total_throughput": 10020.546101952848,
    "itl": 182.71737143042256,
    "ttft": 2166000.3487597317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.856821233134222,
    "arrivals": 1724092,
    "finished_requests": 77679,
    "scheduler_time": 122.72514915031972
}
#Debug simulation 
Total elapsed time: 45.115303670056164. Arrivals time: 0.5064411107450724 Scheduler time: 44.48101012920961 Scheduler overhead time: 0.0484638256020844 Adapter cache time: 0.016665572300553322 Engine time: 0.045447193551808596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 46.844449813012034,
    "estimated_duration": 3600.0991919396224,
    "input_throughput": 5305.708254585322,
    "output_throughput": 4705.82061681264,
    "total_throughput": 10011.52887139796,
    "itl": 179.85367471264402,
    "ttft": 2169668.629640093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7568543250858764,
    "arrivals": 1724092,
    "finished_requests": 77629,
    "scheduler_time": 123.58216607732716
}
#Debug simulation 
Total elapsed time: 46.84457229310647. Arrivals time: 0.8154664938338101 Scheduler time: 45.902443861123174 Scheduler overhead time: 0.0481620067730546 Adapter cache time: 0.015516136307269335 Engine time: 0.04547943687066436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 63.82352204201743,
    "estimated_duration": 3600.036654898329,
    "input_throughput": 5347.060556677482,
    "output_throughput": 4709.089274669837,
    "total_throughput": 10056.14983134732,
    "itl": 182.23442690104812,
    "ttft": 2161744.92388606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5455463106860583,
    "arrivals": 1601458,
    "finished_requests": 77937,
    "scheduler_time": 122.77346089779229
}
#Debug simulation 
Total elapsed time: 63.823660511057824. Arrivals time: 0.5811133934184909 Scheduler time: 63.11186444712803 Scheduler overhead time: 0.04960248153656721 Adapter cache time: 0.016067125368863344 Engine time: 0.046911106910556555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 66.74903628788888,
    "estimated_duration": 3600.007433411593,
    "input_throughput": 5347.228681068981,
    "output_throughput": 4709.14749860233,
    "total_throughput": 10056.376179671312,
    "itl": 182.18151401645278,
    "ttft": 2161196.6017105784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 509,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6612666036933712,
    "arrivals": 1601458,
    "finished_requests": 77948,
    "scheduler_time": 122.81014427835026
}
#Debug simulation 
Total elapsed time: 66.7491784337908. Arrivals time: 0.5883251284249127 Scheduler time: 66.02704772213474 Scheduler overhead time: 0.0512941675260663 Adapter cache time: 0.01644161995500326 Engine time: 0.0482185366563499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 72.75333258882165,
    "estimated_duration": 3600.122825309973,
    "input_throughput": 5325.601078165772,
    "output_throughput": 4694.146788879332,
    "total_throughput": 10019.747867045106,
    "itl": 180.26838321670135,
    "ttft": 2160140.342097253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3419347789697427,
    "arrivals": 1601458,
    "finished_requests": 77643,
    "scheduler_time": 123.25004778665524
}
#Debug simulation 
Total elapsed time: 72.753473023884. Arrivals time: 0.585508153308183 Scheduler time: 72.03268732503057 Scheduler overhead time: 0.05239972798153758 Adapter cache time: 0.01513268193230033 Engine time: 0.049435291439294815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 63.54703190596774,
    "estimated_duration": 3600.070288712012,
    "input_throughput": 5347.010601531029,
    "output_throughput": 4709.045279797911,
    "total_throughput": 10056.055881328939,
    "itl": 182.2359070698097,
    "ttft": 2161756.809917142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5790280077420116,
    "arrivals": 1601458,
    "finished_requests": 77937,
    "scheduler_time": 122.77361301436332
}
#Debug simulation 
Total elapsed time: 63.54716857895255. Arrivals time: 0.5866623492911458 Scheduler time: 62.827777810860425 Scheduler overhead time: 0.0503421132452786 Adapter cache time: 0.01617991179227829 Engine time: 0.04813388455659151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 73.0303738671355,
    "estimated_duration": 3600.1409132277718,
    "input_throughput": 5325.574321147964,
    "output_throughput": 4694.123204429919,
    "total_throughput": 10019.697525577882,
    "itl": 180.26910840873003,
    "ttft": 2160148.1406243206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3599175704270643,
    "arrivals": 1601458,
    "finished_requests": 77643,
    "scheduler_time": 123.25015291301156
}
#Debug simulation 
Total elapsed time: 73.03051640186459. Arrivals time: 0.5796216516755521 Scheduler time: 72.31652504950762 Scheduler overhead time: 0.05207553645595908 Adapter cache time: 0.015191648155450821 Engine time: 0.04883219255134463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 63.68452075403184,
    "estimated_duration": 3600.0009151511817,
    "input_throughput": 5347.113640717398,
    "output_throughput": 4709.136025119056,
    "total_throughput": 10056.249665836454,
    "itl": 182.23287050997115,
    "ttft": 2161732.056929422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.509975398925582,
    "arrivals": 1601458,
    "finished_requests": 77937,
    "scheduler_time": 122.77329206233085
}
#Debug simulation 
Total elapsed time: 63.684658005833626. Arrivals time: 0.5837368783541024 Scheduler time: 62.971436644438654 Scheduler overhead time: 0.04856135090813041 Adapter cache time: 0.01575086312368512 Engine time: 0.047618418000638485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 72.68886143993586,
    "estimated_duration": 3600.1581128895987,
    "input_throughput": 5325.54887835504,
    "output_throughput": 4694.100778378295,
    "total_throughput": 10019.649656733336,
    "itl": 180.26980451117325,
    "ttft": 2160155.418289668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.377020085379483,
    "arrivals": 1601458,
    "finished_requests": 77643,
    "scheduler_time": 123.2502500598975
}
#Debug simulation 
Total elapsed time: 72.6890039271675. Arrivals time: 0.5892998757772148 Scheduler time: 71.9654373624362 Scheduler overhead time: 0.05184932053089142 Adapter cache time: 0.015072308480739594 Engine time: 0.04921321664005518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 65.28011801512912,
    "estimated_duration": 3600.062299911707,
    "input_throughput": 5295.059199522051,
    "output_throughput": 4708.891010140508,
    "total_throughput": 10003.95020966256,
    "itl": 182.52651431079104,
    "ttft": 2161178.931662038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.423906293194789,
    "arrivals": 1509498,
    "finished_requests": 77566,
    "scheduler_time": 122.67385447667763
}
#Debug simulation 
Total elapsed time: 65.2803092901595. Arrivals time: 0.5036499821580946 Scheduler time: 64.64469136344269 Scheduler overhead time: 0.04883135622367263 Adapter cache time: 0.019496202003210783 Engine time: 0.04605944687500596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 65.05133961280808,
    "estimated_duration": 3600.092504851586,
    "input_throughput": 5296.983334261892,
    "output_throughput": 4707.810417971108,
    "total_throughput": 10004.793752233001,
    "itl": 182.48430168043754,
    "ttft": 2161552.1911862153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.541310149999341,
    "arrivals": 1509498,
    "finished_requests": 77575,
    "scheduler_time": 122.67726913444673
}
#Debug simulation 
Total elapsed time: 65.05146907875314. Arrivals time: 0.5589599199593067 Scheduler time: 64.36001991014928 Scheduler overhead time: 0.04875505529344082 Adapter cache time: 0.01957585383206606 Engine time: 0.04635933227837086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 45.28224449418485,
    "estimated_duration": 3600.05329901448,
    "input_throughput": 5290.275287094686,
    "output_throughput": 4698.318217852576,
    "total_throughput": 9988.593504947263,
    "itl": 181.1362068335473,
    "ttft": 2160413.4139295886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3856627505458854,
    "arrivals": 1509498,
    "finished_requests": 77463,
    "scheduler_time": 122.93840404935578
}
#Debug simulation 
Total elapsed time: 45.28238182608038. Arrivals time: 0.5631505744531751 Scheduler time: 44.587405000813305 Scheduler overhead time: 0.04872149042785168 Adapter cache time: 0.018636451568454504 Engine time: 0.04681389220058918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 65.09385958267376,
    "estimated_duration": 3600.1838309640293,
    "input_throughput": 5297.491988039139,
    "output_throughput": 4707.975980065821,
    "total_throughput": 10005.46796810496,
    "itl": 182.47825324687213,
    "ttft": 2161619.656713844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.43139800933886,
    "arrivals": 1509498,
    "finished_requests": 77582,
    "scheduler_time": 122.68385407326487
}
#Debug simulation 
Total elapsed time: 65.09398650564253. Arrivals time: 0.558899053838104 Scheduler time: 64.40290985768661 Scheduler overhead time: 0.04868476092815399 Adapter cache time: 0.019662309903651476 Engine time: 0.04636064311489463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 45.311000826302916,
    "estimated_duration": 3600.0848515801567,
    "input_throughput": 5290.228921032405,
    "output_throughput": 4698.277039935874,
    "total_throughput": 9988.50596096828,
    "itl": 181.1376372312358,
    "ttft": 2160426.371802549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4171011971496044,
    "arrivals": 1509498,
    "finished_requests": 77463,
    "scheduler_time": 122.93851816844862
}
#Debug simulation 
Total elapsed time: 45.31112179812044. Arrivals time: 0.562629263382405 Scheduler time: 44.61657116608694 Scheduler overhead time: 0.04843536904081702 Adapter cache time: 0.018493475392460823 Engine time: 0.04716564854606986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 65.31793271470815,
    "estimated_duration": 3600.0063465166327,
    "input_throughput": 5295.141498415669,
    "output_throughput": 4708.964198466775,
    "total_throughput": 10004.105696882445,
    "itl": 182.5239675509824,
    "ttft": 2161164.3418539544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.368119833562476,
    "arrivals": 1509498,
    "finished_requests": 77566,
    "scheduler_time": 122.67368754109012
}
#Debug simulation 
Total elapsed time: 65.31807471485808. Arrivals time: 0.5692836004309356 Scheduler time: 64.61644246382639 Scheduler overhead time: 0.04850013041868806 Adapter cache time: 0.019812388811260462 Engine time: 0.046265417244285345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 45.30740920826793,
    "estimated_duration": 3600.1157573914265,
    "input_throughput": 5290.183506154766,
    "output_throughput": 4698.2367067707,
    "total_throughput": 9988.420212925466,
    "itl": 181.13907052648716,
    "ttft": 2160438.634714502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.447910874821256,
    "arrivals": 1509498,
    "finished_requests": 77463,
    "scheduler_time": 122.93861430206844
}
#Debug simulation 
Total elapsed time: 45.307538364082575. Arrivals time: 0.654667146038264 Scheduler time: 44.52022581268102 Scheduler overhead time: 0.0494461921043694 Adapter cache time: 0.018858412746340036 Engine time: 0.04681427637115121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 64.573672960978,
    "estimated_duration": 3600.156267840315,
    "input_throughput": 5332.007160765678,
    "output_throughput": 4717.963814995541,
    "total_throughput": 10049.970975761218,
    "itl": 182.74569946883753,
    "ttft": 2159153.501390915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 581,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7781433792249557,
    "arrivals": 1494384,
    "finished_requests": 77382,
    "scheduler_time": 122.7931241659414
}
#Debug simulation 
Total elapsed time: 64.57382145198062. Arrivals time: 0.3998885019682348 Scheduler time: 64.03800849989057 Scheduler overhead time: 0.0520581747405231 Adapter cache time: 0.016503796447068453 Engine time: 0.04890602733939886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 64.31172086019069,
    "estimated_duration": 3600.1223315548777,
    "input_throughput": 5325.887354421897,
    "output_throughput": 4716.493062241701,
    "total_throughput": 10042.380416663598,
    "itl": 182.89709252326364,
    "ttft": 2158637.6141841626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9122089545754781,
    "arrivals": 1494384,
    "finished_requests": 77289,
    "scheduler_time": 122.76955693050215
}
#Debug simulation 
Total elapsed time: 64.3118743323721. Arrivals time: 0.777385008521378 Scheduler time: 63.39933873573318 Scheduler overhead time: 0.051897291559726 Adapter cache time: 0.0167157924734056 Engine time: 0.048487634398043156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 64.13917998271063,
    "estimated_duration": 3600.036055059168,
    "input_throughput": 5327.1910910583365,
    "output_throughput": 4702.4504035756845,
    "total_throughput": 10029.641494634021,
    "itl": 180.5513200123336,
    "ttft": 2161747.3757861014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9735133385658388,
    "arrivals": 1494384,
    "finished_requests": 77227,
    "scheduler_time": 123.22533787357759
}
#Debug simulation 
Total elapsed time: 64.13938741665334. Arrivals time: 0.39621339505538344 Scheduler time: 63.60880677681416 Scheduler overhead time: 0.05137719726189971 Adapter cache time: 0.016783776227384806 Engine time: 0.04794331220909953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 64.19387347716838,
    "estimated_duration": 3600.040274060342,
    "input_throughput": 5326.008750000617,
    "output_throughput": 4716.600567595592,
    "total_throughput": 10042.609317596209,
    "itl": 182.89351549273215,
    "ttft": 2158604.0740132118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8304898908873921,
    "arrivals": 1494384,
    "finished_requests": 77289,
    "scheduler_time": 122.76921849962972
}
#Debug simulation 
Total elapsed time: 64.19402061589062. Arrivals time: 0.4948337967507541 Scheduler time: 63.563923416193575 Scheduler overhead time: 0.05099915061146021 Adapter cache time: 0.016955077182501554 Engine time: 0.04883572272956371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 63.69452218012884,
    "estimated_duration": 3600.0613070163777,
    "input_throughput": 5327.153724472047,
    "output_throughput": 4702.4174191161865,
    "total_throughput": 10029.571143588233,
    "itl": 180.55241489101172,
    "ttft": 2161757.1752833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9986640958488024,
    "arrivals": 1494384,
    "finished_requests": 77227,
    "scheduler_time": 123.22543907352784
}
#Debug simulation 
Total elapsed time: 63.69466932816431. Arrivals time: 0.39370008185505867 Scheduler time: 63.165066722314805 Scheduler overhead time: 0.05208776146173477 Adapter cache time: 0.016657168045639992 Engine time: 0.04876626189798117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 64.35163710918278,
    "estimated_duration": 3600.154492955769,
    "input_throughput": 5326.269758011628,
    "output_throughput": 4716.901742196605,
    "total_throughput": 10043.171500208233,
    "itl": 182.89068925535764,
    "ttft": 2158601.8821691885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7491794225177475,
    "arrivals": 1494384,
    "finished_requests": 77293,
    "scheduler_time": 122.77574335270953
}
#Debug simulation 
Total elapsed time: 64.35178324207664. Arrivals time: 0.7799354684539139 Scheduler time: 63.43798463977873 Scheduler overhead time: 0.05110497027635574 Adapter cache time: 0.01635874854400754 Engine time: 0.04802663531154394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 64.05580381397158,
    "estimated_duration": 3600.086692088955,
    "input_throughput": 5327.116161436627,
    "output_throughput": 4702.384261245924,
    "total_throughput": 10029.500422682551,
    "itl": 180.55315384727663,
    "ttft": 2161766.0169666945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 603,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.025072390995915,
    "arrivals": 1494384,
    "finished_requests": 77227,
    "scheduler_time": 123.22566491890538
}
#Debug simulation 
Total elapsed time: 64.0559490933083. Arrivals time: 0.37728102086111903 Scheduler time: 63.54379565920681 Scheduler overhead time: 0.051713184453547 Adapter cache time: 0.01706182351335883 Engine time: 0.04827523557469249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 71.33321185503155,
    "estimated_duration": 3600.160551047243,
    "input_throughput": 5389.28548460321,
    "output_throughput": 4749.676231807477,
    "total_throughput": 10138.961716410686,
    "itl": 179.76395336886418,
    "ttft": 2147675.303950128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5363648474542597,
    "arrivals": 1486681,
    "finished_requests": 78525,
    "scheduler_time": 124.1669975991929
}
#Debug simulation 
Total elapsed time: 71.33334620622918. Arrivals time: 0.7806261060759425 Scheduler time: 70.4167529977858 Scheduler overhead time: 0.052570067811757326 Adapter cache time: 0.015652449801564217 Engine time: 0.04905439633876085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 70.1393017959781,
    "estimated_duration": 3600.0789808185577,
    "input_throughput": 5396.201612105437,
    "output_throughput": 4753.817649886316,
    "total_throughput": 10150.019261991753,
    "itl": 179.6877158692651,
    "ttft": 2148668.9372476335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6392403487395544,
    "arrivals": 1486681,
    "finished_requests": 78614,
    "scheduler_time": 124.25341937084261
}
#Debug simulation 
Total elapsed time: 70.13943971367553. Arrivals time: 0.5680744079872966 Scheduler time: 69.43523581465706 Scheduler overhead time: 0.05284306360408664 Adapter cache time: 0.015885192435234785 Engine time: 0.04903642274439335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 44.05419223709032,
    "estimated_duration": 3600.1755429375607,
    "input_throughput": 5415.877300274789,
    "output_throughput": 4797.2777421591245,
    "total_throughput": 10213.155042433913,
    "itl": 176.72146176749027,
    "ttft": 2163390.6213006275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0215351451560974,
    "arrivals": 1486681,
    "finished_requests": 79111,
    "scheduler_time": 125.63037027025842
}
#Debug simulation 
Total elapsed time: 44.05438501993194. Arrivals time: 0.38100751815363765 Scheduler time: 43.54558314662427 Scheduler overhead time: 0.04721662448719144 Adapter cache time: 0.017603637650609016 Engine time: 0.0453922301530838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 70.09090763935819,
    "estimated_duration": 3600.1835617954084,
    "input_throughput": 5373.785160652159,
    "output_throughput": 4743.255644299403,
    "total_throughput": 10117.040804951563,
    "itl": 180.89331794992717,
    "ttft": 2149370.5496328776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5874596802424579,
    "arrivals": 1486681,
    "finished_requests": 78374,
    "scheduler_time": 123.62932665695234
}
#Debug simulation 
Total elapsed time: 70.09104533819482. Arrivals time: 0.5090062087401748 Scheduler time: 69.44407139439136 Scheduler overhead time: 0.0535454866476357 Adapter cache time: 0.01619153143838048 Engine time: 0.049822053872048855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 44.26961059309542,
    "estimated_duration": 3600.002469498733,
    "input_throughput": 5415.909895949271,
    "output_throughput": 4797.232264789167,
    "total_throughput": 10213.142160738438,
    "itl": 176.7219614505519,
    "ttft": 2163353.473639762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0461828872934027,
    "arrivals": 1486681,
    "finished_requests": 79108,
    "scheduler_time": 125.62363805002327
}
#Debug simulation 
Total elapsed time: 44.2697413363494. Arrivals time: 0.5409685275517404 Scheduler time: 43.600990826729685 Scheduler overhead time: 0.04711213428527117 Adapter cache time: 0.01731247454881668 Engine time: 0.04566239798441529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 70.52515453565866,
    "estimated_duration": 3600.125017546114,
    "input_throughput": 5389.338677250942,
    "output_throughput": 4749.723111464412,
    "total_throughput": 10139.061788715355,
    "itl": 179.7625133363445,
    "ttft": 2147661.082039472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5010052480408758,
    "arrivals": 1486681,
    "finished_requests": 78525,
    "scheduler_time": 124.16682369740103
}
#Debug simulation 
Total elapsed time: 70.52529025357217. Arrivals time: 0.38641195511445403 Scheduler time: 70.00288545666263 Scheduler overhead time: 0.053104724269360304 Adapter cache time: 0.015548905357718468 Engine time: 0.04907817114144564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 44.0348217417486,
    "estimated_duration": 3600.0292351877283,
    "input_throughput": 5415.869629454075,
    "output_throughput": 4797.196598071358,
    "total_throughput": 10213.066227525433,
    "itl": 176.72308941059566,
    "ttft": 2163364.0921687307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.072842690013346,
    "arrivals": 1486681,
    "finished_requests": 79108,
    "scheduler_time": 125.62374393632918
}
#Debug simulation 
Total elapsed time: 44.034929257817566. Arrivals time: 0.36934926779940724 Scheduler time: 43.538141997065395 Scheduler overhead time: 0.04722527787089348 Adapter cache time: 0.017455744557082653 Engine time: 0.04513304587453604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 40.926291178911924,
    "estimated_duration": 3600.0704707145114,
    "input_throughput": 5441.294874461758,
    "output_throughput": 4830.78515864395,
    "total_throughput": 10272.080033105707,
    "itl": 178.45483517214896,
    "ttft": 2151433.138104966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4231268009287439,
    "arrivals": 1482883,
    "finished_requests": 79713,
    "scheduler_time": 125.72423089497353
}
#Debug simulation 
Total elapsed time: 40.92641143593937. Arrivals time: 0.5549257858656347 Scheduler time: 40.24579947302118 Scheduler overhead time: 0.04784244718030095 Adapter cache time: 0.014448211062699556 Engine time: 0.045655432622879744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 41.361324564088136,
    "estimated_duration": 3600.1673115562967,
    "input_throughput": 5441.148509159692,
    "output_throughput": 4830.65521543277,
    "total_throughput": 10271.803724592462,
    "itl": 178.45890168584017,
    "ttft": 2151477.582424942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5194895077566675,
    "arrivals": 1482883,
    "finished_requests": 79713,
    "scheduler_time": 125.72470902990746
}
#Debug simulation 
Total elapsed time: 41.36142896488309. Arrivals time: 0.9816327653825283 Scheduler time: 40.25237536849454 Scheduler overhead time: 0.04843625379726291 Adapter cache time: 0.01483930367976427 Engine time: 0.04640247533097863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 40.56745303189382,
    "estimated_duration": 3600.0383254576004,
    "input_throughput": 5410.233791753946,
    "output_throughput": 4814.192359409681,
    "total_throughput": 10224.426151163627,
    "itl": 177.15614057568095,
    "ttft": 2149806.577269798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5485153562016878,
    "arrivals": 1482883,
    "finished_requests": 79401,
    "scheduler_time": 125.87958778502801
}
#Debug simulation 
Total elapsed time: 40.56757054384798. Arrivals time: 0.5587662318721414 Scheduler time: 39.87965293601155 Scheduler overhead time: 0.04891002969816327 Adapter cache time: 0.01498602144420147 Engine time: 0.04730903636664152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 41.04832964902744,
    "estimated_duration": 3600.1016132001396,
    "input_throughput": 5441.247804832722,
    "output_throughput": 4830.743370196417,
    "total_throughput": 10271.99117502914,
    "itl": 178.45613123923252,
    "ttft": 2151447.568456076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4541142568062015,
    "arrivals": 1482883,
    "finished_requests": 79713,
    "scheduler_time": 125.72438592468454
}
#Debug simulation 
Total elapsed time: 41.04843406006694. Arrivals time: 0.5569485416635871 Scheduler time: 40.36337807169184 Scheduler overhead time: 0.049202804919332266 Adapter cache time: 0.014539188239723444 Engine time: 0.04687305819243193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 40.482941600028425,
    "estimated_duration": 3600.059422600191,
    "input_throughput": 5410.202086590127,
    "output_throughput": 4814.164147180175,
    "total_throughput": 10224.3662337703,
    "itl": 177.1570174418326,
    "ttft": 2149815.9950720784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5695162385329642,
    "arrivals": 1482883,
    "finished_requests": 79401,
    "scheduler_time": 125.87968404530608
}
#Debug simulation 
Total elapsed time: 40.48308778973296. Arrivals time: 0.3754432457499206 Scheduler time: 39.97941217198968 Scheduler overhead time: 0.04901376459747553 Adapter cache time: 0.014793126378208399 Engine time: 0.04621262988075614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 41.034394070040435,
    "estimated_duration": 3600.0375563181174,
    "input_throughput": 5441.344623091763,
    "output_throughput": 4830.8293255103,
    "total_throughput": 10272.173948602063,
    "itl": 178.45344746253468,
    "ttft": 2151418.002138331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 465,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3903733871294992,
    "arrivals": 1482883,
    "finished_requests": 79713,
    "scheduler_time": 125.72406991232425
}
#Debug simulation 
Total elapsed time: 41.0345179149881. Arrivals time: 0.5537439435720444 Scheduler time: 40.354940336197615 Scheduler overhead time: 0.047728228848427534 Adapter cache time: 0.014447666704654694 Engine time: 0.04624573700129986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 40.50743139907718,
    "estimated_duration": 3600.0791386043265,
    "input_throughput": 5410.172457361822,
    "output_throughput": 4814.1377821819115,
    "total_throughput": 10224.310239543733,
    "itl": 177.15783455939427,
    "ttft": 2149825.2709491844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5891338292136776,
    "arrivals": 1482883,
    "finished_requests": 79401,
    "scheduler_time": 125.8797824587746
}
#Debug simulation 
Total elapsed time: 40.50752953113988. Arrivals time: 0.5550949964672327 Scheduler time: 39.82402903446928 Scheduler overhead time: 0.04852733574807644 Adapter cache time: 0.014672412537038326 Engine time: 0.04709461610764265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 44.389147465117276,
    "estimated_duration": 3600.184850918503,
    "input_throughput": 5485.451113700897,
    "output_throughput": 4841.521122325997,
    "total_throughput": 10326.972236026893,
    "itl": 177.48367960305822,
    "ttft": 2148849.1719395034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1905297323898465,
    "arrivals": 1481051,
    "finished_requests": 79557,
    "scheduler_time": 126.11843559108362
}
#Debug simulation 
Total elapsed time: 44.38927231822163. Arrivals time: 0.3828545664437115 Scheduler time: 43.88090107217431 Scheduler overhead time: 0.04827538970857859 Adapter cache time: 0.012762404978275299 Engine time: 0.04639283008873463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 44.16021789656952,
    "estimated_duration": 3600.038157345464,
    "input_throughput": 5484.971585567875,
    "output_throughput": 4845.7266944256935,
    "total_throughput": 10330.698279993569,
    "itl": 177.4265836701666,
    "ttft": 2148242.64486942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2821417393279322,
    "arrivals": 1481051,
    "finished_requests": 79647,
    "scheduler_time": 126.16178612551819
}
#Debug simulation 
Total elapsed time: 44.16034257784486. Arrivals time: 0.3832430192269385 Scheduler time: 43.651399568188936 Scheduler overhead time: 0.04855460813269019 Adapter cache time: 0.012818536721169949 Engine time: 0.04646700248122215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 42.833018770907074,
    "estimated_duration": 3600.062249459951,
    "input_throughput": 5455.997324198076,
    "output_throughput": 4816.168943356734,
    "total_throughput": 10272.16626755481,
    "itl": 175.65710036600075,
    "ttft": 2149256.2728565587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2844617511332108,
    "arrivals": 1481051,
    "finished_requests": 79177,
    "scheduler_time": 126.28533133848205
}
#Debug simulation 
Total elapsed time: 42.833137014880776. Arrivals time: 0.3814580370672047 Scheduler time: 42.3255565520376 Scheduler overhead time: 0.04847560077905655 Adapter cache time: 0.013251199387013912 Engine time: 0.04641801072284579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 44.259852679912,
    "estimated_duration": 3600.1799499067583,
    "input_throughput": 5484.992771128407,
    "output_throughput": 4845.569455618964,
    "total_throughput": 10330.562226747372,
    "itl": 177.42395448455136,
    "ttft": 2148252.7958309483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.226572776020037,
    "arrivals": 1481051,
    "finished_requests": 79649,
    "scheduler_time": 126.16832406023782
}
#Debug simulation 
Total elapsed time: 44.259979576803744. Arrivals time: 0.4863797971047461 Scheduler time: 43.64758976129815 Scheduler overhead time: 0.048987587448209524 Adapter cache time: 0.012887701392173767 Engine time: 0.04626163421198726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 42.90244714869186,
    "estimated_duration": 3600.07906565416,
    "input_throughput": 5455.9718388937445,
    "output_throughput": 4816.146446730739,
    "total_throughput": 10272.118285624483,
    "itl": 175.65778006353187,
    "ttft": 2149264.3538434613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3011870047263845,
    "arrivals": 1481051,
    "finished_requests": 79177,
    "scheduler_time": 126.28542227911319
}
#Debug simulation 
Total elapsed time: 42.90254692686722. Arrivals time: 0.3771573663689196 Scheduler time: 42.39938345504925 Scheduler overhead time: 0.048412477131932974 Adapter cache time: 0.013039733748883009 Engine time: 0.04655427346006036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 44.312023917678744,
    "estimated_duration": 3600.1572677296726,
    "input_throughput": 5485.493141374312,
    "output_throughput": 4841.55821642534,
    "total_throughput": 10327.051357799652,
    "itl": 177.48257865660645,
    "ttft": 2148835.2201928776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.163129564716942,
    "arrivals": 1481051,
    "finished_requests": 79557,
    "scheduler_time": 126.11825256989701
}
#Debug simulation 
Total elapsed time: 44.31214514281601. Arrivals time: 0.3819811074063182 Scheduler time: 43.80386031186208 Scheduler overhead time: 0.04909811494871974 Adapter cache time: 0.0128958853892982 Engine time: 0.0463650650344789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 42.900181063916534,
    "estimated_duration": 3600.0953945388533,
    "input_throughput": 5455.947092345311,
    "output_throughput": 4816.124602226253,
    "total_throughput": 10272.071694571563,
    "itl": 175.65840529614314,
    "ttft": 2149272.843136402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3174092431738993,
    "arrivals": 1481051,
    "finished_requests": 79177,
    "scheduler_time": 126.28552892536865
}
#Debug simulation 
Total elapsed time: 42.90034403698519. Arrivals time: 0.3791198958642781 Scheduler time: 42.394803535658866 Scheduler overhead time: 0.048863987904042006 Adapter cache time: 0.01306479424238205 Engine time: 0.04650233406573534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 41.001939326990396,
    "estimated_duration": 3600.150179027824,
    "input_throughput": 5517.1326228850885,
    "output_throughput": 4869.748796072238,
    "total_throughput": 10386.881418957328,
    "itl": 176.38916627481046,
    "ttft": 2148824.0686881435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1293199775111893,
    "arrivals": 1480152,
    "finished_requests": 80326,
    "scheduler_time": 126.9124390757615
}
#Debug simulation 
Total elapsed time: 41.00203760387376. Arrivals time: 0.3829841329716146 Scheduler time: 40.492631572298706 Scheduler overhead time: 0.04881609370931983 Adapter cache time: 0.012758467346429825 Engine time: 0.04678713623434305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 41.07923048501834,
    "estimated_duration": 3600.068325214493,
    "input_throughput": 5507.31297546102,
    "output_throughput": 4860.701358760299,
    "total_throughput": 10368.014334221318,
    "itl": 176.70125618715738,
    "ttft": 2149168.203127526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.206424462131696,
    "arrivals": 1480152,
    "finished_requests": 80176,
    "scheduler_time": 126.68800842375458
}
#Debug simulation 
Total elapsed time: 41.079352545086294. Arrivals time: 0.38442107755690813 Scheduler time: 40.568610748276114 Scheduler overhead time: 0.04928034311160445 Adapter cache time: 0.012801175937056541 Engine time: 0.04653529915958643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 47.382859534118325,
    "estimated_duration": 3600.101334329444,
    "input_throughput": 5532.299274489649,
    "output_throughput": 4900.75482924878,
    "total_throughput": 10433.05410373843,
    "itl": 173.28091791096625,
    "ttft": 2164813.358074715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2405117886699806,
    "arrivals": 1480152,
    "finished_requests": 80711,
    "scheduler_time": 128.36098688030282
}
#Debug simulation 
Total elapsed time: 47.382982860319316. Arrivals time: 0.4924352872185409 Scheduler time: 46.76157214632258 Scheduler overhead time: 0.05014113150537014 Adapter cache time: 0.01322019100189209 Engine time: 0.047245734836906195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 41.12399912299588,
    "estimated_duration": 3600.173758860656,
    "input_throughput": 5517.096487666715,
    "output_throughput": 4869.716900983213,
    "total_throughput": 10386.813388649927,
    "itl": 176.39011418514025,
    "ttft": 2148834.7602088386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1527685923501858,
    "arrivals": 1480152,
    "finished_requests": 80326,
    "scheduler_time": 126.91257029371953
}
#Debug simulation 
Total elapsed time: 41.124144536908716. Arrivals time: 0.39826831920072436 Scheduler time: 40.598023909609765 Scheduler overhead time: 0.04943389818072319 Adapter cache time: 0.01271660067141056 Engine time: 0.04758373228833079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 47.013598232064396,
    "estimated_duration": 3600.117523907916,
    "input_throughput": 5532.274395970367,
    "output_throughput": 4900.7327907585495,
    "total_throughput": 10433.007186728915,
    "itl": 173.28151319473588,
    "ttft": 2164820.389689434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2566082733310808,
    "arrivals": 1480152,
    "finished_requests": 80711,
    "scheduler_time": 128.36107997412438
}
#Debug simulation 
Total elapsed time: 47.01373522821814. Arrivals time: 0.39006239641457796 Scheduler time: 46.494491156190634 Scheduler overhead time: 0.04961207788437605 Adapter cache time: 0.013127046637237072 Engine time: 0.04774290695786476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 41.21715439623222,
    "estimated_duration": 3600.124030426276,
    "input_throughput": 5517.17269519966,
    "output_throughput": 4869.784166276107,
    "total_throughput": 10386.956861475766,
    "itl": 176.3881553271781,
    "ttft": 2148811.9390896433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1033285588189006,
    "arrivals": 1480152,
    "finished_requests": 80326,
    "scheduler_time": 126.91228189286004
}
#Debug simulation 
Total elapsed time: 41.2172964271158. Arrivals time: 0.3838959392160177 Scheduler time: 40.70523796696216 Scheduler overhead time: 0.04927465971559286 Adapter cache time: 0.012896514032036066 Engine time: 0.047709100879728794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_96_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 47.241942416876554,
    "estimated_duration": 3600.132823457199,
    "input_throughput": 5532.250885364254,
    "output_throughput": 4900.711964026167,
    "total_throughput": 10432.962849390422,
    "itl": 173.28207396081692,
    "ttft": 2164827.090788449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2718244814872768,
    "arrivals": 1480152,
    "finished_requests": 80711,
    "scheduler_time": 128.3611633152599
}
#Debug simulation 
Total elapsed time: 47.242120910901576. Arrivals time: 0.5153892575763166 Scheduler time: 46.59727362776175 Scheduler overhead time: 0.049862146843224764 Adapter cache time: 0.01333739748224616 Engine time: 0.04778879787772894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 59.524912148248404,
    "estimated_duration": 3600.0984258810954,
    "input_throughput": 5319.6256697656545,
    "output_throughput": 4725.491080382446,
    "total_throughput": 10045.1167501481,
    "itl": 182.8141844159629,
    "ttft": 2155187.3715885337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1935902201337794,
    "arrivals": 1357053,
    "finished_requests": 77824,
    "scheduler_time": 122.85943258824702
}
#Debug simulation 
Total elapsed time: 59.5250581568107. Arrivals time: 0.5138243543915451 Scheduler time: 58.86982064321637 Scheduler overhead time: 0.055125641636550426 Adapter cache time: 0.015223568305373192 Engine time: 0.05245050275698304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 59.558112566825,
    "estimated_duration": 3600.074048294128,
    "input_throughput": 5318.572824654206,
    "output_throughput": 4724.951423724232,
    "total_throughput": 10043.524248378439,
    "itl": 182.84114628360786,
    "ttft": 2155065.374328066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2697729428298827,
    "arrivals": 1357053,
    "finished_requests": 77812,
    "scheduler_time": 122.83902875272479
}
#Debug simulation 
Total elapsed time: 59.558248728048056. Arrivals time: 0.49728525104001164 Scheduler time: 58.919787706341594 Scheduler overhead time: 0.0549101741053164 Adapter cache time: 0.015486360993236303 Engine time: 0.05151440715417266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 51.78200171189383,
    "estimated_duration": 3600.100759932805,
    "input_throughput": 5303.850995647242,
    "output_throughput": 4726.737148411817,
    "total_throughput": 10030.588144059058,
    "itl": 180.3174189782828,
    "ttft": 2156420.7847275883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4288312528096223,
    "arrivals": 1357053,
    "finished_requests": 77760,
    "scheduler_time": 123.48155819394646
}
#Debug simulation 
Total elapsed time: 51.78213857207447. Arrivals time: 0.4949424755759537 Scheduler time: 51.14946354320273 Scheduler overhead time: 0.05329361371695995 Adapter cache time: 0.015865326393395662 Engine time: 0.05048156809061766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 59.601763672195375,
    "estimated_duration": 3600.1021642244336,
    "input_throughput": 5317.543815905599,
    "output_throughput": 4724.343983628043,
    "total_throughput": 10041.887799533642,
    "itl": 182.85442248601174,
    "ttft": 2155038.558087177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2203329092985962,
    "arrivals": 1357053,
    "finished_requests": 77802,
    "scheduler_time": 122.83180074796209
}
#Debug simulation 
Total elapsed time: 59.60190696828067. Arrivals time: 0.5079941619187593 Scheduler time: 58.95299368258566 Scheduler overhead time: 0.05543835926800966 Adapter cache time: 0.015272464603185654 Engine time: 0.05131161957979202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 55.63311068713665,
    "estimated_duration": 3600.1183502817594,
    "input_throughput": 5303.825080779802,
    "output_throughput": 4726.714053349997,
    "total_throughput": 10030.5391341298,
    "itl": 180.31807657515728,
    "ttft": 2156428.534691817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4463110291212846,
    "arrivals": 1357053,
    "finished_requests": 77760,
    "scheduler_time": 123.48166876660801
}
#Debug simulation 
Total elapsed time: 55.633244779426605. Arrivals time: 0.48510868521407247 Scheduler time: 55.00994669878855 Scheduler overhead time: 0.05370618496090174 Adapter cache time: 0.015639312099665403 Engine time: 0.050215274561196566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 59.63160233898088,
    "estimated_duration": 3600.0707970798494,
    "input_throughput": 5319.666495318433,
    "output_throughput": 4725.527346239761,
    "total_throughput": 10045.193841558194,
    "itl": 182.81308564017561,
    "ttft": 2155175.76855092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.166119615011844,
    "arrivals": 1357053,
    "finished_requests": 77824,
    "scheduler_time": 122.85927439207947
}
#Debug simulation 
Total elapsed time: 59.63174808304757. Arrivals time: 0.40114451199769974 Scheduler time: 59.08789904881269 Scheduler overhead time: 0.056141798850148916 Adapter cache time: 0.01558788912370801 Engine time: 0.05201121559366584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 51.78513877466321,
    "estimated_duration": 3600.137817311477,
    "input_throughput": 5303.796401399815,
    "output_throughput": 4726.688494583191,
    "total_throughput": 10030.484895983007,
    "itl": 180.3188611861081,
    "ttft": 2156436.549265562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4656771122291692,
    "arrivals": 1357053,
    "finished_requests": 77760,
    "scheduler_time": 123.48176971324173
}
#Debug simulation 
Total elapsed time: 51.78531959792599. Arrivals time: 0.5073867524042726 Scheduler time: 51.13962891558185 Scheduler overhead time: 0.05362430075183511 Adapter cache time: 0.016049670055508614 Engine time: 0.05035272007808089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 47.250408844091,
    "estimated_duration": 3600.0722913116488,
    "input_throughput": 5369.034684844541,
    "output_throughput": 4723.3576506333475,
    "total_throughput": 10092.392335477887,
    "itl": 181.56767115372693,
    "ttft": 2142058.1923110317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.377219484769751,
    "arrivals": 1265213,
    "finished_requests": 78172,
    "scheduler_time": 123.11898896908212
}
#Debug simulation 
Total elapsed time: 47.25054490705952. Arrivals time: 0.3875830415636301 Scheduler time: 46.72189675690606 Scheduler overhead time: 0.054515096824616194 Adapter cache time: 0.016487785149365664 Engine time: 0.05152439698576927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 46.73051700834185,
    "estimated_duration": 3600.1624278106315,
    "input_throughput": 5368.900261468064,
    "output_throughput": 4723.239392935088,
    "total_throughput": 10092.139654403152,
    "itl": 181.57148112919472,
    "ttft": 2142091.5982396416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4668754422827677,
    "arrivals": 1265213,
    "finished_requests": 78172,
    "scheduler_time": 123.119469510523
}
#Debug simulation 
Total elapsed time: 46.73064747033641. Arrivals time: 0.38475494971498847 Scheduler time: 46.21139124175534 Scheduler overhead time: 0.0517345261760056 Adapter cache time: 0.01573054539039731 Engine time: 0.048995453398674726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 44.950344778131694,
    "estimated_duration": 3600.043599241356,
    "input_throughput": 5351.166025894695,
    "output_throughput": 4717.319257905312,
    "total_throughput": 10068.485283800006,
    "itl": 180.13406249175367,
    "ttft": 2135641.4422695646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4798882868513552,
    "arrivals": 1265213,
    "finished_requests": 77911,
    "scheduler_time": 123.4237431633654
}
#Debug simulation 
Total elapsed time: 44.95045726094395. Arrivals time: 0.38950392697006464 Scheduler time: 44.426634192466736 Scheduler overhead time: 0.05181135283783078 Adapter cache time: 0.015705124009400606 Engine time: 0.04910641489550471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 46.98899524798617,
    "estimated_duration": 3600.100776523572,
    "input_throughput": 5368.992203230742,
    "output_throughput": 4723.320277834078,
    "total_throughput": 10092.31248106482,
    "itl": 181.56895656681104,
    "ttft": 2142068.2306035846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4055861445167062,
    "arrivals": 1265213,
    "finished_requests": 78172,
    "scheduler_time": 123.11910752121409
}
#Debug simulation 
Total elapsed time: 46.989113678690046. Arrivals time: 0.3905005636624992 Scheduler time: 46.4626453765668 Scheduler overhead time: 0.05196434585377574 Adapter cache time: 0.016216886695474386 Engine time: 0.04898967407643795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 45.25597038771957,
    "estimated_duration": 3600.0620660670616,
    "input_throughput": 5351.138576631735,
    "output_throughput": 4717.29506001346,
    "total_throughput": 10068.433636645195,
    "itl": 180.13482710322896,
    "ttft": 2135648.6607127255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.498248339667921,
    "arrivals": 1265213,
    "finished_requests": 77911,
    "scheduler_time": 123.42384993627914
}
#Debug simulation 
Total elapsed time: 45.25608601607382. Arrivals time: 0.3888494400307536 Scheduler time: 44.73176652844995 Scheduler overhead time: 0.05251424899324775 Adapter cache time: 0.015633877366781235 Engine time: 0.04879234917461872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 47.110318393912166,
    "estimated_duration": 3600.0404222044103,
    "input_throughput": 5369.082213850349,
    "output_throughput": 4723.399463828156,
    "total_throughput": 10092.481677678505,
    "itl": 181.5663300145081,
    "ttft": 2142046.249745988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3455226327059682,
    "arrivals": 1265213,
    "finished_requests": 78172,
    "scheduler_time": 123.11881671384768
}
#Debug simulation 
Total elapsed time: 47.11043450888246. Arrivals time: 0.49187441309913993 Scheduler time: 46.48262103041634 Scheduler overhead time: 0.052830148953944445 Adapter cache time: 0.016027726233005524 Engine time: 0.04920444590970874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 45.12403731327504,
    "estimated_duration": 3600.081520177633,
    "input_throughput": 5351.10966016388,
    "output_throughput": 4717.269568707448,
    "total_throughput": 10068.379228871328,
    "itl": 180.1356724202282,
    "ttft": 2135655.4787299787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5176144227758048,
    "arrivals": 1265213,
    "finished_requests": 77911,
    "scheduler_time": 123.42393796377027
}
#Debug simulation 
Total elapsed time: 45.124210466165096. Arrivals time: 0.3975411169230938 Scheduler time: 44.59134710859507 Scheduler overhead time: 0.052810315042734146 Adapter cache time: 0.01570159662514925 Engine time: 0.04852005606517196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 27.984139189124107,
    "estimated_duration": 3600.0514191145776,
    "input_throughput": 5332.447169524447,
    "output_throughput": 4705.266127606074,
    "total_throughput": 10037.713297130522,
    "itl": 182.53419335650344,
    "ttft": 2142385.3111334015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7628409405052914,
    "arrivals": 1250156,
    "finished_requests": 77708,
    "scheduler_time": 122.57406564900025
}
#Debug simulation 
Total elapsed time: 27.984284989070147. Arrivals time: 0.3776609217748046 Scheduler time: 27.48217156017199 Scheduler overhead time: 0.04657287569716573 Adapter cache time: 0.016450978815555573 Engine time: 0.044369278475642204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 28.3025044281967,
    "estimated_duration": 3600.1189200904296,
    "input_throughput": 5337.8733943370125,
    "output_throughput": 4706.285646690363,
    "total_throughput": 10044.159041027375,
    "itl": 182.38467129648322,
    "ttft": 2142037.6118866378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8442360423086268,
    "arrivals": 1250156,
    "finished_requests": 77806,
    "scheduler_time": 122.61080368033025
}
#Debug simulation 
Total elapsed time: 28.30259687732905. Arrivals time: 0.3700608038343489 Scheduler time: 27.807283241301775 Scheduler overhead time: 0.047175949439406395 Adapter cache time: 0.016191557981073856 Engine time: 0.04494855087250471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 38.03429609304294,
    "estimated_duration": 3600.0803953351456,
    "input_throughput": 5350.769395305882,
    "output_throughput": 4714.229443873307,
    "total_throughput": 10064.99883917919,
    "itl": 179.57509953202594,
    "ttft": 2148610.6910570445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6769927686452968,
    "arrivals": 1250156,
    "finished_requests": 77919,
    "scheduler_time": 123.51434975738512
}
#Debug simulation 
Total elapsed time: 38.034422415774316. Arrivals time: 0.46966370241716504 Scheduler time: 37.42673703189939 Scheduler overhead time: 0.05344905145466328 Adapter cache time: 0.016978459898382425 Engine time: 0.04909517476335168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 28.180494414176792,
    "estimated_duration": 3600.094347765857,
    "input_throughput": 5332.383583756161,
    "output_throughput": 4705.210020540742,
    "total_throughput": 10037.593604296904,
    "itl": 182.53609563775916,
    "ttft": 2142404.4519710443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8056224148254751,
    "arrivals": 1250156,
    "finished_requests": 77708,
    "scheduler_time": 122.57421282589912
}
#Debug simulation 
Total elapsed time: 28.180626993067563. Arrivals time: 0.35752209834754467 Scheduler time: 27.6975574134849 Scheduler overhead time: 0.0466387839987874 Adapter cache time: 0.0166553882881999 Engine time: 0.04539094213396311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 38.4305534386076,
    "estimated_duration": 3600.102765847867,
    "input_throughput": 5350.736146406445,
    "output_throughput": 4714.200150340149,
    "total_throughput": 10064.936296746593,
    "itl": 179.57604612501865,
    "ttft": 2148617.853593925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6992511888407205,
    "arrivals": 1250156,
    "finished_requests": 77919,
    "scheduler_time": 123.51446184994579
}
#Debug simulation 
Total elapsed time: 38.43070228071883. Arrivals time: 0.7556442534551024 Scheduler time: 37.53859630553052 Scheduler overhead time: 0.052247737534344196 Adapter cache time: 0.016914813313633204 Engine time: 0.04911891184747219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 28.062116791959852,
    "estimated_duration": 3600.010684134839,
    "input_throughput": 5332.507507436323,
    "output_throughput": 4705.319368814834,
    "total_throughput": 10037.826876251158,
    "itl": 182.53237433542168,
    "ttft": 2142366.0875012297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7222689698636289,
    "arrivals": 1250156,
    "finished_requests": 77708,
    "scheduler_time": 122.5739026398188
}
#Debug simulation 
Total elapsed time: 28.062245345674455. Arrivals time: 0.38240959076210856 Scheduler time: 27.552419503685087 Scheduler overhead time: 0.05001127440482378 Adapter cache time: 0.01643183082342148 Engine time: 0.04401898942887783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 38.088799010030925,
    "estimated_duration": 3600.124872820483,
    "input_throughput": 5350.703289607961,
    "output_throughput": 4714.171202262704,
    "total_throughput": 10064.874491870665,
    "itl": 179.57699576714722,
    "ttft": 2148624.9686359493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.721258101463315,
    "arrivals": 1250156,
    "finished_requests": 77919,
    "scheduler_time": 123.5145619099717
}
#Debug simulation 
Total elapsed time: 38.088944791350514. Arrivals time: 0.48717370722442865 Scheduler time: 37.46494779130444 Scheduler overhead time: 0.0523112085647881 Adapter cache time: 0.01709305215626955 Engine time: 0.04920917470008135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 41.49281156482175,
    "estimated_duration": 3600.111763016778,
    "input_throughput": 5346.189581590026,
    "output_throughput": 4709.744617981456,
    "total_throughput": 10055.934199571482,
    "itl": 182.37660399993956,
    "ttft": 2143344.5218290924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9273277864116232,
    "arrivals": 1242527,
    "finished_requests": 77617,
    "scheduler_time": 122.64066233457551
}
#Debug simulation 
Total elapsed time: 41.492927097249776. Arrivals time: 0.3904857593588531 Scheduler time: 40.967331123072654 Scheduler overhead time: 0.05338216293603182 Adapter cache time: 0.01368636591359973 Engine time: 0.04955075215548277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 41.70821002218872,
    "estimated_duration": 3600.1742529362414,
    "input_throughput": 5346.096785260483,
    "output_throughput": 4709.662868726783,
    "total_throughput": 10055.759653987265,
    "itl": 182.37895963663198,
    "ttft": 2143372.5057648174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9893386843171956,
    "arrivals": 1242527,
    "finished_requests": 77617,
    "scheduler_time": 122.64114135611727
}
#Debug simulation 
Total elapsed time: 41.70832611899823. Arrivals time: 0.39158129692077637 Scheduler time: 41.17913122428581 Scheduler overhead time: 0.0540803805924952 Adapter cache time: 0.013977723196148872 Engine time: 0.050013700034469366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 41.28821539320052,
    "estimated_duration": 3600.0390939572103,
    "input_throughput": 5331.750155774321,
    "output_throughput": 4699.078137344547,
    "total_throughput": 10030.828293118868,
    "itl": 180.53070316896543,
    "ttft": 2145191.1085322383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.961569545455282,
    "arrivals": 1242527,
    "finished_requests": 77443,
    "scheduler_time": 123.01419008251432
}
#Debug simulation 
Total elapsed time: 41.288338692393154. Arrivals time: 0.39273454435169697 Scheduler time: 40.75741305574775 Scheduler overhead time: 0.0551274586468935 Adapter cache time: 0.013736606575548649 Engine time: 0.05044759204611182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 41.725924795959145,
    "estimated_duration": 3600.1334734481957,
    "input_throughput": 5346.157341651393,
    "output_throughput": 4709.716216093504,
    "total_throughput": 10055.873557744897,
    "itl": 182.37748037376645,
    "ttft": 2143353.7582713882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9488877477915991,
    "arrivals": 1242527,
    "finished_requests": 77617,
    "scheduler_time": 122.64081280458917
}
#Debug simulation 
Total elapsed time: 41.726060322951525. Arrivals time: 0.3961739377118647 Scheduler time: 41.19293507607654 Scheduler overhead time: 0.0546619133092463 Adapter cache time: 0.013561568222939968 Engine time: 0.0502899456769228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 41.43986240774393,
    "estimated_duration": 3600.051144173365,
    "input_throughput": 5331.732309155123,
    "output_throughput": 4699.062408427091,
    "total_throughput": 10030.794717582216,
    "itl": 180.53112417579624,
    "ttft": 2145196.790989767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9735161551646939,
    "arrivals": 1242527,
    "finished_requests": 77443,
    "scheduler_time": 123.01429368897152
}
#Debug simulation 
Total elapsed time: 41.439964895602316. Arrivals time: 0.39882821729406714 Scheduler time: 40.902774795889854 Scheduler overhead time: 0.05396829638630152 Adapter cache time: 0.013506436254829168 Engine time: 0.050702929962426424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 41.52175841713324,
    "estimated_duration": 3600.0902650600688,
    "input_throughput": 5346.221506387385,
    "output_throughput": 4709.772742244586,
    "total_throughput": 10055.99424863197,
    "itl": 182.37577906960107,
    "ttft": 2143335.008062676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059852393553605,
    "arrivals": 1242527,
    "finished_requests": 77617,
    "scheduler_time": 122.64050692488972
}
#Debug simulation 
Total elapsed time: 41.52188853593543. Arrivals time: 0.47860021982342005 Scheduler time: 40.90825129067525 Scheduler overhead time: 0.053285298869013786 Adapter cache time: 0.013698196038603783 Engine time: 0.05024764547124505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 41.25927981408313,
    "estimated_duration": 3600.064187120691,
    "input_throughput": 5331.712992415185,
    "output_throughput": 4699.045383835227,
    "total_throughput": 10030.758376250413,
    "itl": 180.53162810567258,
    "ttft": 2145202.4687826848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.986468795165425,
    "arrivals": 1242527,
    "finished_requests": 77443,
    "scheduler_time": 123.01438399631135
}
#Debug simulation 
Total elapsed time: 41.25943269673735. Arrivals time: 0.39568343246355653 Scheduler time: 40.72549775335938 Scheduler overhead time: 0.054760697297751904 Adapter cache time: 0.013570563402026892 Engine time: 0.050888385623693466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 36.12522532697767,
    "estimated_duration": 3600.1663214961372,
    "input_throughput": 5350.960561176026,
    "output_throughput": 4707.051143392066,
    "total_throughput": 10058.01170456809,
    "itl": 182.39808262026878,
    "ttft": 2148460.197140229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9854770535463454,
    "arrivals": 1238735,
    "finished_requests": 77568,
    "scheduler_time": 122.66327410537987
}
#Debug simulation 
Total elapsed time: 36.12535892799497. Arrivals time: 0.37687388015910983 Scheduler time: 35.6171704325825 Scheduler overhead time: 0.051408179104328156 Adapter cache time: 0.013600173871964216 Engine time: 0.04872782854363322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 33.82553820312023,
    "estimated_duration": 3600.0367837301815,
    "input_throughput": 5349.9603356951475,
    "output_throughput": 4708.413835271083,
    "total_throughput": 10058.37417096623,
    "itl": 182.39558049928922,
    "ttft": 2148729.119823519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1025709463260183,
    "arrivals": 1238735,
    "finished_requests": 77556,
    "scheduler_time": 122.655754320156
}
#Debug simulation 
Total elapsed time: 33.825675955973566. Arrivals time: 0.37509877560660243 Scheduler time: 33.317392486613244 Scheduler overhead time: 0.05156203592196107 Adapter cache time: 0.014342914335429668 Engine time: 0.04887763690203428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 30.860320639796555,
    "estimated_duration": 3600.1374364879634,
    "input_throughput": 5351.355146817438,
    "output_throughput": 4707.986930780793,
    "total_throughput": 10059.34207759823,
    "itl": 180.07206631357099,
    "ttft": 2152810.0568139674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5373530232906434,
    "arrivals": 1238735,
    "finished_requests": 77605,
    "scheduler_time": 123.33378581583231
}
#Debug simulation 
Total elapsed time: 30.860423252917826. Arrivals time: 0.36839576764032245 Scheduler time: 30.360682987142354 Scheduler overhead time: 0.050444369204342365 Adapter cache time: 0.015756402164697647 Engine time: 0.047539277002215385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 36.27105203317478,
    "estimated_duration": 3600.189138855022,
    "input_throughput": 5350.926647739038,
    "output_throughput": 4707.021310938523,
    "total_throughput": 10057.947958677561,
    "itl": 182.39898790350264,
    "ttft": 2148470.2619155743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.008150275305383,
    "arrivals": 1238735,
    "finished_requests": 77568,
    "scheduler_time": 122.66341824248049
}
#Debug simulation 
Total elapsed time: 36.271142174024135. Arrivals time: 0.3881919668056071 Scheduler time: 35.74824705440551 Scheduler overhead time: 0.05338262673467398 Adapter cache time: 0.013873836025595665 Engine time: 0.049464340321719646 
