INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:46:59 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 175.22635443881154,
    "estimated_duration": 3600.047657090772,
    "input_throughput": 6986.297237055114,
    "output_throughput": 6137.973745007799,
    "total_throughput": 13124.270982062913,
    "itl": 73.4313615527926,
    "ttft": 1113054.484770202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42820213897153775,
    "arrivals": 128637,
    "finished_requests": 100753,
    "scheduler_time": 205.29930292694252
}
#Debug simulation 
Total elapsed time: 175.22656414518133. Arrivals time: 0.4692311310209334 Scheduler time: 174.42855540523306 Scheduler overhead time: 0.12614662246778607 Adapter cache time: 0.022760430816560984 Engine time: 0.13257142389193177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 175.1378233199939,
    "estimated_duration": 3600.0000172664536,
    "input_throughput": 6984.308299834935,
    "output_throughput": 6137.92080389441,
    "total_throughput": 13122.229103729345,
    "itl": 73.43312284899581,
    "ttft": 1113003.683518195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3887065383372833,
    "arrivals": 128637,
    "finished_requests": 100741,
    "scheduler_time": 205.29790402499376
}
#Debug simulation 
Total elapsed time: 175.13801255822182. Arrivals time: 0.46412232937291265 Scheduler time: 174.34241544036195 Scheduler overhead time: 0.12870119279250503 Adapter cache time: 0.023024070542305708 Engine time: 0.13206715928390622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 176.0019131358713,
    "estimated_duration": 3600.0519227560876,
    "input_throughput": 6986.288959061783,
    "output_throughput": 6137.966472184442,
    "total_throughput": 13124.255431246225,
    "itl": 73.4315260140926,
    "ttft": 1113055.0033375975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4333580442145464,
    "arrivals": 128637,
    "finished_requests": 100753,
    "scheduler_time": 205.299313850904
}
#Debug simulation 
Total elapsed time: 176.00207516085356. Arrivals time: 0.4684346755966544 Scheduler time: 175.20350135629997 Scheduler overhead time: 0.1279208459891379 Adapter cache time: 0.022812128998339176 Engine time: 0.13157507730647922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 182.67586932424456,
    "estimated_duration": 3600.018781125208,
    "input_throughput": 6922.243331244798,
    "output_throughput": 6117.971138227232,
    "total_throughput": 13040.21446947203,
    "itl": 73.12394224114071,
    "ttft": 1113962.2626875404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.440710235126316,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.11402491846744
}
#Debug simulation 
Total elapsed time: 182.67603333108127. Arrivals time: 0.46710366010665894 Scheduler time: 181.87486801855266 Scheduler overhead time: 0.13051922619342804 Adapter cache time: 0.02264065109193325 Engine time: 0.13399481354281306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 182.76140758628026,
    "estimated_duration": 3600.020329314256,
    "input_throughput": 6922.240354333468,
    "output_throughput": 6117.968507193225,
    "total_throughput": 13040.208861526693,
    "itl": 73.12555791695907,
    "ttft": 1113960.9273697303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4706095836730673,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.1105057449058
}
#Debug simulation 
Total elapsed time: 182.76157483411953. Arrivals time: 0.47553971828892827 Scheduler time: 181.9510137969628 Scheduler overhead time: 0.1302716424688697 Adapter cache time: 0.022757850121706724 Engine time: 0.1344178980216384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 182.4581885128282,
    "estimated_duration": 3600.0208496769224,
    "input_throughput": 6922.239353762749,
    "output_throughput": 6117.967622875456,
    "total_throughput": 13040.206976638205,
    "itl": 73.12557074399993,
    "ttft": 1113960.9910728403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47131696868688017,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.110518799696
}
#Debug simulation 
Total elapsed time: 182.45836615702137. Arrivals time: 0.4696089648641646 Scheduler time: 181.65523651009426 Scheduler overhead time: 0.1298747742548585 Adapter cache time: 0.023219481110572815 Engine time: 0.13324957573786378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 182.58639985183254,
    "estimated_duration": 3600.000306796237,
    "input_throughput": 6922.278854519694,
    "output_throughput": 6118.002534172179,
    "total_throughput": 13040.281388691872,
    "itl": 73.12494194395082,
    "ttft": 1113957.823499199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4497712224326099,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.11019006289075
}
#Debug simulation 
Total elapsed time: 182.5865567889996. Arrivals time: 0.46929030725732446 Scheduler time: 181.7802613810636 Scheduler overhead time: 0.131050746422261 Adapter cache time: 0.02296259719878435 Engine time: 0.13467324990779161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 185.49840166792274,
    "estimated_duration": 3600.0270239141746,
    "input_throughput": 6922.227481755177,
    "output_throughput": 6117.957130236552,
    "total_throughput": 13040.184611991728,
    "itl": 73.12599464477123,
    "ttft": 1113961.9441477947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47773041179403714,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.11057970958458
}
#Debug simulation 
Total elapsed time: 185.49856962496415. Arrivals time: 0.47327985148876905 Scheduler time: 184.69108714349568 Scheduler overhead time: 0.13009391399100423 Adapter cache time: 0.022772924043238163 Engine time: 0.13431611144915223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 184.36151719419286,
    "estimated_duration": 3600.0089853255486,
    "input_throughput": 6922.262167005804,
    "output_throughput": 6117.987785524457,
    "total_throughput": 13040.249952530261,
    "itl": 73.12359960400124,
    "ttft": 1113960.6400342924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4305672424659138,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.11387110189628
}
#Debug simulation 
Total elapsed time: 184.36169564398006. Arrivals time: 0.47509898198768497 Scheduler time: 183.5527581665665 Scheduler overhead time: 0.12960636150091887 Adapter cache time: 0.023241885006427765 Engine time: 0.13278937246650457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 186.71416163211688,
    "estimated_duration": 3600.032336191934,
    "input_throughput": 6922.2172671816215,
    "output_throughput": 6117.948102459976,
    "total_throughput": 13040.165369641598,
    "itl": 73.12618247359678,
    "ttft": 1113962.845770801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48364083975553485,
    "arrivals": 127906,
    "finished_requests": 100613,
    "scheduler_time": 204.11058179087303
}
#Debug simulation 
Total elapsed time: 186.71445741318166. Arrivals time: 0.47402676939964294 Scheduler time: 185.90590122528374 Scheduler overhead time: 0.12867128988727927 Adapter cache time: 0.02366769313812256 Engine time: 0.13484333315864205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 189.9367776629515,
    "estimated_duration": 3600.063091512135,
    "input_throughput": 6937.036203304499,
    "output_throughput": 6108.77516337157,
    "total_throughput": 13045.81136667607,
    "itl": 72.72552400510698,
    "ttft": 1111619.900643763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.477436088053509,
    "arrivals": 127669,
    "finished_requests": 100436,
    "scheduler_time": 203.98853660942333
}
#Debug simulation 
Total elapsed time: 189.93694295873865. Arrivals time: 0.4738751593977213 Scheduler time: 189.12814566353336 Scheduler overhead time: 0.13120302045717835 Adapter cache time: 0.02349027106538415 Engine time: 0.13242099015042186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 185.50794582162052,
    "estimated_duration": 3600.0761562145117,
    "input_throughput": 6936.36548685057,
    "output_throughput": 6101.56148004863,
    "total_throughput": 13037.9269668992,
    "itl": 72.68659307134209,
    "ttft": 1127497.6203250831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5548884164355701,
    "arrivals": 127669,
    "finished_requests": 100374,
    "scheduler_time": 203.96685462930736
}
#Debug simulation 
Total elapsed time: 185.5081149386242. Arrivals time: 0.47119510965421796 Scheduler time: 184.70285977749154 Scheduler overhead time: 0.13005237467586994 Adapter cache time: 0.023105479776859283 Engine time: 0.13307844009250402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 186.1616653832607,
    "estimated_duration": 3600.0770334425874,
    "input_throughput": 6936.363796671584,
    "output_throughput": 6101.559993285712,
    "total_throughput": 13037.923789957296,
    "itl": 72.68661726503254,
    "ttft": 1127497.7359862777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5558444186672589,
    "arrivals": 127669,
    "finished_requests": 100374,
    "scheduler_time": 203.96687589371294
}
#Debug simulation 
Total elapsed time: 186.16183497011662. Arrivals time: 0.47673225961625576 Scheduler time: 185.35028782719746 Scheduler overhead time: 0.13126091891899705 Adapter cache time: 0.022913048043847084 Engine time: 0.13203990180045366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 189.83278472116217,
    "estimated_duration": 3600.117589552685,
    "input_throughput": 6941.069111885549,
    "output_throughput": 6107.446063374822,
    "total_throughput": 13048.515175260372,
    "itl": 72.71291675734786,
    "ttft": 1119520.4816415089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45452553706709325,
    "arrivals": 127669,
    "finished_requests": 100461,
    "scheduler_time": 203.98925022227212
}
#Debug simulation 
Total elapsed time: 189.83295576507226. Arrivals time: 0.4790830919519067 Scheduler time: 189.01709713088349 Scheduler overhead time: 0.12975992681458592 Adapter cache time: 0.023196612019091845 Engine time: 0.135410875082016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 187.02396525721997,
    "estimated_duration": 3600.0844457573526,
    "input_throughput": 6936.349515197758,
    "output_throughput": 6101.5474306128335,
    "total_throughput": 13037.896945810591,
    "itl": 72.68685091477239,
    "ttft": 1127498.9673541856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5636411534249794,
    "arrivals": 127669,
    "finished_requests": 100374,
    "scheduler_time": 203.9669916666494
}
#Debug simulation 
Total elapsed time: 187.0241373651661. Arrivals time: 0.48506162455305457 Scheduler time: 186.20225418312475 Scheduler overhead time: 0.12951041478663683 Adapter cache time: 0.02294803597033024 Engine time: 0.13530985871329904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 195.0681698997505,
    "estimated_duration": 3600.052158908039,
    "input_throughput": 6937.057269629948,
    "output_throughput": 6108.793714441783,
    "total_throughput": 13045.85098407173,
    "itl": 72.72520588924874,
    "ttft": 1111618.0879287424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46644784600474,
    "arrivals": 127669,
    "finished_requests": 100436,
    "scheduler_time": 203.9881920930125
}
#Debug simulation 
Total elapsed time: 195.06834583496675. Arrivals time: 0.4803779753856361 Scheduler time: 194.25201166700572 Scheduler overhead time: 0.1301701874472201 Adapter cache time: 0.023254733998328447 Engine time: 0.1347087062895298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 188.92637475999072,
    "estimated_duration": 3600.0918485302386,
    "input_throughput": 6936.3352521671795,
    "output_throughput": 6101.534884163525,
    "total_throughput": 13037.870136330705,
    "itl": 72.68699397097616,
    "ttft": 1127500.2573427162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5701803503185515,
    "arrivals": 127669,
    "finished_requests": 100374,
    "scheduler_time": 203.96725501117663
}
#Debug simulation 
Total elapsed time: 188.92654141318053. Arrivals time: 0.4787531807087362 Scheduler time: 188.11121276114136 Scheduler overhead time: 0.13045015232637525 Adapter cache time: 0.023247499018907547 Engine time: 0.13406352512538433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 189.80521313101053,
    "estimated_duration": 3600.1158128782063,
    "input_throughput": 6911.302383938545,
    "output_throughput": 6073.542112668618,
    "total_throughput": 12984.844496607164,
    "itl": 71.53056732966608,
    "ttft": 1134588.3770598501,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6151580365304827,
    "arrivals": 127143,
    "finished_requests": 100029,
    "scheduler_time": 203.90464394742887
}
#Debug simulation 
Total elapsed time: 189.80537327099591. Arrivals time: 0.4706572429277003 Scheduler time: 188.99744055606425 Scheduler overhead time: 0.130482685752213 Adapter cache time: 0.023360157385468483 Engine time: 0.13555111922323704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 189.81928566936404,
    "estimated_duration": 3600.063164950659,
    "input_throughput": 6913.555918217096,
    "output_throughput": 6073.702598576401,
    "total_throughput": 12987.258516793498,
    "itl": 71.52469723275078,
    "ttft": 1134205.0983848388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6519446410145628,
    "arrivals": 127143,
    "finished_requests": 100020,
    "scheduler_time": 203.904456595137
}
#Debug simulation 
Total elapsed time: 189.81944576837122. Arrivals time: 0.4764051171950996 Scheduler time: 189.00568145466968 Scheduler overhead time: 0.13073509372770786 Adapter cache time: 0.02374218637123704 Engine time: 0.13485514372587204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 190.29596811998636,
    "estimated_duration": 3600.065932109109,
    "input_throughput": 6913.550604174232,
    "output_throughput": 6073.697930079272,
    "total_throughput": 12987.248534253504,
    "itl": 71.52483639121628,
    "ttft": 1134205.7935131176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6532204390317229,
    "arrivals": 127143,
    "finished_requests": 100020,
    "scheduler_time": 203.90474749258226
}
#Debug simulation 
Total elapsed time: 190.29613537434489. Arrivals time: 0.47482175938785076 Scheduler time: 189.48066357476637 Scheduler overhead time: 0.1326217628084123 Adapter cache time: 0.02339390153065324 Engine time: 0.13599992683157325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 190.16474945098162,
    "estimated_duration": 3600.003920783098,
    "input_throughput": 6911.358583906133,
    "output_throughput": 6073.581996333991,
    "total_throughput": 12984.940580240123,
    "itl": 71.52957435812569,
    "ttft": 1134604.354098533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6291931862477226,
    "arrivals": 127143,
    "finished_requests": 100024,
    "scheduler_time": 203.897257820725
}
#Debug simulation 
Total elapsed time: 190.164917988237. Arrivals time: 0.47384577756747603 Scheduler time: 189.3531977152452 Scheduler overhead time: 0.1298303627409041 Adapter cache time: 0.022791964001953602 Engine time: 0.13794692372903228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 189.8468005368486,
    "estimated_duration": 3600.0756378733654,
    "input_throughput": 6913.531965317972,
    "output_throughput": 6073.681555456568,
    "total_throughput": 12987.213520774541,
    "itl": 71.52516712545092,
    "ttft": 1134207.1234730878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6612686813622735,
    "arrivals": 127143,
    "finished_requests": 100020,
    "scheduler_time": 203.90490443582513
}
#Debug simulation 
Total elapsed time: 189.84695992805064. Arrivals time: 0.47492309752851725 Scheduler time: 189.03255555778742 Scheduler overhead time: 0.1305964356288314 Adapter cache time: 0.023273262195289135 Engine time: 0.1376213445328176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 190.03663861891255,
    "estimated_duration": 3600.100977676239,
    "input_throughput": 6911.3308638526805,
    "output_throughput": 6073.5671403621345,
    "total_throughput": 12984.898004214816,
    "itl": 71.53004946083976,
    "ttft": 1134586.0384434056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6010001092753381,
    "arrivals": 127143,
    "finished_requests": 100029,
    "scheduler_time": 203.9043668270009
}
#Debug simulation 
Total elapsed time: 190.0369103238918. Arrivals time: 0.4718553349375725 Scheduler time: 189.22575445566326 Scheduler overhead time: 0.13170292787253857 Adapter cache time: 0.023596507962793112 Engine time: 0.13571847137063742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 190.43686049012467,
    "estimated_duration": 3600.084210076784,
    "input_throughput": 6913.515503424614,
    "output_throughput": 6073.6670933410305,
    "total_throughput": 12987.182596765644,
    "itl": 71.52549266025137,
    "ttft": 1134208.3550154935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6698199388384836,
    "arrivals": 127143,
    "finished_requests": 100020,
    "scheduler_time": 203.9051270922736
}
#Debug simulation 
Total elapsed time: 190.4370273728855. Arrivals time: 0.4751230329275131 Scheduler time: 189.624646615237 Scheduler overhead time: 0.1327830795198679 Adapter cache time: 0.022736245300620794 Engine time: 0.13361181318759918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 87.75220584217459,
    "estimated_duration": 3599.9014308533638,
    "input_throughput": 5991.62238586265,
    "output_throughput": 5290.799863785443,
    "total_throughput": 11282.422249648094,
    "itl": 68.83652975790744,
    "ttft": 740373.8230887825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6641258404334067,
    "arrivals": 101151,
    "finished_requests": 86427,
    "scheduler_time": 153.22191274742278
}
#Debug simulation 
Total elapsed time: 87.7523888121359. Arrivals time: 0.38342560175806284 Scheduler time: 87.03076121676713 Scheduler overhead time: 0.13347065215930343 Adapter cache time: 0.022398519329726696 Engine time: 0.13085134839639068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.22137119015679,
    "estimated_duration": 3599.942138221053,
    "input_throughput": 6187.923067843526,
    "output_throughput": 5460.039979896211,
    "total_throughput": 11647.963047739737,
    "itl": 71.12830052871162,
    "ttft": 666596.4629177657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6889211474428907,
    "arrivals": 101151,
    "finished_requests": 89290,
    "scheduler_time": 150.86254711309476
}
#Debug simulation 
Total elapsed time: 82.22154124593362. Arrivals time: 0.3888196125626564 Scheduler time: 81.50444847391918 Scheduler overhead time: 0.12876832205802202 Adapter cache time: 0.021758880466222763 Engine time: 0.1284746597521007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.91004661517218,
    "estimated_duration": 3599.945869745625,
    "input_throughput": 6187.9166537507,
    "output_throughput": 5460.034320290737,
    "total_throughput": 11647.950974041438,
    "itl": 71.12854598498396,
    "ttft": 666597.3891627761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6900716213136942,
    "arrivals": 101151,
    "finished_requests": 89290,
    "scheduler_time": 150.86292731501345
}
#Debug simulation 
Total elapsed time: 82.91021236311644. Arrivals time: 0.3880304768681526 Scheduler time: 82.19614077918231 Scheduler overhead time: 0.12843953631818295 Adapter cache time: 0.02187761152163148 Engine time: 0.12613391317427158 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 74.23229736229405,
    "estimated_duration": 3600.0116807678937,
    "input_throughput": 6359.28019964556,
    "output_throughput": 5575.719964252571,
    "total_throughput": 11935.000163898132,
    "itl": 73.89657910332168,
    "ttft": 587184.5183589736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8043993138126122,
    "arrivals": 101151,
    "finished_requests": 91638,
    "scheduler_time": 144.54875979146854
}
#Debug simulation 
Total elapsed time: 74.23246907629073. Arrivals time: 0.3769298102706671 Scheduler time: 73.53919972060248 Scheduler overhead time: 0.12486983090639114 Adapter cache time: 0.021428768057376146 Engine time: 0.12266342993825674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 76.9839977202937,
    "estimated_duration": 3600.0366136153466,
    "input_throughput": 6281.822222160412,
    "output_throughput": 5525.556302613045,
    "total_throughput": 11807.378524773456,
    "itl": 71.70858124610594,
    "ttft": 620265.7547760969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7819531028531524,
    "arrivals": 101151,
    "finished_requests": 90610,
    "scheduler_time": 147.67184745339785
}
#Debug simulation 
Total elapsed time: 76.98416674043983. Arrivals time: 0.38551245955750346 Scheduler time: 76.27933791652322 Scheduler overhead time: 0.1259038159623742 Adapter cache time: 0.021280073560774326 Engine time: 0.1240607388317585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 86.84172949381173,
    "estimated_duration": 3599.9901760614625,
    "input_throughput": 6238.2931346134255,
    "output_throughput": 5474.662439652034,
    "total_throughput": 11712.95557426546,
    "itl": 69.7023458569931,
    "ttft": 662351.5626915555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6428608134039686,
    "arrivals": 101151,
    "finished_requests": 89940,
    "scheduler_time": 153.3315966755049
}
#Debug simulation 
Total elapsed time: 86.84189884597436. Arrivals time: 0.39444205863401294 Scheduler time: 86.12017211457714 Scheduler overhead time: 0.1281427056528628 Adapter cache time: 0.022532841190695763 Engine time: 0.12749658897519112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 82.00786430295557,
    "estimated_duration": 3599.9036932058775,
    "input_throughput": 6210.208357016027,
    "output_throughput": 5487.518190356834,
    "total_throughput": 11697.72654737286,
    "itl": 71.55069701788965,
    "ttft": 661818.4421121565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7144786979630612,
    "arrivals": 101151,
    "finished_requests": 89574,
    "scheduler_time": 151.0056027544713
}
#Debug simulation 
Total elapsed time: 82.00803642394021. Arrivals time: 0.3859090395271778 Scheduler time: 81.29658195842057 Scheduler overhead time: 0.12909448193386197 Adapter cache time: 0.02180032292380929 Engine time: 0.12573268543928862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 78.30502199102193,
    "estimated_duration": 3600.028094281482,
    "input_throughput": 6112.4417431503125,
    "output_throughput": 5402.722559553234,
    "total_throughput": 11515.164302703546,
    "itl": 70.42566298463618,
    "ttft": 579184.769252239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.710033156592398,
    "arrivals": 97469,
    "finished_requests": 88606,
    "scheduler_time": 144.3566682430925
}
#Debug simulation 
Total elapsed time: 78.30519298883155. Arrivals time: 0.3736705887131393 Scheduler time: 77.59859962249175 Scheduler overhead time: 0.13047906011343002 Adapter cache time: 0.022673707455396652 Engine time: 0.12993234070017934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 88.3670172970742,
    "estimated_duration": 3600.089710934743,
    "input_throughput": 6008.5949898130475,
    "output_throughput": 5299.341553087708,
    "total_throughput": 11307.936542900756,
    "itl": 68.6074448248148,
    "ttft": 631358.675612396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7170573860523314,
    "arrivals": 97469,
    "finished_requests": 87098,
    "scheduler_time": 148.80956493584645
}
#Debug simulation 
Total elapsed time: 88.36718874936923. Arrivals time: 0.38216579519212246 Scheduler time: 87.64394097262993 Scheduler overhead time: 0.13452451350167394 Adapter cache time: 0.02274189703166485 Engine time: 0.13232229137793183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 88.19862060202286,
    "estimated_duration": 3600.1008393265183,
    "input_throughput": 6008.2844801776355,
    "output_throughput": 5299.292673026619,
    "total_throughput": 11307.577153204255,
    "itl": 68.60883374310269,
    "ttft": 631762.8060438447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7184750334359737,
    "arrivals": 97469,
    "finished_requests": 87089,
    "scheduler_time": 148.81017216942865
}
#Debug simulation 
Total elapsed time: 88.19879030110314. Arrivals time: 0.38604759564623237 Scheduler time: 87.46794675802812 Scheduler overhead time: 0.1363950208760798 Adapter cache time: 0.02272984478622675 Engine time: 0.1341274781152606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 77.01426830608398,
    "estimated_duration": 3600.0008383295685,
    "input_throughput": 6117.386631003863,
    "output_throughput": 5407.642907392516,
    "total_throughput": 11525.029538396378,
    "itl": 70.06899087925603,
    "ttft": 575738.1427841313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7545155325019752,
    "arrivals": 97469,
    "finished_requests": 88620,
    "scheduler_time": 143.33156863410383
}
#Debug simulation 
Total elapsed time: 77.01443919586018. Arrivals time: 0.3730123261921108 Scheduler time: 76.31503647798672 Scheduler overhead time: 0.12819602899253368 Adapter cache time: 0.02189280278980732 Engine time: 0.1271865675225854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 88.55488488264382,
    "estimated_duration": 3600.040073692959,
    "input_throughput": 6007.9295111326255,
    "output_throughput": 5300.006280326566,
    "total_throughput": 11307.935791459193,
    "itl": 68.50575432817791,
    "ttft": 631122.1804478483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7279065674170878,
    "arrivals": 97469,
    "finished_requests": 87094,
    "scheduler_time": 148.69268569608906
}
#Debug simulation 
Total elapsed time: 88.55516982171685. Arrivals time: 0.38286553137004375 Scheduler time: 87.83037355262786 Scheduler overhead time: 0.13589297980070114 Adapter cache time: 0.022802600171416998 Engine time: 0.13272111024707556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.08447434194386,
    "estimated_duration": 3599.9954516877515,
    "input_throughput": 6117.39578439616,
    "output_throughput": 5407.650998801465,
    "total_throughput": 11525.046783197624,
    "itl": 70.07473276825988,
    "ttft": 575667.8504730121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206021210714253,
    "arrivals": 97469,
    "finished_requests": 88620,
    "scheduler_time": 143.31546818309914
}
#Debug simulation 
Total elapsed time: 77.08464490901679. Arrivals time: 0.37112185917794704 Scheduler time: 76.3816275158897 Scheduler overhead time: 0.129246368072927 Adapter cache time: 0.02289797877892852 Engine time: 0.12915845774114132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 89.60641932114959,
    "estimated_duration": 3599.97611310197,
    "input_throughput": 5987.945842625487,
    "output_throughput": 5294.312629084975,
    "total_throughput": 11282.258471710462,
    "itl": 68.49731842812189,
    "ttft": 641706.5901386563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7201027343794725,
    "arrivals": 97469,
    "finished_requests": 86754,
    "scheduler_time": 149.38710866821413
}
#Debug simulation 
Total elapsed time: 89.6065892581828. Arrivals time: 0.38515955908223987 Scheduler time: 88.87924883654341 Scheduler overhead time: 0.1345612332224846 Adapter cache time: 0.022740691900253296 Engine time: 0.1328453067690134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 60.55560505017638,
    "estimated_duration": 3600.011838867385,
    "input_throughput": 6168.984990612444,
    "output_throughput": 5400.514184452433,
    "total_throughput": 11569.499175064877,
    "itl": 70.63556367519323,
    "ttft": 440600.5874993407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8599970560451027,
    "arrivals": 95579,
    "finished_requests": 89514,
    "scheduler_time": 129.24915553695624
}
#Debug simulation 
Total elapsed time: 60.55577835207805. Arrivals time: 0.3417889242991805 Scheduler time: 59.90055338246748 Scheduler overhead time: 0.12372473115101457 Adapter cache time: 0.020431154407560825 Engine time: 0.12126323720440269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 60.59788100793958,
    "estimated_duration": 3600.009726359356,
    "input_throughput": 6168.973610651612,
    "output_throughput": 5400.697630798352,
    "total_throughput": 11569.671241449965,
    "itl": 70.6409397416897,
    "ttft": 440519.0298254657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9170200527343014,
    "arrivals": 95579,
    "finished_requests": 89517,
    "scheduler_time": 129.24710294667545
}
#Debug simulation 
Total elapsed time: 60.59806166077033. Arrivals time: 0.3383468142710626 Scheduler time: 59.945822298526764 Scheduler overhead time: 0.12307580327615142 Adapter cache time: 0.02089741686359048 Engine time: 0.12157734576612711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 60.7791449772194,
    "estimated_duration": 3600.0109921558487,
    "input_throughput": 6168.97144158458,
    "output_throughput": 5400.695731864118,
    "total_throughput": 11569.667173448697,
    "itl": 70.640723927568,
    "ttft": 440519.77066638303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9186313254758762,
    "arrivals": 95579,
    "finished_requests": 89517,
    "scheduler_time": 129.24732416257487
}
#Debug simulation 
Total elapsed time: 60.7793165021576. Arrivals time: 0.3478765687905252 Scheduler time: 60.11722757015377 Scheduler overhead time: 0.12391874007880688 Adapter cache time: 0.02071052649989724 Engine time: 0.1211184561252594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 60.626859130803496,
    "estimated_duration": 3600.0079198064964,
    "input_throughput": 6168.991706327613,
    "output_throughput": 5400.520063590588,
    "total_throughput": 11569.511769918201,
    "itl": 70.63491604367813,
    "ttft": 440602.6266158659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8769777115271454,
    "arrivals": 95579,
    "finished_requests": 89514,
    "scheduler_time": 129.2489968862381
}
#Debug simulation 
Total elapsed time: 60.627035959623754. Arrivals time: 0.3451534570194781 Scheduler time: 59.96813590498641 Scheduler overhead time: 0.1233264347538352 Adapter cache time: 0.02105700969696045 Engine time: 0.12057717051357031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 61.49811095977202,
    "estimated_duration": 3600.008271868377,
    "input_throughput": 6168.976103067126,
    "output_throughput": 5400.6998128116675,
    "total_throughput": 11569.675915878794,
    "itl": 70.64107213010507,
    "ttft": 440518.614004803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.934335962887858,
    "arrivals": 95579,
    "finished_requests": 89517,
    "scheduler_time": 129.2465690240462
}
#Debug simulation 
Total elapsed time: 61.4982793959789. Arrivals time: 0.346153958234936 Scheduler time: 60.83564144000411 Scheduler overhead time: 0.12396386498585343 Adapter cache time: 0.020964426919817924 Engine time: 0.12223011115565896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 74.84842314803973,
    "estimated_duration": 3600.0559026363303,
    "input_throughput": 5968.882034377315,
    "output_throughput": 5247.517402761943,
    "total_throughput": 11216.399437139258,
    "itl": 65.39810171031279,
    "ttft": 544168.6294070478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6787414169427948,
    "arrivals": 95579,
    "finished_requests": 86604,
    "scheduler_time": 134.68513894623905
}
#Debug simulation 
Total elapsed time: 74.84860333241522. Arrivals time: 0.3625159412622452 Scheduler time: 74.15131204761565 Scheduler overhead time: 0.1317423554137349 Adapter cache time: 0.022304650396108627 Engine time: 0.12981568789109588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 60.84829154005274,
    "estimated_duration": 3600.016350548059,
    "input_throughput": 6168.9622594683615,
    "output_throughput": 5400.687693276755,
    "total_throughput": 11569.649952745116,
    "itl": 70.64023667879873,
    "ttft": 440521.69551257807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9422730373218703,
    "arrivals": 95579,
    "finished_requests": 89517,
    "scheduler_time": 129.2472699150111
}
#Debug simulation 
Total elapsed time: 60.84847086202353. Arrivals time: 0.34452946577221155 Scheduler time: 60.18788116984069 Scheduler overhead time: 0.12345460848882794 Adapter cache time: 0.021064516622573137 Engine time: 0.12280092434957623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 68.507174057886,
    "estimated_duration": 3600.0435188879933,
    "input_throughput": 5962.399867496665,
    "output_throughput": 5343.365684074247,
    "total_throughput": 11305.765551570912,
    "itl": 69.41482838357936,
    "ttft": 522560.81649858196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7528799850074565,
    "arrivals": 94646,
    "finished_requests": 86986,
    "scheduler_time": 135.67287683296124
}
#Debug simulation 
Total elapsed time: 68.5073462096043. Arrivals time: 0.3536009360104799 Scheduler time: 67.82904338790104 Scheduler overhead time: 0.12783436849713326 Adapter cache time: 0.021660191472619772 Engine time: 0.12588884821161628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 57.98678116314113,
    "estimated_duration": 3600.063233387727,
    "input_throughput": 6106.029415298475,
    "output_throughput": 5448.490687076627,
    "total_throughput": 11554.520102375101,
    "itl": 71.61810569128401,
    "ttft": 422786.1221996391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9084584971680355,
    "arrivals": 94646,
    "finished_requests": 89048,
    "scheduler_time": 127.69451596212572
}
#Debug simulation 
Total elapsed time: 57.98694874998182. Arrivals time: 0.336572437081486 Scheduler time: 57.34284702176228 Scheduler overhead time: 0.12080559693276882 Adapter cache time: 0.020492673851549625 Engine time: 0.11856573121622205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 58.371561402920634,
    "estimated_duration": 3600.039302235305,
    "input_throughput": 6106.07000494441,
    "output_throughput": 5448.526905753745,
    "total_throughput": 11554.596910698156,
    "itl": 71.67234987484598,
    "ttft": 422680.02823184483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9098380164243327,
    "arrivals": 94646,
    "finished_requests": 89048,
    "scheduler_time": 127.66970548333923
}
#Debug simulation 
Total elapsed time: 58.37173416884616. Arrivals time: 0.3437810451723635 Scheduler time: 57.71788694150746 Scheduler overhead time: 0.12239560391753912 Adapter cache time: 0.020873662550002337 Engine time: 0.11885500885546207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 70.32103919191286,
    "estimated_duration": 3600.022403021254,
    "input_throughput": 5961.323457873298,
    "output_throughput": 5347.735331825473,
    "total_throughput": 11309.05878969877,
    "itl": 68.87236313410611,
    "ttft": 521555.4903132004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7165919523709471,
    "arrivals": 94646,
    "finished_requests": 86991,
    "scheduler_time": 135.34062310713082
}
#Debug simulation 
Total elapsed time: 70.32133051706478. Arrivals time: 0.35637699626386166 Scheduler time: 69.63977142097428 Scheduler overhead time: 0.1275195386260748 Adapter cache time: 0.02197363320738077 Engine time: 0.12617001868784428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 57.36435723584145,
    "estimated_duration": 3600.042060491003,
    "input_throughput": 6116.1157647688615,
    "output_throughput": 5482.366502492513,
    "total_throughput": 11598.482267261374,
    "itl": 72.57934482565362,
    "ttft": 412657.54821341485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9312524301745043,
    "arrivals": 94646,
    "finished_requests": 89198,
    "scheduler_time": 127.09401699995207
}
#Debug simulation 
Total elapsed time: 57.3645249158144. Arrivals time: 0.336117216385901 Scheduler time: 56.722366601228714 Scheduler overhead time: 0.11955320741981268 Adapter cache time: 0.020099683199077845 Engine time: 0.11893340572714806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 68.62680357182398,
    "estimated_duration": 3600.0503165292293,
    "input_throughput": 6024.403575811497,
    "output_throughput": 5413.604890608691,
    "total_throughput": 11438.008466420188,
    "itl": 69.22036031646404,
    "ttft": 493472.62367494067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7534926743153493,
    "arrivals": 94646,
    "finished_requests": 87892,
    "scheduler_time": 135.73711195261424
}
#Debug simulation 
Total elapsed time: 68.6269721519202. Arrivals time: 0.35350755602121353 Scheduler time: 67.95289036817849 Scheduler overhead time: 0.12663366785272956 Adapter cache time: 0.021122411359101534 Engine time: 0.12422085739672184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 61.321326029952615,
    "estimated_duration": 3600.0264503890617,
    "input_throughput": 5979.321345728911,
    "output_throughput": 5369.598881116803,
    "total_throughput": 11348.920226845714,
    "itl": 67.90559528051602,
    "ttft": 473439.19632797653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9151468788459941,
    "arrivals": 94646,
    "finished_requests": 87188,
    "scheduler_time": 126.53003561545111
}
#Debug simulation 
Total elapsed time: 61.321501177269965. Arrivals time: 0.3464777744375169 Scheduler time: 60.654704364947975 Scheduler overhead time: 0.12614965485408902 Adapter cache time: 0.021881577093154192 Engine time: 0.1230231262743473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 69.96354771591723,
    "estimated_duration": 3600.0110476814107,
    "input_throughput": 6170.648007957419,
    "output_throughput": 5488.172046784362,
    "total_throughput": 11658.82005474178,
    "itl": 75.15231086620152,
    "ttft": 430503.98109554517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7283960830559945,
    "arrivals": 94180,
    "finished_requests": 89457,
    "scheduler_time": 138.23265981860067
}
#Debug simulation 
Total elapsed time: 69.96371739404276. Arrivals time: 0.3522323821671307 Scheduler time: 69.29462420195341 Scheduler overhead time: 0.1245260532014072 Adapter cache time: 0.021065021865069866 Engine time: 0.1230170913040638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 77.01711895735934,
    "estimated_duration": 3600.1130728773796,
    "input_throughput": 5977.01448938154,
    "output_throughput": 5287.931410661114,
    "total_throughput": 11264.945900042654,
    "itl": 71.12616833090014,
    "ttft": 538974.155953831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6847053111926673,
    "arrivals": 94180,
    "finished_requests": 86809,
    "scheduler_time": 141.34585368083665
}
#Debug simulation 
Total elapsed time: 77.01729697594419. Arrivals time: 0.36184991523623466 Scheduler time: 76.32550475001335 Scheduler overhead time: 0.13085755240172148 Adapter cache time: 0.021310774609446526 Engine time: 0.1280674091540277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 77.24184667505324,
    "estimated_duration": 3600.008481864065,
    "input_throughput": 5977.043417647284,
    "output_throughput": 5287.699764013719,
    "total_throughput": 11264.743181661002,
    "itl": 71.12495422304075,
    "ttft": 538985.5070401888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6860163599811524,
    "arrivals": 94180,
    "finished_requests": 86806,
    "scheduler_time": 141.34179766192133
}
#Debug simulation 
Total elapsed time: 77.24202925805002. Arrivals time: 0.3608503849245608 Scheduler time: 76.55080285063013 Scheduler overhead time: 0.13098320551216602 Adapter cache time: 0.021816683933138847 Engine time: 0.12718402035534382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 72.23080324195325,
    "estimated_duration": 3600.0216876811205,
    "input_throughput": 6010.733511424529,
    "output_throughput": 5343.829751312331,
    "total_throughput": 11354.56326273686,
    "itl": 72.10114576256177,
    "ttft": 513601.2289116114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7187648120289686,
    "arrivals": 94180,
    "finished_requests": 87142,
    "scheduler_time": 138.9199437018837
}
#Debug simulation 
Total elapsed time: 72.23097898392007. Arrivals time: 0.36072734696790576 Scheduler time: 71.54630514374003 Scheduler overhead time: 0.1273329770192504 Adapter cache time: 0.021705007646232843 Engine time: 0.1254692692309618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 69.2804545336403,
    "estimated_duration": 3600.0081257309052,
    "input_throughput": 6170.6530163705775,
    "output_throughput": 5488.176501265164,
    "total_throughput": 11658.829517635742,
    "itl": 75.15319871971744,
    "ttft": 430518.97309613304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.787702893055979,
    "arrivals": 94180,
    "finished_requests": 89457,
    "scheduler_time": 138.23553502532027
}
#Debug simulation 
Total elapsed time: 69.28062978386879. Arrivals time: 0.35151952831074595 Scheduler time: 68.61301853973418 Scheduler overhead time: 0.1237479280680418 Adapter cache time: 0.02112871501594782 Engine time: 0.1230867593549192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 72.46183102484792,
    "estimated_duration": 3600.0095696848252,
    "input_throughput": 6010.595966803052,
    "output_throughput": 5350.107722543143,
    "total_throughput": 11360.703689346194,
    "itl": 71.87844157066846,
    "ttft": 514156.20201992727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 222,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6637911654682839,
    "arrivals": 94180,
    "finished_requests": 87189,
    "scheduler_time": 139.36048269061948
}
#Debug simulation 
Total elapsed time: 72.46200729394332. Arrivals time: 0.3569603874348104 Scheduler time: 71.77783560240641 Scheduler overhead time: 0.12914562271907926 Adapter cache time: 0.02094187680631876 Engine time: 0.12759339762851596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 69.52732326090336,
    "estimated_duration": 3600.006028589495,
    "input_throughput": 6171.359109835917,
    "output_throughput": 5488.498308915764,
    "total_throughput": 11659.85741875168,
    "itl": 75.14957637899661,
    "ttft": 430204.73309873213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7975116883963379,
    "arrivals": 94180,
    "finished_requests": 89465,
    "scheduler_time": 138.2287150427505
}
#Debug simulation 
Total elapsed time: 69.5274929353036. Arrivals time: 0.3493106630630791 Scheduler time: 68.86069705989212 Scheduler overhead time: 0.12497108057141304 Adapter cache time: 0.020764834713190794 Engine time: 0.12312880484387279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 75.10523227415979,
    "estimated_duration": 3600.1205125249494,
    "input_throughput": 5901.180231630585,
    "output_throughput": 5219.817207402378,
    "total_throughput": 11120.997439032963,
    "itl": 65.97723361541479,
    "ttft": 550132.6963154449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6580048649455412,
    "arrivals": 93946,
    "finished_requests": 85633,
    "scheduler_time": 136.4453263483677
}
#Debug simulation 
Total elapsed time: 75.10539257200435. Arrivals time: 0.3603406324982643 Scheduler time: 74.41170319216326 Scheduler overhead time: 0.13103980757296085 Adapter cache time: 0.02223463961854577 Engine time: 0.12945882696658373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.46559302695096,
    "estimated_duration": 3600.1204024326976,
    "input_throughput": 5901.18041208961,
    "output_throughput": 5219.8173670252145,
    "total_throughput": 11120.997779114825,
    "itl": 65.97819589878497,
    "ttft": 550136.0680052072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7021071345778205,
    "arrivals": 93946,
    "finished_requests": 85633,
    "scheduler_time": 136.44453418788206
}
#Debug simulation 
Total elapsed time: 74.46576170390472. Arrivals time: 0.3529053032398224 Scheduler time: 73.78027250710875 Scheduler overhead time: 0.1318685538135469 Adapter cache time: 0.021689848974347115 Engine time: 0.1286008646711707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 74.95274352608249,
    "estimated_duration": 3600.1208175588476,
    "input_throughput": 5901.179731630696,
    "output_throughput": 5219.816765133557,
    "total_throughput": 11120.996496764252,
    "itl": 65.97848882388419,
    "ttft": 550134.5757511879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7032574391923876,
    "arrivals": 93946,
    "finished_requests": 85633,
    "scheduler_time": 136.4440991251457
}
#Debug simulation 
Total elapsed time: 74.9530389290303. Arrivals time: 0.3605829500593245 Scheduler time: 74.25807909108698 Scheduler overhead time: 0.13145130686461926 Adapter cache time: 0.021872141864150763 Engine time: 0.12913270853459835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 75.05668881954625,
    "estimated_duration": 3599.975238474454,
    "input_throughput": 5905.1447834426135,
    "output_throughput": 5251.584454789621,
    "total_throughput": 11156.729238232234,
    "itl": 65.98672897244899,
    "ttft": 544926.7680965416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6556950435834018,
    "arrivals": 93946,
    "finished_requests": 85731,
    "scheduler_time": 136.4240787842691
}
#Debug simulation 
Total elapsed time: 75.05685654468834. Arrivals time: 0.36052485881373286 Scheduler time: 74.36283748969436 Scheduler overhead time: 0.13120346888899803 Adapter cache time: 0.02170981653034687 Engine time: 0.12917694356292486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 74.87161910301074,
    "estimated_duration": 3600.1008003904335,
    "input_throughput": 5901.038103626442,
    "output_throughput": 5219.845232656263,
    "total_throughput": 11120.883336282704,
    "itl": 65.9784072386315,
    "ttft": 550168.1067503189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7126889731735019,
    "arrivals": 93946,
    "finished_requests": 85632,
    "scheduler_time": 136.44238847255718
}
#Debug simulation 
Total elapsed time: 74.87178331334144. Arrivals time: 0.3377640973776579 Scheduler time: 74.19753145473078 Scheduler overhead time: 0.13211081689223647 Adapter cache time: 0.022209797520190477 Engine time: 0.13106363452970982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 77.52882546465844,
    "estimated_duration": 3600.0711087187874,
    "input_throughput": 5901.676760924121,
    "output_throughput": 5230.67612592286,
    "total_throughput": 11132.35288684698,
    "itl": 66.5051148782789,
    "ttft": 557032.8904186339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6010001092753381,
    "arrivals": 93946,
    "finished_requests": 85756,
    "scheduler_time": 138.93310062649553
}
#Debug simulation 
Total elapsed time: 77.52899315860122. Arrivals time: 0.3380045657977462 Scheduler time: 76.85519993305206 Scheduler overhead time: 0.13347186846658587 Adapter cache time: 0.02119101071730256 Engine time: 0.13032741891220212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 75.02649517497048,
    "estimated_duration": 3600.1201176819905,
    "input_throughput": 5901.180878842174,
    "output_throughput": 5219.817779885519,
    "total_throughput": 11120.998658727693,
    "itl": 65.97869263916971,
    "ttft": 550134.0105224502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7214917382225418,
    "arrivals": 93946,
    "finished_requests": 85633,
    "scheduler_time": 136.44318536505673
}
#Debug simulation 
Total elapsed time: 75.0266535198316. Arrivals time: 0.33593875216320157 Scheduler time: 74.35629003914073 Scheduler overhead time: 0.13098380994051695 Adapter cache time: 0.02159534115344286 Engine time: 0.1304583135060966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.300328167155385,
    "estimated_duration": 3600.0227431065227,
    "input_throughput": 4944.55404041132,
    "output_throughput": 4395.053623007578,
    "total_throughput": 9339.607663418898,
    "itl": 47.91998001237474,
    "ttft": 285334.6769462128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8538760805572372,
    "arrivals": 74944,
    "finished_requests": 71586,
    "scheduler_time": 86.43331597694745
}
#Debug simulation 
Total elapsed time: 36.300447557121515. Arrivals time: 0.22743064863607287 Scheduler time: 35.731186324264854 Scheduler overhead time: 0.13610750576481223 Adapter cache time: 0.020977808628231287 Engine time: 0.12986805848777294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.205950737930834,
    "estimated_duration": 3600.0164185853114,
    "input_throughput": 4941.516629802124,
    "output_throughput": 4393.918849463477,
    "total_throughput": 9335.435479265601,
    "itl": 46.97579018452469,
    "ttft": 280718.95166533923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0428998234937954,
    "arrivals": 74944,
    "finished_requests": 71560,
    "scheduler_time": 84.55707790539748
}
#Debug simulation 
Total elapsed time: 34.206109162885696. Arrivals time: 0.22421667212620378 Scheduler time: 33.6437178212218 Scheduler overhead time: 0.1345667247660458 Adapter cache time: 0.021353446878492832 Engine time: 0.12786686979234219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 34.2096847910434,
    "estimated_duration": 3600.0198323282407,
    "input_throughput": 4941.511943975867,
    "output_throughput": 4393.914682900485,
    "total_throughput": 9335.426626876353,
    "itl": 46.97584403780279,
    "ttft": 280763.7868943586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0444024665653762,
    "arrivals": 74944,
    "finished_requests": 71560,
    "scheduler_time": 84.55729740056415
}
#Debug simulation 
Total elapsed time: 34.209847155958414. Arrivals time: 0.23067251686006784 Scheduler time: 33.63940283888951 Scheduler overhead time: 0.13500607945024967 Adapter cache time: 0.02132025733590126 Engine time: 0.12854626961052418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 34.26515989098698,
    "estimated_duration": 3600.011943690586,
    "input_throughput": 4941.52277221694,
    "output_throughput": 4393.924311202102,
    "total_throughput": 9335.447083419042,
    "itl": 46.97491906320882,
    "ttft": 280718.87671723374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9967285525100341,
    "arrivals": 74944,
    "finished_requests": 71560,
    "scheduler_time": 84.55737485833343
}
#Debug simulation 
Total elapsed time: 34.26533051999286. Arrivals time: 0.22987394267693162 Scheduler time: 33.69963919883594 Scheduler overhead time: 0.13322645146399736 Adapter cache time: 0.020981929264962673 Engine time: 0.12735820515081286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 40.75217954115942,
    "estimated_duration": 3600.0215264559342,
    "input_throughput": 4813.169274867694,
    "output_throughput": 4275.393323870559,
    "total_throughput": 9088.562598738254,
    "itl": 46.26308571397405,
    "ttft": 386421.3159053966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8499399788863996,
    "arrivals": 74944,
    "finished_requests": 69677,
    "scheduler_time": 91.76748394711981
}
#Debug simulation 
Total elapsed time: 40.75234706001356. Arrivals time: 0.2369532436132431 Scheduler time: 40.16695116460323 Scheduler overhead time: 0.13819226203486323 Adapter cache time: 0.02191389398649335 Engine time: 0.1325466986745596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 36.7956580510363,
    "estimated_duration": 3600.0257150726247,
    "input_throughput": 4944.549958482978,
    "output_throughput": 4395.049994713943,
    "total_throughput": 9339.599953196921,
    "itl": 47.92256090618669,
    "ttft": 285335.0695158593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342240322777081,
    "arrivals": 74944,
    "finished_requests": 71586,
    "scheduler_time": 86.43372615813868
}
#Debug simulation 
Total elapsed time: 36.795796315185726. Arrivals time: 0.22834594640880823 Scheduler time: 36.23226958978921 Scheduler overhead time: 0.1323715103790164 Adapter cache time: 0.021081997081637383 Engine time: 0.12747979443520308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 37.476146624889225,
    "estimated_duration": 3600.0168439580502,
    "input_throughput": 4835.189598963675,
    "output_throughput": 4299.423216859916,
    "total_throughput": 9134.61281582359,
    "itl": 46.895544494849254,
    "ttft": 360355.43463412015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9336054820567417,
    "arrivals": 74944,
    "finished_requests": 70100,
    "scheduler_time": 88.78099743564516
}
#Debug simulation 
Total elapsed time: 37.47625675983727. Arrivals time: 0.2313642269000411 Scheduler time: 36.90202349703759 Scheduler overhead time: 0.13597513362765312 Adapter cache time: 0.021334134973585606 Engine time: 0.13053983403369784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.745812771841884,
    "estimated_duration": 3600.016148096032,
    "input_throughput": 4895.702484368372,
    "output_throughput": 4304.4462476079525,
    "total_throughput": 9200.148731976324,
    "itl": 44.20406201366322,
    "ttft": 224141.9245842973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9701746148266817,
    "arrivals": 73090,
    "finished_requests": 70776,
    "scheduler_time": 78.77952072448356
}
#Debug simulation 
Total elapsed time: 30.74592703813687. Arrivals time: 0.21541297528892756 Scheduler time: 30.18854977376759 Scheduler overhead time: 0.13523754850029945 Adapter cache time: 0.021337981801480055 Engine time: 0.13017023028805852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 31.98570150276646,
    "estimated_duration": 3600.0315203929285,
    "input_throughput": 4823.781375698788,
    "output_throughput": 4247.392255701994,
    "total_throughput": 9071.173631400781,
    "itl": 45.2861717019396,
    "ttft": 275466.39619375695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9629667100473359,
    "arrivals": 73090,
    "finished_requests": 69715,
    "scheduler_time": 79.6828901830966
}
#Debug simulation 
Total elapsed time: 31.985938796773553. Arrivals time: 0.21590565098449588 Scheduler time: 31.425888039637357 Scheduler overhead time: 0.13691504718735814 Adapter cache time: 0.021467861253768206 Engine time: 0.1302604735828936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.695239015389234,
    "estimated_duration": 3600.0172139335104,
    "input_throughput": 4895.7010349244165,
    "output_throughput": 4304.444973214009,
    "total_throughput": 9200.146008138425,
    "itl": 44.20422259340498,
    "ttft": 224143.55453195886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.036966438889509,
    "arrivals": 73090,
    "finished_requests": 70776,
    "scheduler_time": 78.7793210450815
}
#Debug simulation 
Total elapsed time: 30.695400145370513. Arrivals time: 0.21936626778915524 Scheduler time: 30.135803976096213 Scheduler overhead time: 0.13485075859352946 Adapter cache time: 0.021467632614076138 Engine time: 0.12870016368106008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 32.109824772924185,
    "estimated_duration": 3600.008791667348,
    "input_throughput": 4823.803497423416,
    "output_throughput": 4247.4079606117,
    "total_throughput": 9071.211458035117,
    "itl": 45.2824598256422,
    "ttft": 275515.6302273361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9221071782032988,
    "arrivals": 73090,
    "finished_requests": 69714,
    "scheduler_time": 79.68272261197603
}
#Debug simulation 
Total elapsed time: 32.10993780102581. Arrivals time: 0.21993168909102678 Scheduler time: 31.54655118426308 Scheduler overhead time: 0.13609108608216047 Adapter cache time: 0.02139129862189293 Engine time: 0.13047371758148074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 30.8673695968464,
    "estimated_duration": 3600.038030937971,
    "input_throughput": 4895.961611663232,
    "output_throughput": 4304.852022899929,
    "total_throughput": 9200.813634563161,
    "itl": 44.215041555876596,
    "ttft": 223979.61154416873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0500448326766545,
    "arrivals": 73090,
    "finished_requests": 70779,
    "scheduler_time": 78.77580939934403
}
#Debug simulation 
Total elapsed time: 30.867473342921585. Arrivals time: 0.2175130508840084 Scheduler time: 30.30672343634069 Scheduler overhead time: 0.13651308370754123 Adapter cache time: 0.02138844784349203 Engine time: 0.13008794747292995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.713643772993237,
    "estimated_duration": 3600.0157498834424,
    "input_throughput": 4895.703025902215,
    "output_throughput": 4304.446723740505,
    "total_throughput": 9200.14974964272,
    "itl": 44.20307214166685,
    "ttft": 224142.57559405957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.947845943483991,
    "arrivals": 73090,
    "finished_requests": 70776,
    "scheduler_time": 78.77985153266506
}
#Debug simulation 
Total elapsed time: 30.71375814639032. Arrivals time: 0.21587550081312656 Scheduler time: 30.154798340518028 Scheduler overhead time: 0.13715915521606803 Adapter cache time: 0.021276677027344704 Engine time: 0.1296265870332718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.813409799244255,
    "estimated_duration": 3600.0244201286323,
    "input_throughput": 4895.69123516397,
    "output_throughput": 4304.436356975131,
    "total_throughput": 9200.127592139102,
    "itl": 44.20521754629058,
    "ttft": 224142.15295831297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0638777491822886,
    "arrivals": 73090,
    "finished_requests": 70776,
    "scheduler_time": 78.77887365758762
}
#Debug simulation 
Total elapsed time: 30.81356433033943. Arrivals time: 0.21842048270627856 Scheduler time: 30.2543861893937 Scheduler overhead time: 0.13481474854052067 Adapter cache time: 0.02136973151937127 Engine time: 0.12938728090375662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.17635667975992,
    "estimated_duration": 3599.9900962501356,
    "input_throughput": 4862.263098510417,
    "output_throughput": 4242.793894324849,
    "total_throughput": 9105.056992835267,
    "itl": 44.14490225092283,
    "ttft": 216424.41412300576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9793560780584799,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.78736956452455
}
#Debug simulation 
Total elapsed time: 30.176463193725795. Arrivals time: 0.21383468294516206 Scheduler time: 29.617894077673554 Scheduler overhead time: 0.1376244118437171 Adapter cache time: 0.021620776504278183 Engine time: 0.13008502265438437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.067857509944588,
    "estimated_duration": 3599.9901660986025,
    "input_throughput": 4862.26300417082,
    "output_throughput": 4242.793812004444,
    "total_throughput": 9105.056816175264,
    "itl": 44.14632915202792,
    "ttft": 216425.56444969747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0442554925149365,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.7872619138538
}
#Debug simulation 
Total elapsed time: 30.067995187826455. Arrivals time: 0.2146697356365621 Scheduler time: 29.511348481290042 Scheduler overhead time: 0.13533060904592276 Adapter cache time: 0.021626936737447977 Engine time: 0.12958211731165648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.146288578864187,
    "estimated_duration": 3599.9592183816203,
    "input_throughput": 4862.304803516373,
    "output_throughput": 4242.830285968214,
    "total_throughput": 9105.135089484587,
    "itl": 44.14606045015832,
    "ttft": 216425.79285548907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0460969954356607,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.78677647256424
}
#Debug simulation 
Total elapsed time: 30.14638120913878. Arrivals time: 0.21206923900172114 Scheduler time: 29.58753771102056 Scheduler overhead time: 0.13831617031246424 Adapter cache time: 0.021275676786899567 Engine time: 0.131368612870574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 30.200945036951452,
    "estimated_duration": 3599.9886406250907,
    "input_throughput": 4862.2650645254935,
    "output_throughput": 4242.7956098627765,
    "total_throughput": 9105.06067438827,
    "itl": 44.1448059076834,
    "ttft": 216425.29954148992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9989014121680555,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.78748691405525
}
#Debug simulation 
Total elapsed time: 30.201090813148767. Arrivals time: 0.21339345024898648 Scheduler time: 29.643589828163385 Scheduler overhead time: 0.136374328751117 Adapter cache time: 0.021746040787547827 Engine time: 0.13042461685836315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 30.16414707619697,
    "estimated_duration": 3599.983525461212,
    "input_throughput": 4862.271973246728,
    "output_throughput": 4242.8016383889335,
    "total_throughput": 9105.073611635662,
    "itl": 44.144986061952274,
    "ttft": 216426.17332797544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0600556657277098,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.78713245697264
}
#Debug simulation 
Total elapsed time: 30.164305058307946. Arrivals time: 0.21556260716170073 Scheduler time: 29.605677993502468 Scheduler overhead time: 0.1360319247469306 Adapter cache time: 0.021484078373759985 Engine time: 0.13010515412315726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.52260963106528,
    "estimated_duration": 3599.958793485063,
    "input_throughput": 4823.603823306387,
    "output_throughput": 4215.819644231984,
    "total_throughput": 9039.423467538372,
    "itl": 43.59528991307071,
    "ttft": 238258.8904036492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0016668487922302,
    "arrivals": 72131,
    "finished_requests": 69422,
    "scheduler_time": 75.63050931438909
}
#Debug simulation 
Total elapsed time: 29.522716287057847. Arrivals time: 0.2138590356335044 Scheduler time: 28.96389562031254 Scheduler overhead time: 0.13726523611694574 Adapter cache time: 0.02134315436705947 Engine time: 0.1305915592238307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.117895330302417,
    "estimated_duration": 3599.9663767979255,
    "input_throughput": 4862.295134981075,
    "output_throughput": 4242.82184923789,
    "total_throughput": 9105.116984218965,
    "itl": 44.146785042681614,
    "ttft": 216424.758084172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0730083057284403,
    "arrivals": 72131,
    "finished_requests": 69944,
    "scheduler_time": 76.78643381348881
}
#Debug simulation 
Total elapsed time: 30.117997913155705. Arrivals time: 0.21426552906632423 Scheduler time: 29.561314878985286 Scheduler overhead time: 0.13598243333399296 Adapter cache time: 0.02141353627666831 Engine time: 0.12981913099065423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.190889498218894,
    "estimated_duration": 3600.0036994228653,
    "input_throughput": 4840.514470247247,
    "output_throughput": 4260.365899751382,
    "total_throughput": 9100.88036999863,
    "itl": 45.22556212082321,
    "ttft": 213056.2229063488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8661180315329682,
    "arrivals": 71695,
    "finished_requests": 69630,
    "scheduler_time": 77.2911502155107
}
#Debug simulation 
Total elapsed time: 30.19110804097727. Arrivals time: 0.21082227677106857 Scheduler time: 29.638253097888082 Scheduler overhead time: 0.1355416146107018 Adapter cache time: 0.021427097730338573 Engine time: 0.12961149588227272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.023528923280537,
    "estimated_duration": 3600.025461439576,
    "input_throughput": 4842.284085687873,
    "output_throughput": 4262.440686701114,
    "total_throughput": 9104.724772388987,
    "itl": 44.18945165542653,
    "ttft": 205325.38419867674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9671825462975593,
    "arrivals": 71695,
    "finished_requests": 69676,
    "scheduler_time": 75.64315516819569
}
#Debug simulation 
Total elapsed time: 29.023692372255027. Arrivals time: 0.21526019228622317 Scheduler time: 28.46637185057625 Scheduler overhead time: 0.13563255639746785 Adapter cache time: 0.021193185355514288 Engine time: 0.12951397337019444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 29.24909054208547,
    "estimated_duration": 3600.0330870272037,
    "input_throughput": 4842.273828765028,
    "output_throughput": 4262.4316580021605,
    "total_throughput": 9104.705486767189,
    "itl": 44.18928223169384,
    "ttft": 205324.32868235305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9686683256365409,
    "arrivals": 71695,
    "finished_requests": 69676,
    "scheduler_time": 75.6430925844517
}
#Debug simulation 
Total elapsed time: 29.249221781268716. Arrivals time: 0.21647438686341047 Scheduler time: 28.687343867961317 Scheduler overhead time: 0.13746506813913584 Adapter cache time: 0.021363418083637953 Engine time: 0.1292441776022315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 29.150238261092454,
    "estimated_duration": 3600.0371866277706,
    "input_throughput": 4842.268314547395,
    "output_throughput": 4262.426804089177,
    "total_throughput": 9104.695118636571,
    "itl": 44.190562784411554,
    "ttft": 205322.89172510532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9250972284982011,
    "arrivals": 71695,
    "finished_requests": 69676,
    "scheduler_time": 75.64316228356945
}
#Debug simulation 
Total elapsed time: 29.15033175284043. Arrivals time: 0.21467948611825705 Scheduler time: 28.589202689472586 Scheduler overhead time: 0.13588322466239333 Adapter cache time: 0.02111033070832491 Engine time: 0.13417887454852462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 29.131836571265012,
    "estimated_duration": 3600.012391661448,
    "input_throughput": 4842.301665510315,
    "output_throughput": 4262.456161412864,
    "total_throughput": 9104.757826923178,
    "itl": 44.18825995376661,
    "ttft": 205316.60369226002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9816209656372719,
    "arrivals": 71695,
    "finished_requests": 69676,
    "scheduler_time": 75.64034781724257
}
#Debug simulation 
Total elapsed time: 29.13198172720149. Arrivals time: 0.20946533419191837 Scheduler time: 28.578790934756398 Scheduler overhead time: 0.13668398698791862 Adapter cache time: 0.021560517605394125 Engine time: 0.12922360328957438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 30.242979721166193,
    "estimated_duration": 3600.004510075679,
    "input_throughput": 4840.513380255092,
    "output_throughput": 4260.364940397694,
    "total_throughput": 9100.878320652786,
    "itl": 45.224605257342716,
    "ttft": 213055.33412112456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8461842334573169,
    "arrivals": 71695,
    "finished_requests": 69630,
    "scheduler_time": 77.29100311065928
}
#Debug simulation 
Total elapsed time: 30.243120905011892. Arrivals time: 0.2169581949710846 Scheduler time: 29.682184600736946 Scheduler overhead time: 0.13720576697960496 Adapter cache time: 0.021240484435111284 Engine time: 0.12999159703031182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 30.333861507009715,
    "estimated_duration": 3600.0300472433446,
    "input_throughput": 4840.4790435967425,
    "output_throughput": 4260.334719079435,
    "total_throughput": 9100.813762676178,
    "itl": 45.22424065222015,
    "ttft": 213104.41720565874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9497490788623741,
    "arrivals": 71695,
    "finished_requests": 69630,
    "scheduler_time": 77.29101502964185
}
#Debug simulation 
Total elapsed time: 30.333961666096002. Arrivals time: 0.21457009203732014 Scheduler time: 29.77710984321311 Scheduler overhead time: 0.13591856369748712 Adapter cache time: 0.02134976861998439 Engine time: 0.12958589242771268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 29.91948218503967,
    "estimated_duration": 3599.954385508266,
    "input_throughput": 4721.638715319486,
    "output_throughput": 4236.84229483551,
    "total_throughput": 8958.481010154996,
    "itl": 43.49554528839279,
    "ttft": 238619.55114028146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8569365683011699,
    "arrivals": 71480,
    "finished_requests": 68760,
    "scheduler_time": 75.61986255022963
}
#Debug simulation 
Total elapsed time: 29.919625092763454. Arrivals time: 0.21090484922751784 Scheduler time: 29.366664139088243 Scheduler overhead time: 0.13614694168791175 Adapter cache time: 0.020954621955752373 Engine time: 0.12946165539324284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.83307088119909,
    "estimated_duration": 3599.97561975916,
    "input_throughput": 4704.379359417163,
    "output_throughput": 4221.445811073769,
    "total_throughput": 8925.82517049093,
    "itl": 42.95081841901007,
    "ttft": 243774.43584875282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9557608235022105,
    "arrivals": 71480,
    "finished_requests": 68469,
    "scheduler_time": 73.22538665284226
}
#Debug simulation 
Total elapsed time: 27.833230201154947. Arrivals time: 0.21059077698737383 Scheduler time: 27.28132438333705 Scheduler overhead time: 0.1352659221738577 Adapter cache time: 0.02145026670768857 Engine time: 0.12897977232933044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 27.834169073030353,
    "estimated_duration": 3599.978251539579,
    "input_throughput": 4704.375920259307,
    "output_throughput": 4221.442724966673,
    "total_throughput": 8925.81864522598,
    "itl": 42.95148875536358,
    "ttft": 243774.48945090416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575142841227402,
    "arrivals": 71480,
    "finished_requests": 68469,
    "scheduler_time": 73.22546466400163
}
#Debug simulation 
Total elapsed time: 27.834283242933452. Arrivals time: 0.2082329671829939 Scheduler time: 27.283162011299282 Scheduler overhead time: 0.13599561108276248 Adapter cache time: 0.02132505178451538 Engine time: 0.1299559730105102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 29.696031578816473,
    "estimated_duration": 3599.942778655686,
    "input_throughput": 4721.644494123701,
    "output_throughput": 4236.715119592949,
    "total_throughput": 8958.359613716651,
    "itl": 43.49495337003091,
    "ttft": 238668.99945935863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8739876612322433,
    "arrivals": 71480,
    "finished_requests": 68759,
    "scheduler_time": 75.61981185576747
}
#Debug simulation 
Total elapsed time: 29.696144804824144. Arrivals time: 0.20880550844594836 Scheduler time: 29.14556823996827 Scheduler overhead time: 0.13584386464208364 Adapter cache time: 0.02115268586203456 Engine time: 0.12940832134336233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 26.81674071215093,
    "estimated_duration": 3599.9483246084533,
    "input_throughput": 4758.199411616015,
    "output_throughput": 4266.649022433007,
    "total_throughput": 9024.848434049021,
    "itl": 43.36513112609336,
    "ttft": 208897.79990047312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0372762900218422,
    "arrivals": 71480,
    "finished_requests": 69167,
    "scheduler_time": 72.72649447924516
}
#Debug simulation 
Total elapsed time: 26.816884480882436. Arrivals time: 0.20532474061474204 Scheduler time: 26.272462306078523 Scheduler overhead time: 0.13428669655695558 Adapter cache time: 0.02106994530186057 Engine time: 0.12852155603468418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 25.688862677197903,
    "estimated_duration": 3599.9490999418053,
    "input_throughput": 4712.439406511119,
    "output_throughput": 4225.38168671493,
    "total_throughput": 8937.82109322605,
    "itl": 43.23317391550837,
    "ttft": 231581.59035223213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9358857423043823,
    "arrivals": 71480,
    "finished_requests": 68546,
    "scheduler_time": 70.8728576151056
}
#Debug simulation 
Total elapsed time: 25.689000540412962. Arrivals time: 0.20681074121966958 Scheduler time: 25.140478941146284 Scheduler overhead time: 0.13528562476858497 Adapter cache time: 0.021480270195752382 Engine time: 0.12950443010777235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 28.234427565708756,
    "estimated_duration": 3599.9410579824007,
    "input_throughput": 4673.049566380496,
    "output_throughput": 4193.034484975668,
    "total_throughput": 8866.084051356165,
    "itl": 42.632896444046544,
    "ttft": 265404.7778781979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9572251204028777,
    "arrivals": 71480,
    "finished_requests": 68020,
    "scheduler_time": 73.49069307792476
}
#Debug simulation 
Total elapsed time: 28.234657221008092. Arrivals time: 0.20987144811078906 Scheduler time: 27.681907591875643 Scheduler overhead time: 0.13604156440123916 Adapter cache time: 0.02107529668137431 Engine time: 0.13010374316945672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.918305254075676,
    "estimated_duration": 3600.020326446625,
    "input_throughput": 4706.480648325642,
    "output_throughput": 4123.581439511658,
    "total_throughput": 8830.0620878373,
    "itl": 42.454801330723,
    "ttft": 178061.31029934666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.288465340195698,
    "arrivals": 69367,
    "finished_requests": 67430,
    "scheduler_time": 65.37461052013354
}
#Debug simulation 
Total elapsed time: 21.918453089892864. Arrivals time: 0.19758283533155918 Scheduler time: 21.374946610536426 Scheduler overhead time: 0.13711049128323793 Adapter cache time: 0.022194904275238514 Engine time: 0.13051662174984813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.62627545138821,
    "estimated_duration": 3600.028255435203,
    "input_throughput": 4710.012199043196,
    "output_throughput": 4127.772046667889,
    "total_throughput": 8837.784245711086,
    "itl": 42.34378784350107,
    "ttft": 175443.02587487054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3679250702983758,
    "arrivals": 69367,
    "finished_requests": 67438,
    "scheduler_time": 64.8381838755158
}
#Debug simulation 
Total elapsed time: 21.626377610955387. Arrivals time: 0.19889838993549347 Scheduler time: 21.081418965011835 Scheduler overhead time: 0.1370979342609644 Adapter cache time: 0.022275301162153482 Engine time: 0.13039099425077438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.664137291722,
    "estimated_duration": 3600.0003568550087,
    "input_throughput": 4710.0486997765365,
    "output_throughput": 4127.804035270126,
    "total_throughput": 8837.852735046663,
    "itl": 42.34289433314908,
    "ttft": 175394.0355009576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3696554047055622,
    "arrivals": 69367,
    "finished_requests": 67438,
    "scheduler_time": 64.83785758210287
}
#Debug simulation 
Total elapsed time: 21.664247496053576. Arrivals time: 0.19558423291891813 Scheduler time: 21.12258807104081 Scheduler overhead time: 0.1374646401964128 Adapter cache time: 0.02219636645168066 Engine time: 0.13027104223147035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 21.52034142287448,
    "estimated_duration": 3600.026883644516,
    "input_throughput": 4700.268233238802,
    "output_throughput": 4118.808130951771,
    "total_throughput": 8819.076364190574,
    "itl": 41.12199961266942,
    "ttft": 177676.75928669432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2946755713527074,
    "arrivals": 69367,
    "finished_requests": 67357,
    "scheduler_time": 64.26482067096664
}
#Debug simulation 
Total elapsed time: 21.520478202030063. Arrivals time: 0.19948258344084024 Scheduler time: 20.97408643644303 Scheduler overhead time: 0.13758109929040074 Adapter cache time: 0.021988388616591692 Engine time: 0.13112042378634214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 21.623404796235263,
    "estimated_duration": 3600.024138852584,
    "input_throughput": 4710.017584883291,
    "output_throughput": 4127.776766723647,
    "total_throughput": 8837.794351606937,
    "itl": 42.34500712346985,
    "ttft": 175441.68897926252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3883927188813732,
    "arrivals": 69367,
    "finished_requests": 67438,
    "scheduler_time": 64.83772319366668
}
#Debug simulation 
Total elapsed time: 21.623498376924545. Arrivals time: 0.2011391455307603 Scheduler time: 21.077097275294363 Scheduler overhead time: 0.13678956916555762 Adapter cache time: 0.022348052356392145 Engine time: 0.12970469798892736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.52887720009312,
    "estimated_duration": 3600.0057094509757,
    "input_throughput": 4700.295878858641,
    "output_throughput": 4118.832356591273,
    "total_throughput": 8819.128235449914,
    "itl": 41.12195225892872,
    "ttft": 177626.36763707767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2378808220894937,
    "arrivals": 69367,
    "finished_requests": 67357,
    "scheduler_time": 64.26430696275185
}
#Debug simulation 
Total elapsed time: 21.52902637794614. Arrivals time: 0.2000797144137323 Scheduler time: 20.98138343775645 Scheduler overhead time: 0.13736817380413413 Adapter cache time: 0.022123875562101603 Engine time: 0.13129989942535758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.840556610841304,
    "estimated_duration": 3600.0148878174236,
    "input_throughput": 4706.487758519318,
    "output_throughput": 4123.587669105459,
    "total_throughput": 8830.075427624777,
    "itl": 42.4587711551934,
    "ttft": 178062.3052480378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4151288055256015,
    "arrivals": 69367,
    "finished_requests": 67430,
    "scheduler_time": 65.37449242006822
}
#Debug simulation 
Total elapsed time: 21.840651474893093. Arrivals time: 0.19811617769300938 Scheduler time: 21.2969704028219 Scheduler overhead time: 0.13692743564024568 Adapter cache time: 0.022269594483077526 Engine time: 0.12999909976497293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.898896630853415,
    "estimated_duration": 3600.024752315869,
    "input_throughput": 4617.832971649904,
    "output_throughput": 4062.5201231163023,
    "total_throughput": 8680.353094766206,
    "itl": 40.96962284559397,
    "ttft": 173081.79347914841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1293199775111893,
    "arrivals": 68405,
    "finished_requests": 66543,
    "scheduler_time": 63.758211455036744
}
#Debug simulation 
Total elapsed time: 21.8990381360054. Arrivals time: 0.19313459564000368 Scheduler time: 21.357365924399346 Scheduler overhead time: 0.13796813786029816 Adapter cache time: 0.021804198622703552 Engine time: 0.13179364195093513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.561851107049733,
    "estimated_duration": 3600.0222338881636,
    "input_throughput": 4590.484148802449,
    "output_throughput": 4037.3161763232374,
    "total_throughput": 8627.800325125687,
    "itl": 40.785027481789335,
    "ttft": 189861.18658036488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2476168048102472,
    "arrivals": 68405,
    "finished_requests": 66196,
    "scheduler_time": 63.16163354862469
}
#Debug simulation 
Total elapsed time: 21.56195953907445. Arrivals time: 0.19552982691675425 Scheduler time: 21.01825363561511 Scheduler overhead time: 0.13830130640417337 Adapter cache time: 0.022037940099835396 Engine time: 0.13128112349659204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.603632661979645,
    "estimated_duration": 3600.0254236280534,
    "input_throughput": 4590.480081483839,
    "output_throughput": 4037.3125991294846,
    "total_throughput": 8627.792680613324,
    "itl": 40.78507442445176,
    "ttft": 189861.75573426433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2496340538188886,
    "arrivals": 68405,
    "finished_requests": 66196,
    "scheduler_time": 63.16184303139899
}
#Debug simulation 
Total elapsed time: 21.603822393342853. Arrivals time: 0.19539939938113093 Scheduler time: 21.06061339750886 Scheduler overhead time: 0.1377516482025385 Adapter cache time: 0.02193142520263791 Engine time: 0.1309490855783224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 21.857246499042958,
    "estimated_duration": 3600.0012382467457,
    "input_throughput": 4617.86313387389,
    "output_throughput": 4062.5466582124504,
    "total_throughput": 8680.40979208634,
    "itl": 40.97119501009921,
    "ttft": 173037.92196768825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1544029736239476,
    "arrivals": 68405,
    "finished_requests": 66543,
    "scheduler_time": 63.759669869174125
}
#Debug simulation 
Total elapsed time: 21.857383070979267. Arrivals time: 0.19775849115103483 Scheduler time: 21.31121490849182 Scheduler overhead time: 0.13772787433117628 Adapter cache time: 0.02194054238498211 Engine time: 0.13201905088499188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 21.915752179920673,
    "estimated_duration": 3600.0019872716402,
    "input_throughput": 4617.862173070407,
    "output_throughput": 4062.545812949422,
    "total_throughput": 8680.407986019829,
    "itl": 40.96910002950764,
    "ttft": 173032.01639554437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.222626550849532,
    "arrivals": 68405,
    "finished_requests": 66543,
    "scheduler_time": 63.75776607238472
}
#Debug simulation 
Total elapsed time: 21.915846390649676. Arrivals time: 0.19294844381511211 Scheduler time: 21.374776804819703 Scheduler overhead time: 0.137765989638865 Adapter cache time: 0.022057922091335058 Engine time: 0.13184037432074547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.853786701802164,
    "estimated_duration": 3600.017546820238,
    "input_throughput": 4617.842214320216,
    "output_throughput": 4062.528254318614,
    "total_throughput": 8680.37046863883,
    "itl": 40.96831569084712,
    "ttft": 173031.2890404635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1033285588189006,
    "arrivals": 68405,
    "finished_requests": 66543,
    "scheduler_time": 63.75808753402162
}
#Debug simulation 
Total elapsed time: 21.85392296873033. Arrivals time: 0.19559010257944465 Scheduler time: 21.3104812852107 Scheduler overhead time: 0.13855947833508253 Adapter cache time: 0.022093888837844133 Engine time: 0.1304193427786231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.57503762608394,
    "estimated_duration": 3600.011311352933,
    "input_throughput": 4590.427243349261,
    "output_throughput": 4037.1987038390557,
    "total_throughput": 8627.625947188317,
    "itl": 40.78730816443592,
    "ttft": 189863.73468003777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.282078530713918,
    "arrivals": 68405,
    "finished_requests": 66195,
    "scheduler_time": 63.16135612554602
}
#Debug simulation 
Total elapsed time: 21.575128902681172. Arrivals time: 0.19393433863297105 Scheduler time: 21.033536699134856 Scheduler overhead time: 0.13779796613380313 Adapter cache time: 0.02194960555061698 Engine time: 0.13133652275428176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 19.82541828090325,
    "estimated_duration": 3600.0019850710332,
    "input_throughput": 4572.803034072544,
    "output_throughput": 4047.283323848642,
    "total_throughput": 8620.086357921185,
    "itl": 40.917644329706654,
    "ttft": 156078.279844046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2303160730609737,
    "arrivals": 67954,
    "finished_requests": 66287,
    "scheduler_time": 60.87111218295639
}
#Debug simulation 
Total elapsed time: 19.82555195596069. Arrivals time: 0.19629698572680354 Scheduler time: 19.2779031698592 Scheduler overhead time: 0.1393615328706801 Adapter cache time: 0.022395688574761152 Engine time: 0.1325760125182569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.81471321824938,
    "estimated_duration": 3600.0277550108735,
    "input_throughput": 4572.770300752939,
    "output_throughput": 4047.254352336512,
    "total_throughput": 8620.024653089451,
    "itl": 40.918327619526885,
    "ttft": 156130.76737609916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3127295498480145,
    "arrivals": 67954,
    "finished_requests": 66287,
    "scheduler_time": 60.871814723910425
}
#Debug simulation 
Total elapsed time: 19.814806685317308. Arrivals time: 0.192801165394485 Scheduler time: 19.273481684271246 Scheduler overhead time: 0.1379048083908856 Adapter cache time: 0.022167358547449112 Engine time: 0.13166985800489783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.838283281307667,
    "estimated_duration": 3600.001828624725,
    "input_throughput": 4572.803232794152,
    "output_throughput": 4047.28349973259,
    "total_throughput": 8620.08673252674,
    "itl": 40.91843805910601,
    "ttft": 156080.16152639393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3148886482231394,
    "arrivals": 67954,
    "finished_requests": 66287,
    "scheduler_time": 60.87153128190867
}
#Debug simulation 
Total elapsed time: 19.83842654619366. Arrivals time: 0.19147481070831418 Scheduler time: 19.297667449805886 Scheduler overhead time: 0.13816349674016237 Adapter cache time: 0.02202516281977296 Engine time: 0.13227477483451366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 19.86468827025965,
    "estimated_duration": 3600.001876021118,
    "input_throughput": 4572.803172590189,
    "output_throughput": 4047.2834464474404,
    "total_throughput": 8620.08661903763,
    "itl": 40.917210298208865,
    "ttft": 156080.19998553765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2510316567635145,
    "arrivals": 67954,
    "finished_requests": 66287,
    "scheduler_time": 60.87162070288966
}
#Debug simulation 
Total elapsed time: 19.86478000599891. Arrivals time: 0.1928429608233273 Scheduler time: 19.32106697326526 Scheduler overhead time: 0.1393133206292987 Adapter cache time: 0.022298818454146385 Engine time: 0.1323150023818016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.778723291121423,
    "estimated_duration": 3600.0257367940935,
    "input_throughput": 4572.772864301765,
    "output_throughput": 4047.2566212749152,
    "total_throughput": 8620.02948557668,
    "itl": 40.91876773762964,
    "ttft": 156131.06898253824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3338774699717806,
    "arrivals": 67954,
    "finished_requests": 66287,
    "scheduler_time": 60.87184885487834
}
#Debug simulation 
Total elapsed time: 19.778868955094367. Arrivals time: 0.19179335795342922 Scheduler time: 19.238065177109092 Scheduler overhead time: 0.13825601805001497 Adapter cache time: 0.022314539644867182 Engine time: 0.1318611060269177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.760012711863965,
    "estimated_duration": 3600.01199823653,
    "input_throughput": 4562.0964619132155,
    "output_throughput": 4034.401553971083,
    "total_throughput": 8596.4980158843,
    "itl": 40.67057039592111,
    "ttft": 172585.35081917694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0913683576392923,
    "arrivals": 67954,
    "finished_requests": 66126,
    "scheduler_time": 63.147667021771205
}
#Debug simulation 
Total elapsed time: 21.760118750855327. Arrivals time: 0.19634478073567152 Scheduler time: 21.212355684954673 Scheduler overhead time: 0.13948967214673758 Adapter cache time: 0.021942978259176016 Engine time: 0.13293995056301355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.710901204962283,
    "estimated_duration": 3600.016705720412,
    "input_throughput": 4562.090496386576,
    "output_throughput": 4034.396278473261,
    "total_throughput": 8596.486774859837,
    "itl": 40.67094920057159,
    "ttft": 172637.15821203042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.225708697475496,
    "arrivals": 67954,
    "finished_requests": 66126,
    "scheduler_time": 63.1477075411609
}
#Debug simulation 
Total elapsed time: 21.711016196757555. Arrivals time: 0.19327605282887816 Scheduler time: 21.167875724844635 Scheduler overhead time: 0.13898947788402438 Adapter cache time: 0.02205624897032976 Engine time: 0.13178928615525365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 19.066158017143607,
    "estimated_duration": 3600.028998133433,
    "input_throughput": 4621.0816659047905,
    "output_throughput": 4048.646832444147,
    "total_throughput": 8669.728498348937,
    "itl": 41.25499531425229,
    "ttft": 136858.7911036271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1385014407429879,
    "arrivals": 67703,
    "finished_requests": 66348,
    "scheduler_time": 59.71404972353719
}
#Debug simulation 
Total elapsed time: 19.066264341119677. Arrivals time: 0.19333164719864726 Scheduler time: 18.524756231345236 Scheduler overhead time: 0.1386435041204095 Adapter cache time: 0.021933818701654673 Engine time: 0.13113584788516164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.068867207039148,
    "estimated_duration": 3600.009777486318,
    "input_throughput": 4621.106060332978,
    "output_throughput": 4048.6678928347424,
    "total_throughput": 8669.77395316772,
    "itl": 41.25995123955451,
    "ttft": 136908.57450743899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.214038943995261,
    "arrivals": 67703,
    "finished_requests": 66347,
    "scheduler_time": 59.71289295306901
}
#Debug simulation 
Total elapsed time: 19.06906446814537. Arrivals time: 0.19196167821064591 Scheduler time: 18.52886767173186 Scheduler overhead time: 0.13804445089772344 Adapter cache time: 0.02202757354825735 Engine time: 0.13152849720790982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.05148392310366,
    "estimated_duration": 3600.0130200228596,
    "input_throughput": 4621.101898096569,
    "output_throughput": 4048.6642461941565,
    "total_throughput": 8669.766144290727,
    "itl": 41.26011792727282,
    "ttft": 136908.81804911036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2161636378802427,
    "arrivals": 67703,
    "finished_requests": 66347,
    "scheduler_time": 59.71301040989887
}
#Debug simulation 
Total elapsed time: 19.051637354772538. Arrivals time: 0.19259138451889157 Scheduler time: 18.510455759242177 Scheduler overhead time: 0.13827624497935176 Adapter cache time: 0.021949505899101496 Engine time: 0.13149634515866637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 19.031782889273018,
    "estimated_duration": 3600.0248291493876,
    "input_throughput": 4621.087017316699,
    "output_throughput": 4048.6515209518243,
    "total_throughput": 8669.738538268522,
    "itl": 41.25691093103912,
    "ttft": 136859.1782065982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1564270040951667,
    "arrivals": 67703,
    "finished_requests": 66348,
    "scheduler_time": 59.71415505808564
}
#Debug simulation 
Total elapsed time: 19.031886029057205. Arrivals time: 0.19214784959331155 Scheduler time: 18.49050503456965 Scheduler overhead time: 0.13899814942851663 Adapter cache time: 0.021913753356784582 Engine time: 0.13163135293871164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.0507098371163,
    "estimated_duration": 3600.0100914748373,
    "input_throughput": 4621.105657285705,
    "output_throughput": 4048.667539714833,
    "total_throughput": 8669.773197000539,
    "itl": 41.25877115172739,
    "ttft": 136912.63985598995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2338949217647357,
    "arrivals": 67703,
    "finished_requests": 66347,
    "scheduler_time": 59.71395022850142
}
#Debug simulation 
Total elapsed time: 19.05085322400555. Arrivals time: 0.19071150198578835 Scheduler time: 18.51076421700418 Scheduler overhead time: 0.13857334991917014 Adapter cache time: 0.02208354463800788 Engine time: 0.13178806146606803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 19.04609425365925,
    "estimated_duration": 3600.0217889130413,
    "input_throughput": 4621.090642071624,
    "output_throughput": 4048.654384506023,
    "total_throughput": 8669.745026577646,
    "itl": 41.25675813772993,
    "ttft": 136911.25405128495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1122987097036068,
    "arrivals": 67703,
    "finished_requests": 66347,
    "scheduler_time": 59.713844509523234
}
#Debug simulation 
Total elapsed time: 19.046185008715838. Arrivals time: 0.1922005983069539 Scheduler time: 18.50559877930209 Scheduler overhead time: 0.1383502776734531 Adapter cache time: 0.02194075332954526 Engine time: 0.13171074772253633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.05077109206468,
    "estimated_duration": 3600.0307505903793,
    "input_throughput": 4621.079416411043,
    "output_throughput": 4048.6448616056305,
    "total_throughput": 8669.724278016674,
    "itl": 41.25831427337658,
    "ttft": 136859.99820183872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2474763306975394,
    "arrivals": 67703,
    "finished_requests": 66348,
    "scheduler_time": 59.71432535026569
}
#Debug simulation 
Total elapsed time: 19.050909515004605. Arrivals time: 0.19076947681605816 Scheduler time: 18.51170265302062 Scheduler overhead time: 0.1381874457001686 Adapter cache time: 0.022089450620114803 Engine time: 0.1315190284512937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 19.80514998920262,
    "estimated_duration": 3600.0292178384307,
    "input_throughput": 4535.2712469937405,
    "output_throughput": 3983.9243328729226,
    "total_throughput": 8519.195579866664,
    "itl": 40.00691357484107,
    "ttft": 136369.7559858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.245618511780638,
    "arrivals": 66533,
    "finished_requests": 65298,
    "scheduler_time": 59.60608736813579
}
#Debug simulation 
Total elapsed time: 19.805256130173802. Arrivals time: 0.19056528573855758 Scheduler time: 19.26077796984464 Scheduler overhead time: 0.14174419781193137 Adapter cache time: 0.022260666824877262 Engine time: 0.13259285828098655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.79876423534006,
    "estimated_duration": 3600.032320984673,
    "input_throughput": 4535.267337692748,
    "output_throughput": 3983.9208988204696,
    "total_throughput": 8519.188236513217,
    "itl": 40.00766017262842,
    "ttft": 136381.72710571866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.330948563870049,
    "arrivals": 66533,
    "finished_requests": 65298,
    "scheduler_time": 59.60928739744533
}
#Debug simulation 
Total elapsed time: 19.798877785913646. Arrivals time: 0.1892508789896965 Scheduler time: 19.25778518198058 Scheduler overhead time: 0.13988456455990672 Adapter cache time: 0.022203620057553053 Engine time: 0.13258671388030052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.799595586955547,
    "estimated_duration": 3600.000679375685,
    "input_throughput": 4535.152755257636,
    "output_throughput": 3983.845081520144,
    "total_throughput": 8518.99783677778,
    "itl": 40.00787728283832,
    "ttft": 136384.21042516833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.332804222423591,
    "arrivals": 66533,
    "finished_requests": 65297,
    "scheduler_time": 59.60893110257935
}
#Debug simulation 
Total elapsed time: 19.79970030626282. Arrivals time: 0.19107939396053553 Scheduler time: 19.257150338962674 Scheduler overhead time: 0.13929108763113618 Adapter cache time: 0.02267190534621477 Engine time: 0.13210022170096636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 18.039386296179146,
    "estimated_duration": 3600.0123254199493,
    "input_throughput": 4537.07474406901,
    "output_throughput": 3973.947783173532,
    "total_throughput": 8511.022527242543,
    "itl": 39.683807946119224,
    "ttft": 130080.56158796109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4447355106030553,
    "arrivals": 66533,
    "finished_requests": 65271,
    "scheduler_time": 57.271053064153385
}
#Debug simulation 
Total elapsed time: 18.03950424399227. Arrivals time: 0.1874590632505715 Scheduler time: 17.49992207903415 Scheduler overhead time: 0.139535425696522 Adapter cache time: 0.022569854743778706 Engine time: 0.13250000681728125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.49171756580472,
    "estimated_duration": 3600.02984163074,
    "input_throughput": 4526.148036767115,
    "output_throughput": 3978.5509093202454,
    "total_throughput": 8504.69894608736,
    "itl": 40.30200814476743,
    "ttft": 140475.60258713982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3777474047057374,
    "arrivals": 66533,
    "finished_requests": 65191,
    "scheduler_time": 59.078400959735845
}
#Debug simulation 
Total elapsed time: 19.49184370180592. Arrivals time: 0.19177487259730697 Scheduler time: 18.947268622461706 Scheduler overhead time: 0.1410038904286921 Adapter cache time: 0.022438358515501022 Engine time: 0.13190997811034322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.036088339053094,
    "estimated_duration": 3600.0365277269175,
    "input_throughput": 4537.060075419044,
    "output_throughput": 3973.9469002091237,
    "total_throughput": 8511.006975628166,
    "itl": 39.68405551367665,
    "ttft": 130026.78710446935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381403236244793,
    "arrivals": 66533,
    "finished_requests": 65272,
    "scheduler_time": 57.2719802833703
}
#Debug simulation 
Total elapsed time: 18.03620117297396. Arrivals time: 0.1890060226432979 Scheduler time: 17.49531758064404 Scheduler overhead time: 0.1395273646339774 Adapter cache time: 0.022679727990180254 Engine time: 0.13228336721658707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.070395576301962,
    "estimated_duration": 3600.0132982115797,
    "input_throughput": 4537.0735180656675,
    "output_throughput": 3973.946709337737,
    "total_throughput": 8511.020227403405,
    "itl": 39.68445985896871,
    "ttft": 130082.660624022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5519511116296048,
    "arrivals": 66533,
    "finished_requests": 65271,
    "scheduler_time": 57.27167364877741
}
#Debug simulation 
Total elapsed time: 18.070527740288526. Arrivals time: 0.19256936758756638 Scheduler time: 17.525324455928057 Scheduler overhead time: 0.1398689318448305 Adapter cache time: 0.022914895322173834 Engine time: 0.1324243857525289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.14605411980301,
    "estimated_duration": 3600.0334838765025,
    "input_throughput": 4493.498761178449,
    "output_throughput": 3962.551477337694,
    "total_throughput": 8456.050238516144,
    "itl": 40.591385773570444,
    "ttft": 134452.44740505028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0313843697053378,
    "arrivals": 66085,
    "finished_requests": 65012,
    "scheduler_time": 61.077332608766795
}
#Debug simulation 
Total elapsed time: 21.146281762979925. Arrivals time: 0.19043129868805408 Scheduler time: 20.603372843936086 Scheduler overhead time: 0.13955021230503917 Adapter cache time: 0.02197472471743822 Engine time: 0.13329010270535946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.23233587294817,
    "estimated_duration": 3600.03957796165,
    "input_throughput": 4493.491154660946,
    "output_throughput": 3962.544769598631,
    "total_throughput": 8456.035924259577,
    "itl": 40.592629967579924,
    "ttft": 134453.5904401455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0999894913495574,
    "arrivals": 66085,
    "finished_requests": 65012,
    "scheduler_time": 61.07766398749919
}
#Debug simulation 
Total elapsed time: 21.232472192961723. Arrivals time: 0.1928028855472803 Scheduler time: 20.686444535385817 Scheduler overhead time: 0.1403531040996313 Adapter cache time: 0.02192794159054756 Engine time: 0.1333488393574953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.23411352094263,
    "estimated_duration": 3600.0428746771063,
    "input_throughput": 4493.487039776136,
    "output_throughput": 3962.5411409244616,
    "total_throughput": 8456.028180700598,
    "itl": 40.592423027757,
    "ttft": 134453.78412415384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1018837857991517,
    "arrivals": 66085,
    "finished_requests": 65012,
    "scheduler_time": 61.07776590695179
}
#Debug simulation 
Total elapsed time: 21.23420953610912. Arrivals time: 0.18923295382410288 Scheduler time: 20.69211205933243 Scheduler overhead time: 0.14067640621215105 Adapter cache time: 0.02179555781185627 Engine time: 0.13295642146840692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 20.129927722737193,
    "estimated_duration": 3600.0060799303474,
    "input_throughput": 4510.786548536659,
    "output_throughput": 3980.3724443368837,
    "total_throughput": 8491.158992873543,
    "itl": 40.51403363316457,
    "ttft": 118185.96571036406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.206459614592604,
    "arrivals": 66085,
    "finished_requests": 65245,
    "scheduler_time": 60.00830410237435
}
#Debug simulation 
Total elapsed time: 20.130067500751466. Arrivals time: 0.18619405385106802 Scheduler time: 19.593283362220973 Scheduler overhead time: 0.13951146369799972 Adapter cache time: 0.022119046188890934 Engine time: 0.13189715752378106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 20.266840527765453,
    "estimated_duration": 3600.0216763691888,
    "input_throughput": 4510.859228041671,
    "output_throughput": 3980.4701994051147,
    "total_throughput": 8491.329427446786,
    "itl": 40.51262012182768,
    "ttft": 118132.21431656838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.280934129133825,
    "arrivals": 66085,
    "finished_requests": 65246,
    "scheduler_time": 60.00856701448563
}
#Debug simulation 
Total elapsed time: 20.26692889491096. Arrivals time: 0.18837967980653048 Scheduler time: 19.726716443430632 Scheduler overhead time: 0.13942302344366908 Adapter cache time: 0.02215446811169386 Engine time: 0.1330239102244377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.186658607795835,
    "estimated_duration": 3600.0331617058837,
    "input_throughput": 4493.499163306211,
    "output_throughput": 3962.5518319504445,
    "total_throughput": 8456.050995256655,
    "itl": 40.59138929799959,
    "ttft": 134451.8471178885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0076469493820344,
    "arrivals": 66085,
    "finished_requests": 65012,
    "scheduler_time": 61.07734172883583
}
#Debug simulation 
Total elapsed time: 21.186798273120075. Arrivals time: 0.1878270711749792 Scheduler time: 20.645912521518767 Scheduler overhead time: 0.14012518990784883 Adapter cache time: 0.02228320064023137 Engine time: 0.13314587250351906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.141127107664943,
    "estimated_duration": 3600.0227578636473,
    "input_throughput": 4493.51214923978,
    "output_throughput": 3962.5632834791945,
    "total_throughput": 8456.075432718975,
    "itl": 40.592398865692566,
    "ttft": 134453.690876513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1303041415289086,
    "arrivals": 66085,
    "finished_requests": 65012,
    "scheduler_time": 61.07753905765303
}
#Debug simulation 
Total elapsed time: 21.141249367035925. Arrivals time: 0.18841696484014392 Scheduler time: 20.59751628059894 Scheduler overhead time: 0.14172951225191355 Adapter cache time: 0.022339516784995794 Engine time: 0.13320224406197667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 19.017064818181098,
    "estimated_duration": 3600.0248339966074,
    "input_throughput": 4518.396886151961,
    "output_throughput": 3981.7891989612617,
    "total_throughput": 8500.186085113222,
    "itl": 41.20838119500333,
    "ttft": 114350.309726059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0834126613521964,
    "arrivals": 65845,
    "finished_requests": 64982,
    "scheduler_time": 58.50770780836593
}
#Debug simulation 
Total elapsed time: 19.017183632124215. Arrivals time: 0.18495924677699804 Scheduler time: 18.477378136012703 Scheduler overhead time: 0.1398435328155756 Adapter cache time: 0.02214454486966133 Engine time: 0.13564338209107518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.965893124230206,
    "estimated_duration": 3600.00652347325,
    "input_throughput": 4518.397090099292,
    "output_throughput": 3981.7755624982306,
    "total_throughput": 8500.172652597523,
    "itl": 41.210723290492545,
    "ttft": 114404.85366263449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1569492761394997,
    "arrivals": 65845,
    "finished_requests": 64981,
    "scheduler_time": 58.50756873496085
}
#Debug simulation 
Total elapsed time: 18.96602230705321. Arrivals time: 0.1887937318533659 Scheduler time: 18.42541711963713 Scheduler overhead time: 0.13926683040335774 Adapter cache time: 0.02209015004336834 Engine time: 0.13190637528896332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.024755529128015,
    "estimated_duration": 3600.0095388348736,
    "input_throughput": 4518.393305497879,
    "output_throughput": 3981.7722273700606,
    "total_throughput": 8500.165532867939,
    "itl": 41.21003851949127,
    "ttft": 114405.10796851382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1586823186464672,
    "arrivals": 65845,
    "finished_requests": 64981,
    "scheduler_time": 58.507650591115635
}
#Debug simulation 
Total elapsed time: 19.024850516114384. Arrivals time: 0.18778969207778573 Scheduler time: 18.48539621056989 Scheduler overhead time: 0.13931608479470015 Adapter cache time: 0.02198786474764347 Engine time: 0.13306796737015247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 18.98680992797017,
    "estimated_duration": 3600.02206494036,
    "input_throughput": 4518.400361601527,
    "output_throughput": 3981.792261664228,
    "total_throughput": 8500.192623265755,
    "itl": 41.24190821199429,
    "ttft": 114331.87919837316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1058748613344518,
    "arrivals": 65845,
    "finished_requests": 64982,
    "scheduler_time": 58.50448125688406
}
#Debug simulation 
Total elapsed time: 18.98694570781663. Arrivals time: 0.1876723226159811 Scheduler time: 18.448470344301313 Scheduler overhead time: 0.13950103288516402 Adapter cache time: 0.021852894220501184 Engine time: 0.13241173746064305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.051005139015615,
    "estimated_duration": 3600.0004258263334,
    "input_throughput": 4518.404743317854,
    "output_throughput": 3981.7823067922886,
    "total_throughput": 8500.187050110144,
    "itl": 41.209052507864385,
    "ttft": 114405.88813629579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1744015419483234,
    "arrivals": 65845,
    "finished_requests": 64981,
    "scheduler_time": 58.50761924441597
}
#Debug simulation 
Total elapsed time: 19.051097691990435. Arrivals time: 0.18688068678602576 Scheduler time: 18.51238742703572 Scheduler overhead time: 0.140025251545012 Adapter cache time: 0.02214312134310603 Engine time: 0.13242465490475297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.316439234651625,
    "estimated_duration": 3599.9942574379556,
    "input_throughput": 4494.36383587866,
    "output_throughput": 3959.895483318195,
    "total_throughput": 8454.259319196855,
    "itl": 40.056109326968894,
    "ttft": 128269.06507704788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0853882570494882,
    "arrivals": 65845,
    "finished_requests": 64644,
    "scheduler_time": 57.18105418246059
}
#Debug simulation 
Total elapsed time: 18.31657110294327. Arrivals time: 0.18746245745569468 Scheduler time: 17.775983109138906 Scheduler overhead time: 0.14044637512415648 Adapter cache time: 0.022148835007101297 Engine time: 0.13304940704256296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.9341297079809,
    "estimated_duration": 3600.0210193064404,
    "input_throughput": 4518.40167398072,
    "output_throughput": 3981.7934181844334,
    "total_throughput": 8500.195092165153,
    "itl": 41.20874848200907,
    "ttft": 114351.15725842777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.188988981172446,
    "arrivals": 65845,
    "finished_requests": 64982,
    "scheduler_time": 58.50782304759707
}
#Debug simulation 
Total elapsed time: 18.934361468069255. Arrivals time: 0.18595667881891131 Scheduler time: 18.396482780575752 Scheduler overhead time: 0.14049871871247888 Adapter cache time: 0.02183183003216982 Engine time: 0.13228292437270284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 17.99565118411556,
    "estimated_duration": 3599.9207260049866,
    "input_throughput": 4394.935112240048,
    "output_throughput": 3904.0387468744866,
    "total_throughput": 8298.973859114534,
    "itl": 40.46963078388594,
    "ttft": 140051.37029235554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0987151000718607,
    "arrivals": 65172,
    "finished_requests": 63739,
    "scheduler_time": 55.95132047841579
}
#Debug simulation 
Total elapsed time: 17.995784531813115. Arrivals time: 0.18660452123731375 Scheduler time: 17.45578552270308 Scheduler overhead time: 0.14047680888324976 Adapter cache time: 0.02230431418865919 Engine time: 0.13310400815680623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.68937906809151,
    "estimated_duration": 3599.9292533535836,
    "input_throughput": 4421.697450073906,
    "output_throughput": 3923.0679288610086,
    "total_throughput": 8344.765378934915,
    "itl": 40.897896961737025,
    "ttft": 126364.96940155039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 332,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0817704773275238,
    "arrivals": 65172,
    "finished_requests": 64143,
    "scheduler_time": 58.390160131505866
}
#Debug simulation 
Total elapsed time: 19.689476660918444. Arrivals time: 0.18699036305770278 Scheduler time: 19.14702714793384 Scheduler overhead time: 0.14260542392730713 Adapter cache time: 0.0224052919074893 Engine time: 0.1327085760422051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 19.632491734810174,
    "estimated_duration": 3599.932130051057,
    "input_throughput": 4421.693916705658,
    "output_throughput": 3923.0647939464625,
    "total_throughput": 8344.75871065212,
    "itl": 40.89808765347208,
    "ttft": 126365.31615198636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 332,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0839682115987002,
    "arrivals": 65172,
    "finished_requests": 64143,
    "scheduler_time": 58.390238863218684
}
#Debug simulation 
Total elapsed time: 19.632643559016287. Arrivals time: 0.18876059306785464 Scheduler time: 19.089364451356232 Scheduler overhead time: 0.14110683230683208 Adapter cache time: 0.022333288565278053 Engine time: 0.13342046504840255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 19.59002906596288,
    "estimated_duration": 3599.9253934866274,
    "input_throughput": 4421.702191051013,
    "output_throughput": 3923.0721352038104,
    "total_throughput": 8344.774326254823,
    "itl": 40.8969943167795,
    "ttft": 126364.7224601178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 332,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0364163969806426,
    "arrivals": 65172,
    "finished_requests": 64143,
    "scheduler_time": 58.390055661091566
}
#Debug simulation 
Total elapsed time: 19.590117095038295. Arrivals time: 0.18434668565168977 Scheduler time: 19.04934557620436 Scheduler overhead time: 0.14344101445749402 Adapter cache time: 0.022339499555528164 Engine time: 0.13308884855359793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 19.663761258125305,
    "estimated_duration": 3599.950601873555,
    "input_throughput": 4421.6712284095665,
    "output_throughput": 3923.044664182325,
    "total_throughput": 8344.715892591892,
    "itl": 40.897541086671964,
    "ttft": 126365.28980255479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 332,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0979268818907493,
    "arrivals": 65172,
    "finished_requests": 64143,
    "scheduler_time": 58.3903503178554
}
#Debug simulation 
Total elapsed time: 19.663901540450752. Arrivals time: 0.18476805230602622 Scheduler time: 19.124994777608663 Scheduler overhead time: 0.14039217587560415 Adapter cache time: 0.022036096081137657 Engine time: 0.13395292358472943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.097872127778828,
    "estimated_duration": 3599.944739731922,
    "input_throughput": 4394.905795464566,
    "output_throughput": 3904.012704663511,
    "total_throughput": 8298.918500128077,
    "itl": 40.46850688569305,
    "ttft": 140051.33607774356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0734280558698799,
    "arrivals": 65172,
    "finished_requests": 63739,
    "scheduler_time": 55.95152137063726
}
#Debug simulation 
Total elapsed time: 18.097970005124807. Arrivals time: 0.18477870151400566 Scheduler time: 17.559019106905907 Scheduler overhead time: 0.1408126368187368 Adapter cache time: 0.022465435322374105 Engine time: 0.13330072397366166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.984896713867784,
    "estimated_duration": 3599.9438107299993,
    "input_throughput": 4394.906929614471,
    "output_throughput": 3904.0137121334883,
    "total_throughput": 8298.92064174796,
    "itl": 40.46910668804898,
    "ttft": 140053.40169791647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2023545702919398,
    "arrivals": 65172,
    "finished_requests": 63739,
    "scheduler_time": 55.95199839498434
}
#Debug simulation 
Total elapsed time: 17.985045013017952. Arrivals time: 0.1864511682651937 Scheduler time: 17.445842260029167 Scheduler overhead time: 0.14034075569361448 Adapter cache time: 0.022020332515239716 Engine time: 0.13277532160282135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.153420309070498,
    "estimated_duration": 3599.9984764790797,
    "input_throughput": 4403.11936340124,
    "output_throughput": 3915.024156839224,
    "total_throughput": 8318.143520240465,
    "itl": 40.44702274035376,
    "ttft": 129392.12154204583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9487512006191524,
    "arrivals": 64890,
    "finished_requests": 63679,
    "scheduler_time": 56.23265565269398
}
#Debug simulation 
Total elapsed time: 18.153513167984784. Arrivals time: 0.185771060641855 Scheduler time: 17.614730685949326 Scheduler overhead time: 0.1412271475419402 Adapter cache time: 0.021793715190142393 Engine time: 0.13234910741448402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 18.202593236695975,
    "estimated_duration": 3599.9961160508733,
    "input_throughput": 4403.12225041745,
    "output_throughput": 3915.026723823507,
    "total_throughput": 8318.148974240958,
    "itl": 40.449604583126316,
    "ttft": 129391.97731437293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0123120129737129,
    "arrivals": 64890,
    "finished_requests": 63679,
    "scheduler_time": 56.232646922484875
}
#Debug simulation 
Total elapsed time: 18.20280231582001. Arrivals time: 0.1874362900853157 Scheduler time: 17.661926633678377 Scheduler overhead time: 0.1405488564632833 Adapter cache time: 0.022094303276389837 Engine time: 0.13315992057323456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.648438579402864,
    "estimated_duration": 3599.973993198002,
    "input_throughput": 4423.163620094576,
    "output_throughput": 3934.0595311964657,
    "total_throughput": 8357.22315129104,
    "itl": 40.23235058010533,
    "ttft": 109091.48181471515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0670560884848295,
    "arrivals": 64890,
    "finished_requests": 64009,
    "scheduler_time": 55.65371724639586
}
#Debug simulation 
Total elapsed time: 17.64852783223614. Arrivals time: 0.18359749484807253 Scheduler time: 17.11268880823627 Scheduler overhead time: 0.13988820975646377 Adapter cache time: 0.022170658223330975 Engine time: 0.13281578104943037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 18.164083963260055,
    "estimated_duration": 3599.994407913669,
    "input_throughput": 4403.1243396253985,
    "output_throughput": 3915.0285814382823,
    "total_throughput": 8318.15292106368,
    "itl": 40.44762735226495,
    "ttft": 129391.70415112082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.966140741989951,
    "arrivals": 64890,
    "finished_requests": 63679,
    "scheduler_time": 56.23259711920934
}
#Debug simulation 
Total elapsed time: 18.16422285605222. Arrivals time: 0.18443253822624683 Scheduler time: 17.627178709488362 Scheduler overhead time: 0.14004325680434704 Adapter cache time: 0.02212452655658126 Engine time: 0.13275990961119533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 17.583939743228257,
    "estimated_duration": 3599.995667923057,
    "input_throughput": 4423.136989269379,
    "output_throughput": 3934.0358451516604,
    "total_throughput": 8357.17283442104,
    "itl": 40.22997249308854,
    "ttft": 109091.19680882565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0815177739225377,
    "arrivals": 64890,
    "finished_requests": 64009,
    "scheduler_time": 55.65372750826654
}
#Debug simulation 
Total elapsed time: 17.584031130187213. Arrivals time: 0.18243752839043736 Scheduler time: 17.04142797505483 Scheduler overhead time: 0.1421626005321741 Adapter cache time: 0.02194287907332182 Engine time: 0.13825675705447793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.160719432868063,
    "estimated_duration": 3599.9968915540076,
    "input_throughput": 4403.121301906879,
    "output_throughput": 3915.0258804573637,
    "total_throughput": 8318.147182364242,
    "itl": 40.448064288890265,
    "ttft": 129391.16954332808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9269155914196757,
    "arrivals": 64890,
    "finished_requests": 63679,
    "scheduler_time": 56.232407110617764
}
#Debug simulation 
Total elapsed time: 18.160962781868875. Arrivals time: 0.17609560443088412 Scheduler time: 17.627356785349548 Scheduler overhead time: 0.14322472177445889 Adapter cache time: 0.02228870429098606 Engine time: 0.1342024109326303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.56546615343541,
    "estimated_duration": 3599.983114909486,
    "input_throughput": 4423.152412591346,
    "output_throughput": 3934.0495629952657,
    "total_throughput": 8357.201975586611,
    "itl": 40.230897487344556,
    "ttft": 109091.57222487207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0949734290689275,
    "arrivals": 64890,
    "finished_requests": 64009,
    "scheduler_time": 55.653720187628736
}
#Debug simulation 
Total elapsed time: 17.565546836238354. Arrivals time: 0.1747014056891203 Scheduler time: 17.04078396083787 Scheduler overhead time: 0.13908777106553316 Adapter cache time: 0.022564267739653587 Engine time: 0.13165837293490767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 16.33099411521107,
    "estimated_duration": 3599.9852346349844,
    "input_throughput": 4386.483546675195,
    "output_throughput": 3855.612758202704,
    "total_throughput": 8242.0963048779,
    "itl": 38.511936072327956,
    "ttft": 115945.61674210694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.028323881961405,
    "arrivals": 64419,
    "finished_requests": 63310,
    "scheduler_time": 52.6629424868588
}
#Debug simulation 
Total elapsed time: 16.331076491158456. Arrivals time: 0.17276199394837022 Scheduler time: 15.803163186646998 Scheduler overhead time: 0.1429644115269184 Adapter cache time: 0.022120758425444365 Engine time: 0.13233418483287096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.253475887235254,
    "estimated_duration": 3600.007133907684,
    "input_throughput": 4372.655223855916,
    "output_throughput": 3844.3982151161204,
    "total_throughput": 8217.053438972036,
    "itl": 39.73856486998804,
    "ttft": 129256.58892725792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0165278492239358,
    "arrivals": 64419,
    "finished_requests": 63143,
    "scheduler_time": 53.87783360722323
}
#Debug simulation 
Total elapsed time: 17.25359345925972. Arrivals time: 0.17216550093144178 Scheduler time: 16.728075901512057 Scheduler overhead time: 0.1413082228973508 Adapter cache time: 0.02192494412884116 Engine time: 0.13277192786335945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.272337357979268,
    "estimated_duration": 3600.009873715895,
    "input_throughput": 4372.65189602152,
    "output_throughput": 3844.395289314757,
    "total_throughput": 8217.047185336276,
    "itl": 39.7376564963158,
    "ttft": 129256.73406808221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0180308308079893,
    "arrivals": 64419,
    "finished_requests": 63143,
    "scheduler_time": 53.87786997086745
}
#Debug simulation 
Total elapsed time: 17.272434473969042. Arrivals time: 0.17220408795401454 Scheduler time: 16.743051829282194 Scheduler overhead time: 0.14431617502123117 Adapter cache time: 0.022146122995764017 Engine time: 0.13300647400319576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 16.31833796389401,
    "estimated_duration": 3600.013934884194,
    "input_throughput": 4386.448576485295,
    "output_throughput": 3855.5820202530685,
    "total_throughput": 8242.030596738365,
    "itl": 38.51195315880927,
    "ttft": 115946.37935539837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0483765981602504,
    "arrivals": 64419,
    "finished_requests": 63310,
    "scheduler_time": 52.66348852813763
}
#Debug simulation 
Total elapsed time: 16.318417402915657. Arrivals time: 0.17082163458690047 Scheduler time: 15.794587044976652 Scheduler overhead time: 0.14185543404892087 Adapter cache time: 0.022354045417159796 Engine time: 0.1313915904611349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 17.117146758828312,
    "estimated_duration": 3599.994363585313,
    "input_throughput": 4372.670735051543,
    "output_throughput": 3844.411852416508,
    "total_throughput": 8217.08258746805,
    "itl": 39.737184352360025,
    "ttft": 129250.29288036273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0316122397407943,
    "arrivals": 64419,
    "finished_requests": 63143,
    "scheduler_time": 53.875780165090355
}
#Debug simulation 
Total elapsed time: 17.11724174208939. Arrivals time: 0.16744618769735098 Scheduler time: 16.606322657782584 Scheduler overhead time: 0.13625963917002082 Adapter cache time: 0.021442636381834745 Engine time: 0.12919985875487328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 16.193238169886172,
    "estimated_duration": 3600.0146757083216,
    "input_throughput": 4386.447673825936,
    "output_throughput": 3855.5812268373625,
    "total_throughput": 8242.0289006633,
    "itl": 38.511089737462946,
    "ttft": 115946.36935455691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0046568990871323,
    "arrivals": 64419,
    "finished_requests": 63310,
    "scheduler_time": 52.66344761540227
}
#Debug simulation 
Total elapsed time: 16.19335092185065. Arrivals time: 0.1644861283712089 Scheduler time: 15.685982493218035 Scheduler overhead time: 0.13502022624015808 Adapter cache time: 0.02190483035519719 Engine time: 0.1294845324009657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_64_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 8640, 8640, 33, 8640, 33, 66, 33, 8640, 66, 8640, 33, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 8640, 33, 33, 33, 8640, 66, 33, 8640, 8640, 8640, 8640, 66, 66, 66, 8640, 33, 66, 66, 8640, 8640, 66, 8640, 33, 8640, 66, 66, 66, 8640, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 8640, 66]
Prompts retrieved: 192159 . Total input tokens: 43010186 . Total output tokens: 38352570
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.143093064893037,
    "estimated_duration": 3599.984750390768,
    "input_throughput": 4372.682411582798,
    "output_throughput": 3844.4221183152854,
    "total_throughput": 8217.104529898083,
    "itl": 39.7399848775613,
    "ttft": 129256.06028132416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0446906335279396,
    "arrivals": 64419,
    "finished_requests": 63143,
    "scheduler_time": 53.87748930766152
}
#Debug simulation 
Total elapsed time: 17.143194642849267. Arrivals time: 0.16612663678824902 Scheduler time: 16.628074099309742 Scheduler overhead time: 0.14004895417019725 Adapter cache time: 0.022048473358154297 Engine time: 0.12993410043418407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.089084346778691,
    "estimated_duration": 3600.0123731335525,
    "input_throughput": 2976.0633824361967,
    "output_throughput": 2606.6629853918766,
    "total_throughput": 5582.726367828073,
    "itl": 27.61590964871703,
    "ttft": 54973.30435365488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2083,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.374995970612112,
    "arrivals": 43251,
    "finished_requests": 42884,
    "scheduler_time": 19.954726158081915
}
#Debug simulation 
Total elapsed time: 5.089159157592803. Arrivals time: 0.11530890921130776 Scheduler time: 4.590497645083815 Scheduler overhead time: 0.1499404371716082 Adapter cache time: 0.03206910192966461 Engine time: 0.1396557968109846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.125887797214091,
    "estimated_duration": 3600.0242434059264,
    "input_throughput": 2977.0791181840173,
    "output_throughput": 2608.0441033689244,
    "total_throughput": 5585.123221552942,
    "itl": 27.562414187850738,
    "ttft": 54484.856280868626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2089,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.8231516556881635,
    "arrivals": 43251,
    "finished_requests": 42893,
    "scheduler_time": 20.018904452046232
}
#Debug simulation 
Total elapsed time: 5.125956024043262. Arrivals time: 0.11537504382431507 Scheduler time: 4.624371519777924 Scheduler overhead time: 0.15219460614025593 Adapter cache time: 0.03198188962414861 Engine time: 0.14012117963284254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.137249314691871,
    "estimated_duration": 3600.0146225659214,
    "input_throughput": 2975.823468284768,
    "output_throughput": 2607.8946849689037,
    "total_throughput": 5583.7181532536715,
    "itl": 27.558170203538882,
    "ttft": 53524.76442669232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2083,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.814156986791604,
    "arrivals": 43251,
    "finished_requests": 42904,
    "scheduler_time": 20.051748163403587
}
#Debug simulation 
Total elapsed time: 5.137317527085543. Arrivals time: 0.11601515440270305 Scheduler time: 4.635490659158677 Scheduler overhead time: 0.15056094666942954 Adapter cache time: 0.03222783049568534 Engine time: 0.14118517795577645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.108104230836034,
    "estimated_duration": 3600.032102112901,
    "input_throughput": 2979.8700944093885,
    "output_throughput": 2610.32172309926,
    "total_throughput": 5590.1918175086485,
    "itl": 27.58053060656481,
    "ttft": 50982.995122811044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2084,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.518915918757926,
    "arrivals": 43251,
    "finished_requests": 42936,
    "scheduler_time": 20.074024942456166
}
#Debug simulation 
Total elapsed time: 5.108182643074542. Arrivals time: 0.11571330903097987 Scheduler time: 4.60866488981992 Scheduler overhead time: 0.15050333319231868 Adapter cache time: 0.03220642218366265 Engine time: 0.13966168882325292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.069746379740536,
    "estimated_duration": 3600.0309791407176,
    "input_throughput": 2978.1207612160733,
    "output_throughput": 2610.2147604971196,
    "total_throughput": 5588.335521713193,
    "itl": 27.54631478060082,
    "ttft": 51314.29187185305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.004153235796706,
    "arrivals": 43251,
    "finished_requests": 42928,
    "scheduler_time": 19.979852252994238
}
#Debug simulation 
Total elapsed time: 5.069843881763518. Arrivals time: 0.11495456611737609 Scheduler time: 4.57192249270156 Scheduler overhead time: 0.14964124793186784 Adapter cache time: 0.03228208050131798 Engine time: 0.13963299989700317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.116578321438283,
    "estimated_duration": 3600.0284917331805,
    "input_throughput": 2977.383102554185,
    "output_throughput": 2609.633235285045,
    "total_throughput": 5587.01633783923,
    "itl": 27.545390819891736,
    "ttft": 54009.29999361784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.234254864870851,
    "arrivals": 43251,
    "finished_requests": 42898,
    "scheduler_time": 20.004244838892134
}
#Debug simulation 
Total elapsed time: 5.116643687244505. Arrivals time: 0.11525845155119896 Scheduler time: 4.616652153432369 Scheduler overhead time: 0.15269088139757514 Adapter cache time: 0.03202592954039574 Engine time: 0.13803000980988145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 4320, 1080, 4320, 540, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 540, 540, 4320, 1080, 540, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 540, 1080, 1080, 4320, 4320, 1080, 4320, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 4320, 1080]
Prompts retrieved: 129060 . Total input tokens: 28842701 . Total output tokens: 25798776
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.117021259386092,
    "estimated_duration": 3600.0219697752764,
    "input_throughput": 2977.980992896336,
    "output_throughput": 2609.772128859113,
    "total_throughput": 5587.753121755449,
    "itl": 27.62001856190725,
    "ttft": 52380.49829727527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2084,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.994599084406859,
    "arrivals": 43251,
    "finished_requests": 42916,
    "scheduler_time": 20.002888353775056
}
#Debug simulation 
Total elapsed time: 5.117090270388871. Arrivals time: 0.11493499670177698 Scheduler time: 4.61519191134721 Scheduler overhead time: 0.15075082844123244 Adapter cache time: 0.03368975268676877 Engine time: 0.14005768951028585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.687511866912246,
    "estimated_duration": 3599.9610753114926,
    "input_throughput": 2845.621601370679,
    "output_throughput": 2503.547624947629,
    "total_throughput": 5349.169226318308,
    "itl": 27.081618626559383,
    "ttft": 45659.88763945739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6228954778706735,
    "arrivals": 41341,
    "finished_requests": 41057,
    "scheduler_time": 17.04384745882312
}
#Debug simulation 
Total elapsed time: 4.6875767898745835. Arrivals time: 0.10840668994933367 Scheduler time: 4.201075717341155 Scheduler overhead time: 0.1460900604724884 Adapter cache time: 0.03165324777364731 Engine time: 0.13825293630361557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.695215597283095,
    "estimated_duration": 3599.968159262076,
    "input_throughput": 2845.1179418485417,
    "output_throughput": 2502.726580183649,
    "total_throughput": 5347.84452203219,
    "itl": 27.04683229561089,
    "ttft": 46463.95889309705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.033440142275959,
    "arrivals": 41341,
    "finished_requests": 41048,
    "scheduler_time": 17.044020895283357
}
#Debug simulation 
Total elapsed time: 4.695283734239638. Arrivals time: 0.10843292623758316 Scheduler time: 4.210675216279924 Scheduler overhead time: 0.14539536787196994 Adapter cache time: 0.03165943594649434 Engine time: 0.1372782438993454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.695859637111425,
    "estimated_duration": 3599.9663036605248,
    "input_throughput": 2844.2152332339,
    "output_throughput": 2501.9028624911634,
    "total_throughput": 5346.1180957250635,
    "itl": 27.076768435142004,
    "ttft": 47679.13295082741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.061989546529917,
    "arrivals": 41341,
    "finished_requests": 41034,
    "scheduler_time": 17.03205319217688
}
#Debug simulation 
Total elapsed time: 4.695922089274973. Arrivals time: 0.1090309452265501 Scheduler time: 4.208523902110755 Scheduler overhead time: 0.14708078047260642 Adapter cache time: 0.032168764155358076 Engine time: 0.137194087728858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.694548805244267,
    "estimated_duration": 3599.966965724692,
    "input_throughput": 2845.569444812347,
    "output_throughput": 2503.278248328563,
    "total_throughput": 5348.84769314091,
    "itl": 27.091171140224713,
    "ttft": 46262.01437414603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.71612935515564,
    "arrivals": 41341,
    "finished_requests": 41050,
    "scheduler_time": 17.03779610123453
}
#Debug simulation 
Total elapsed time: 4.694615202024579. Arrivals time: 0.10874120937660336 Scheduler time: 4.20493633300066 Scheduler overhead time: 0.14799730153754354 Adapter cache time: 0.031759914476424456 Engine time: 0.13913835166022182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.702296961098909,
    "estimated_duration": 3599.9723416927427,
    "input_throughput": 2844.725188967594,
    "output_throughput": 2502.796728644701,
    "total_throughput": 5347.5219176122955,
    "itl": 27.049240500239062,
    "ttft": 46924.422135721696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.10290683561922,
    "arrivals": 41341,
    "finished_requests": 41045,
    "scheduler_time": 17.088452906955037
}
#Debug simulation 
Total elapsed time: 4.702366751153022. Arrivals time: 0.10866891406476498 Scheduler time: 4.214126206934452 Scheduler overhead time: 0.14740691147744656 Adapter cache time: 0.03214882733300328 Engine time: 0.13766856724396348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 4320, 1080, 4320, 270, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 270, 270, 4320, 1080, 270, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 270, 1080, 1080, 4320, 4320, 1080, 4320, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 4320, 1080]
Prompts retrieved: 123390 . Total input tokens: 27563012 . Total output tokens: 24672264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.699565661139786,
    "estimated_duration": 3599.975084539683,
    "input_throughput": 2844.891356049358,
    "output_throughput": 2503.286769595047,
    "total_throughput": 5348.178125644405,
    "itl": 27.056761695596524,
    "ttft": 47074.170930336935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.428608134039486,
    "arrivals": 41341,
    "finished_requests": 41043,
    "scheduler_time": 17.084418217826506
}
#Debug simulation 
Total elapsed time: 4.699632870033383. Arrivals time: 0.10929360007867217 Scheduler time: 4.209152347408235 Scheduler overhead time: 0.14806512603536248 Adapter cache time: 0.03180441074073315 Engine time: 0.13898554258048534 
