INFO 06-01 00:47:07 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:08 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.3338459329679608,
    "estimated_duration": 3599.8630059260795,
    "input_throughput": 797.1761690030622,
    "output_throughput": 700.3599847687568,
    "total_throughput": 1497.536153771819,
    "itl": 21.309484873869724,
    "ttft": 5911.149274373765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2923,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.142383400662757,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3339448240585625. Arrivals time: 0.04215424507856369 Scheduler time: 0.863511567004025 Scheduler overhead time: 0.14192419964820147 Adapter cache time: 0.07499403972178698 Engine time: 0.14242869149893522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.3052692720666528,
    "estimated_duration": 3599.8721657136157,
    "input_throughput": 797.1741406076079,
    "output_throughput": 700.3582027197383,
    "total_throughput": 1497.5323433273463,
    "itl": 21.31427029828741,
    "ttft": 5911.054435297498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.696348914727155,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3053982411511242. Arrivals time: 0.04154877085238695 Scheduler time: 0.840298661030829 Scheduler overhead time: 0.14209265494719148 Adapter cache time: 0.07416049623861909 Engine time: 0.13849666295573115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.305174207314849,
    "estimated_duration": 3599.8758609163697,
    "input_throughput": 797.1733223238133,
    "output_throughput": 700.3574838156262,
    "total_throughput": 1497.5308061394396,
    "itl": 21.3079015874382,
    "ttft": 5910.995819021997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.736926961704102,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.305307765956968. Arrivals time: 0.041201990097761154 Scheduler time: 0.8397467699833214 Scheduler overhead time: 0.14511038037016988 Adapter cache time: 0.0731045389547944 Engine time: 0.13693232415243983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.2985907839611173,
    "estimated_duration": 3599.8753655384216,
    "input_throughput": 797.1734320226346,
    "output_throughput": 700.3575801916443,
    "total_throughput": 1497.531012214279,
    "itl": 21.31584486776216,
    "ttft": 5911.095358881761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.82009064055929,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2986733838915825. Arrivals time: 0.041566697880625725 Scheduler time: 0.8362831133417785 Scheduler overhead time: 0.14083280693739653 Adapter cache time: 0.07352836895734072 Engine time: 0.1377095254138112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.1456669382750988,
    "estimated_duration": 3599.9818639873083,
    "input_throughput": 573.1992765414873,
    "output_throughput": 518.3659447475981,
    "total_throughput": 1091.5652212890852,
    "itl": 20.068521743110352,
    "ttft": 4664.7497634977035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.484508999766554,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1457497500814497. Arrivals time: 0.03556845989078283 Scheduler time: 0.6802116646431386 Scheduler overhead time: 0.1459185667335987 Adapter cache time: 0.06758327782154083 Engine time: 0.1449942858889699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.14174336893484,
    "estimated_duration": 3599.9832645328975,
    "input_throughput": 573.199053542751,
    "output_throughput": 518.3657430813446,
    "total_throughput": 1091.5647966240956,
    "itl": 20.259998003210093,
    "ttft": 4665.21193689944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.46641596871975,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1417957697995007. Arrivals time: 0.034877505619078875 Scheduler time: 0.6768548008985817 Scheduler overhead time: 0.14436662290245295 Adapter cache time: 0.06893533654510975 Engine time: 0.14608213678002357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.147409840952605,
    "estimated_duration": 3599.982732087806,
    "input_throughput": 573.1991383201083,
    "output_throughput": 518.3658197487388,
    "total_throughput": 1091.564958068847,
    "itl": 20.25831070541865,
    "ttft": 4665.24473505084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.485054888202592,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1475167209282517. Arrivals time: 0.035259200260043144 Scheduler time: 0.6806690357625484 Scheduler overhead time: 0.1458299132063985 Adapter cache time: 0.06861967360600829 Engine time: 0.14610137045383453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.130738282110542,
    "estimated_duration": 3599.9910281740094,
    "input_throughput": 573.197817397521,
    "output_throughput": 518.3646251881158,
    "total_throughput": 1091.562442585637,
    "itl": 20.069452961826254,
    "ttft": 4664.770605496052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.77711840629208,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1308222222141922. Arrivals time: 0.03544916585087776 Scheduler time: 0.6662670564837754 Scheduler overhead time: 0.1456874804571271 Adapter cache time: 0.06751663330942392 Engine time: 0.14430180611088872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.1422304096631706,
    "estimated_duration": 3599.992346473432,
    "input_throughput": 573.1976074953104,
    "output_throughput": 518.3644353655494,
    "total_throughput": 1091.56204286086,
    "itl": 20.075731163688666,
    "ttft": 4664.859133615765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.604986762142603,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.142297268845141. Arrivals time: 0.03526687994599342 Scheduler time: 0.6772845671512187 Scheduler overhead time: 0.14508780278265476 Adapter cache time: 0.06826472422108054 Engine time: 0.1448909305036068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.153645167592913,
    "estimated_duration": 3599.978051729885,
    "input_throughput": 573.1998835405205,
    "output_throughput": 518.3664936799505,
    "total_throughput": 1091.566377220471,
    "itl": 20.06652633181526,
    "ttft": 4664.721662591369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.180141699929896,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1537326877005398. Arrivals time: 0.03521064389497042 Scheduler time: 0.6854878603480756 Scheduler overhead time: 0.14682918693870306 Adapter cache time: 0.06858653156086802 Engine time: 0.14592429623007774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.131336959078908,
    "estimated_duration": 3599.9878452960384,
    "input_throughput": 573.1983241822061,
    "output_throughput": 518.365083492815,
    "total_throughput": 1091.563407675021,
    "itl": 20.07698181791336,
    "ttft": 4665.00800191699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.791605381182325,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1314098984003067. Arrivals time: 0.03511119820177555 Scheduler time: 0.667220258153975 Scheduler overhead time: 0.1454269285313785 Adapter cache time: 0.06768951006233692 Engine time: 0.14413533126935363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.087250592187047,
    "estimated_duration": 3598.508912021079,
    "input_throughput": 538.75259097556,
    "output_throughput": 483.0067237554249,
    "total_throughput": 1021.7593147309849,
    "itl": 19.852183605350973,
    "ttft": 2751.578860117081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.01163490266952,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0873287580907345. Arrivals time: 0.033890456426888704 Scheduler time: 0.6263969256542623 Scheduler overhead time: 0.14662252413108945 Adapter cache time: 0.06363588804379106 Engine time: 0.14428041223436594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0985495503991842,
    "estimated_duration": 3598.5245946326518,
    "input_throughput": 538.750243055629,
    "output_throughput": 483.00461877972265,
    "total_throughput": 1021.7548618353517,
    "itl": 19.85781839707124,
    "ttft": 2751.788072147521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.761711063147832,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0986139783635736. Arrivals time: 0.033754243049770594 Scheduler time: 0.6324675716459751 Scheduler overhead time: 0.14804401202127337 Adapter cache time: 0.06369226053357124 Engine time: 0.14805931504815817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.085182635113597,
    "estimated_duration": 3598.516786405745,
    "input_throughput": 538.7514120606368,
    "output_throughput": 483.005666825316,
    "total_throughput": 1021.7570788859528,
    "itl": 19.856690230899538,
    "ttft": 2751.59519750828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.775479898098181,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0854751230217516. Arrivals time: 0.033476586919277906 Scheduler time: 0.6248260894790292 Scheduler overhead time: 0.14620991935953498 Adapter cache time: 0.06385932071134448 Engine time: 0.14521521190181375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.085458436049521,
    "estimated_duration": 3598.5191348904673,
    "input_throughput": 538.751060457821,
    "output_throughput": 483.0053516035854,
    "total_throughput": 1021.7564120614064,
    "itl": 19.853789661377743,
    "ttft": 2751.6065676711637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.26690213251667,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0855378927662969. Arrivals time: 0.03391098929569125 Scheduler time: 0.6245753150433302 Scheduler overhead time: 0.14644560310989618 Adapter cache time: 0.06342716654762626 Engine time: 0.1450145631097257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.0862700529396534,
    "estimated_duration": 3598.5238648662885,
    "input_throughput": 538.750352312041,
    "output_throughput": 483.0047167311431,
    "total_throughput": 1021.755069043184,
    "itl": 19.85767410015742,
    "ttft": 2751.662852055381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.93114849978988,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0863475729711354. Arrivals time: 0.034425859339535236 Scheduler time: 0.6265955562703311 Scheduler overhead time: 0.14595124404877424 Adapter cache time: 0.06384884938597679 Engine time: 0.14388394681736827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.1039742659777403,
    "estimated_duration": 3598.5243565501364,
    "input_throughput": 538.7502786999655,
    "output_throughput": 483.00465073586446,
    "total_throughput": 1021.7549294358299,
    "itl": 19.84908805199749,
    "ttft": 2751.502608234388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.761191011353404,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1040414976887405. Arrivals time: 0.033938553649932146 Scheduler time: 0.6355063067749143 Scheduler overhead time: 0.1529122944921255 Adapter cache time: 0.06469865469262004 Engine time: 0.14291616156697273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0924418130889535,
    "estimated_duration": 3598.509816605602,
    "input_throughput": 538.7524555452625,
    "output_throughput": 483.0066023383859,
    "total_throughput": 1021.7590578836483,
    "itl": 19.85897290546036,
    "ttft": 2751.672010743614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.087711963876352,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0925224968232214. Arrivals time: 0.03374953009188175 Scheduler time: 0.6312454584985971 Scheduler overhead time: 0.14631243702024221 Adapter cache time: 0.06354095786809921 Engine time: 0.14595734560862184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.0004899520426989,
    "estimated_duration": 3597.5161329299567,
    "input_throughput": 445.2680518475907,
    "output_throughput": 404.2402997691614,
    "total_throughput": 849.5083516167521,
    "itl": 19.326700139450452,
    "ttft": 7529.81400025357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.889937403858878,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0005514468066394. Arrivals time: 0.031074875965714455 Scheduler time: 0.5452422094531357 Scheduler overhead time: 0.1478168093599379 Adapter cache time: 0.056424014270305634 Engine time: 0.14675266714766622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0108564621768892,
    "estimated_duration": 3597.5217845785687,
    "input_throughput": 445.2673523386738,
    "output_throughput": 404.23966471418026,
    "total_throughput": 849.507017052854,
    "itl": 19.33016653591742,
    "ttft": 7529.795883356006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.42584303943878,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0109137021936476. Arrivals time: 0.031230324413627386 Scheduler time: 0.5524950842373073 Scheduler overhead time: 0.1478376849554479 Adapter cache time: 0.05711476178839803 Engine time: 0.14871686976402998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0027450639754534,
    "estimated_duration": 3597.5011016424273,
    "input_throughput": 445.2699122923622,
    "output_throughput": 404.24198878940217,
    "total_throughput": 849.5119010817643,
    "itl": 19.33084361914704,
    "ttft": 7529.861508481698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.438398192524806,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0028002341277897. Arrivals time: 0.030485495924949646 Scheduler time: 0.5454289293847978 Scheduler overhead time: 0.1489098072052002 Adapter cache time: 0.0565057429485023 Engine time: 0.14767573960125446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.005768849980086,
    "estimated_duration": 3597.5063024271108,
    "input_throughput": 445.2692685817624,
    "output_throughput": 404.2414043913867,
    "total_throughput": 849.5106729731491,
    "itl": 19.32712040006338,
    "ttft": 7529.79483020228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.06926920950617,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0058387769386172. Arrivals time: 0.03109281836077571 Scheduler time: 0.5488002016209066 Scheduler overhead time: 0.14864638540893793 Adapter cache time: 0.05646230885758996 Engine time: 0.14734934642910957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.0101007940247655,
    "estimated_duration": 3597.5000212140963,
    "input_throughput": 445.2700460191795,
    "output_throughput": 404.2421101944042,
    "total_throughput": 849.5121562135837,
    "itl": 19.331233562283803,
    "ttft": 7529.930901120603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.54918727835615,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0101737212389708. Arrivals time: 0.030709162820130587 Scheduler time: 0.5510820960626006 Scheduler overhead time: 0.14842192316427827 Adapter cache time: 0.05706411553546786 Engine time: 0.14900212502107024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.0077915131114423,
    "estimated_duration": 3597.507282537246,
    "input_throughput": 445.26914727195293,
    "output_throughput": 404.24129425926844,
    "total_throughput": 849.5104415312213,
    "itl": 19.32525841929838,
    "ttft": 7529.82450972221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.708349660257571,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0078578633256257. Arrivals time: 0.030891877133399248 Scheduler time: 0.5491339955478907 Scheduler overhead time: 0.1489910832606256 Adapter cache time: 0.05696340696886182 Engine time: 0.14809769531711936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.002315275836736,
    "estimated_duration": 3597.5001980474563,
    "input_throughput": 445.27002413214854,
    "output_throughput": 404.24209032408123,
    "total_throughput": 849.5121144562298,
    "itl": 19.331060691351333,
    "ttft": 7529.86470576231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.662728361598752,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0023906538262963. Arrivals time: 0.031146446242928505 Scheduler time: 0.5447517721913755 Scheduler overhead time: 0.14775317069143057 Adapter cache time: 0.05712200654670596 Engine time: 0.14850079827010632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 0.8369299899786711,
    "estimated_duration": 3599.6371203524786,
    "input_throughput": 293.24039193612913,
    "output_throughput": 249.43292059175127,
    "total_throughput": 542.6733125278804,
    "itl": 18.534365679619235,
    "ttft": 4279.451624042895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6228954778706735,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8369948430918157. Arrivals time: 0.025696586351841688 Scheduler time: 0.38756576273590326 Scheduler overhead time: 0.15107984049245715 Adapter cache time: 0.04579204507172108 Engine time: 0.15200101118534803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.8356491839513183,
    "estimated_duration": 3599.6429248252543,
    "input_throughput": 293.2399190820413,
    "output_throughput": 249.43251837780196,
    "total_throughput": 542.6724374598432,
    "itl": 18.537843193251987,
    "ttft": 4279.508972562718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.069060979683143,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.835717816837132. Arrivals time: 0.025069762486964464 Scheduler time: 0.39060501800850034 Scheduler overhead time: 0.14942208491265774 Adapter cache time: 0.046055447310209274 Engine time: 0.14940792135894299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.838139059022069,
    "estimated_duration": 3599.643338134881,
    "input_throughput": 293.23988541234957,
    "output_throughput": 249.43248973805868,
    "total_throughput": 542.6723751504082,
    "itl": 18.53788950661487,
    "ttft": 4279.482150844381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.080242368224975,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8382454947568476. Arrivals time: 0.024991745129227638 Scheduler time: 0.3895939812064171 Scheduler overhead time: 0.1515744044445455 Adapter cache time: 0.045599720906466246 Engine time: 0.15109806507825851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 0.8358946861699224,
    "estimated_duration": 3599.646294635754,
    "input_throughput": 293.23964456535896,
    "output_throughput": 249.43228487143753,
    "total_throughput": 542.6719294367965,
    "itl": 18.53562015291208,
    "ttft": 4279.49615636836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.765883253400454,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8359987959265709. Arrivals time: 0.025453016627579927 Scheduler time: 0.38737435219809413 Scheduler overhead time: 0.15041592437773943 Adapter cache time: 0.04600298311561346 Engine time: 0.15166680980473757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 0.8355210279114544,
    "estimated_duration": 3599.6485303304685,
    "input_throughput": 293.2394624380435,
    "output_throughput": 249.43212995230135,
    "total_throughput": 542.6715923903449,
    "itl": 18.53735984344729,
    "ttft": 4279.507456440659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.173551677744689,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8356056748889387. Arrivals time: 0.02599594835191965 Scheduler time: 0.3876768434420228 Scheduler overhead time: 0.14925609063357115 Adapter cache time: 0.045548003166913986 Engine time: 0.15195183735340834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 0.8352322811260819,
    "estimated_duration": 3599.64866521284,
    "input_throughput": 293.2394514500728,
    "output_throughput": 249.43212060583443,
    "total_throughput": 542.6715720559072,
    "itl": 18.533505563000155,
    "ttft": 4279.306303059945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.470468838168115,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.835326240863651. Arrivals time: 0.025368972681462765 Scheduler time: 0.38845327822491527 Scheduler overhead time: 0.15061133168637753 Adapter cache time: 0.04577691853046417 Engine time: 0.14960544416680932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.8430116288363934,
    "estimated_duration": 3599.654247948452,
    "input_throughput": 293.23899666241385,
    "output_throughput": 249.43173375935237,
    "total_throughput": 542.6707304217663,
    "itl": 18.53940369013227,
    "ttft": 4279.635244156723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.264471665322523,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8430609120987356. Arrivals time: 0.02506115334108472 Scheduler time: 0.39440624648705125 Scheduler overhead time: 0.15033292584121227 Adapter cache time: 0.046129303984344006 Engine time: 0.15196805261075497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 26.06263754889369,
    "estimated_duration": 3600.1396861105004,
    "input_throughput": 5486.449338675396,
    "output_throughput": 4849.324893518631,
    "total_throughput": 10335.774232194026,
    "itl": 177.4718691546135,
    "ttft": 2125837.513947153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.465973629343804,
    "arrivals": 1078565,
    "finished_requests": 79601,
    "scheduler_time": 98.3189209476852
}
#Debug simulation 
Total elapsed time: 26.062782188877463. Arrivals time: 0.35335805686190724 Scheduler time: 25.590624613687396 Scheduler overhead time: 0.04422594187781215 Adapter cache time: 0.013395562302321196 Engine time: 0.044166273437440395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 28.892551416996866,
    "estimated_duration": 3600.055776014681,
    "input_throughput": 5491.341031911662,
    "output_throughput": 4850.90400997409,
    "total_throughput": 10342.245041885752,
    "itl": 177.49204410546747,
    "ttft": 2125340.8701568246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4456663779658314,
    "arrivals": 1078565,
    "finished_requests": 79666,
    "scheduler_time": 98.34661324235229
}
#Debug simulation 
Total elapsed time: 28.892707981169224. Arrivals time: 0.37995033618062735 Scheduler time: 28.39272744162008 Scheduler overhead time: 0.044179226737469435 Adapter cache time: 0.01353112468495965 Engine time: 0.045006852596998215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 26.115812093950808,
    "estimated_duration": 3600.050119150417,
    "input_throughput": 5468.446923912798,
    "output_throughput": 4828.349168678264,
    "total_throughput": 10296.796092591063,
    "itl": 175.26062960745247,
    "ttft": 2125666.6089350954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6367691114172438,
    "arrivals": 1078565,
    "finished_requests": 79347,
    "scheduler_time": 98.35302268736594
}
#Debug simulation 
Total elapsed time: 26.1160109359771. Arrivals time: 0.7348396782763302 Scheduler time: 25.262744596693665 Scheduler overhead time: 0.04329622210934758 Adapter cache time: 0.014009546022862196 Engine time: 0.04399177245795727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 28.86939534312114,
    "estimated_duration": 3600.158471944579,
    "input_throughput": 5491.421601038995,
    "output_throughput": 4851.082566531213,
    "total_throughput": 10342.504167570207,
    "itl": 177.48811785427887,
    "ttft": 2125360.3318853634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3851942708366511,
    "arrivals": 1078565,
    "finished_requests": 79670,
    "scheduler_time": 98.35181143490118
}
#Debug simulation 
Total elapsed time: 28.86953416094184. Arrivals time: 0.39006699761375785 Scheduler time: 28.357885950710624 Scheduler overhead time: 0.04516975115984678 Adapter cache time: 0.013726333156228065 Engine time: 0.04551704041659832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 25.72384403506294,
    "estimated_duration": 3600.071300426215,
    "input_throughput": 5468.414749916003,
    "output_throughput": 4828.320760742182,
    "total_throughput": 10296.735510658185,
    "itl": 175.26149199813673,
    "ttft": 2125674.3571271556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6578957475349347,
    "arrivals": 1078565,
    "finished_requests": 79347,
    "scheduler_time": 98.35307732706661
}
#Debug simulation 
Total elapsed time: 25.723957092966884. Arrivals time: 0.37411470664665103 Scheduler time: 25.231387995183468 Scheduler overhead time: 0.04374420316889882 Adapter cache time: 0.013783026952296495 Engine time: 0.0435505504719913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 26.08867913391441,
    "estimated_duration": 3600.1058595070335,
    "input_throughput": 5486.500889366809,
    "output_throughput": 4849.370457787199,
    "total_throughput": 10335.871347154009,
    "itl": 177.47046789990492,
    "ttft": 2125824.9258869966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4322340912581282,
    "arrivals": 1078565,
    "finished_requests": 79601,
    "scheduler_time": 98.31883388223983
}
#Debug simulation 
Total elapsed time: 26.088781054131687. Arrivals time: 0.3587311403825879 Scheduler time: 25.612130919937044 Scheduler overhead time: 0.04385835910215974 Adapter cache time: 0.013146649114787579 Engine time: 0.04388888506218791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 25.70997731667012,
    "estimated_duration": 3600.091969867594,
    "input_throughput": 5468.383353751945,
    "output_throughput": 4828.293039591234,
    "total_throughput": 10296.67639334318,
    "itl": 175.26232836605848,
    "ttft": 2125681.701057193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6785193685069661,
    "arrivals": 1078565,
    "finished_requests": 79347,
    "scheduler_time": 98.35312314749521
}
#Debug simulation 
Total elapsed time: 25.710098513867706. Arrivals time: 0.3559554354287684 Scheduler time: 25.23406953457743 Scheduler overhead time: 0.04430261440575123 Adapter cache time: 0.014165285974740982 Engine time: 0.04432686325162649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 21.386043164879084,
    "estimated_duration": 3600.021853735675,
    "input_throughput": 5398.665283054418,
    "output_throughput": 4779.117377342243,
    "total_throughput": 10177.782660396662,
    "itl": 180.11056375871314,
    "ttft": 2116584.97493169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.046686808425002,
    "arrivals": 1002554,
    "finished_requests": 78546,
    "scheduler_time": 96.79274725411081
}
#Debug simulation 
Total elapsed time: 21.38621011795476. Arrivals time: 0.538905733730644 Scheduler time: 20.73658065358177 Scheduler overhead time: 0.041532028932124376 Adapter cache time: 0.011351882014423609 Engine time: 0.04132653586566448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 20.565918354317546,
    "estimated_duration": 3600.1245309319925,
    "input_throughput": 5394.191737854317,
    "output_throughput": 4773.857085313324,
    "total_throughput": 10168.04882316764,
    "itl": 180.21094696341012,
    "ttft": 2116583.0596393934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.30728792764247,
    "arrivals": 1002554,
    "finished_requests": 78473,
    "scheduler_time": 96.73443593053786
}
#Debug simulation 
Total elapsed time: 20.56603305740282. Arrivals time: 0.3659810363315046 Scheduler time: 20.08915141923353 Scheduler overhead time: 0.04111147951334715 Adapter cache time: 0.012579529546201229 Engine time: 0.04112859722226858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 17.417007742915303,
    "estimated_duration": 3600.043545455578,
    "input_throughput": 5357.802136684739,
    "output_throughput": 4742.091250965579,
    "total_throughput": 10099.893387650318,
    "itl": 178.71882695372815,
    "ttft": 2119427.483701515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8484249838627989,
    "arrivals": 1002554,
    "finished_requests": 78000,
    "scheduler_time": 96.47482058122583
}
#Debug simulation 
Total elapsed time: 17.41710434574634. Arrivals time: 0.333917535841465 Scheduler time: 16.97420987766236 Scheduler overhead time: 0.039343226701021194 Adapter cache time: 0.014335918240249157 Engine time: 0.03928758064284921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 21.506660921964794,
    "estimated_duration": 3600.0440120538437,
    "input_throughput": 5398.632054198708,
    "output_throughput": 4779.087961812028,
    "total_throughput": 10177.720016010735,
    "itl": 180.11143107159495,
    "ttft": 2116595.512987365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0687684718403057,
    "arrivals": 1002554,
    "finished_requests": 78546,
    "scheduler_time": 96.79282390883652
}
#Debug simulation 
Total elapsed time: 21.5068815019913. Arrivals time: 0.350909823551774 Scheduler time: 21.042667523492128 Scheduler overhead time: 0.04263987438753247 Adapter cache time: 0.011371643282473087 Engine time: 0.04254024801775813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 17.48240840388462,
    "estimated_duration": 3600.066231731872,
    "input_throughput": 5357.768373811565,
    "output_throughput": 4742.061368073041,
    "total_throughput": 10099.829741884607,
    "itl": 178.719740133121,
    "ttft": 2119435.265676214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8710606654174673,
    "arrivals": 1002554,
    "finished_requests": 78000,
    "scheduler_time": 96.47487117599721
}
#Debug simulation 
Total elapsed time: 17.48253717692569. Arrivals time: 0.46114929812029004 Scheduler time: 16.91171755362302 Scheduler overhead time: 0.03982560941949487 Adapter cache time: 0.014181952457875013 Engine time: 0.03974341554567218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 20.18596459971741,
    "estimated_duration": 3600.165453861064,
    "input_throughput": 5393.955430346567,
    "output_throughput": 4776.608803230753,
    "total_throughput": 10170.564233577321,
    "itl": 180.157375041505,
    "ttft": 2116532.089846554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2408708723843958,
    "arrivals": 1002554,
    "finished_requests": 78499,
    "scheduler_time": 96.78646463860507
}
#Debug simulation 
Total elapsed time: 20.18607142660767. Arrivals time: 0.3469420555047691 Scheduler time: 19.73018552036956 Scheduler overhead time: 0.04031864134594798 Adapter cache time: 0.012807676568627357 Engine time: 0.039738970808684826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 17.57118377601728,
    "estimated_duration": 3600.0905533378727,
    "input_throughput": 5357.732177629969,
    "output_throughput": 4742.029331504373,
    "total_throughput": 10099.761509134341,
    "itl": 178.720734968691,
    "ttft": 2119443.3935027434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8953311461955271,
    "arrivals": 1002554,
    "finished_requests": 78000,
    "scheduler_time": 96.47492230125111
}
#Debug simulation 
Total elapsed time: 17.571302972733974. Arrivals time: 0.40201758313924074 Scheduler time: 17.05983336409554 Scheduler overhead time: 0.03902418399229646 Adapter cache time: 0.01464082533493638 Engine time: 0.039759838953614235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 14.886322468984872,
    "estimated_duration": 3600.112554722885,
    "input_throughput": 5371.8001051466035,
    "output_throughput": 4748.800138921202,
    "total_throughput": 10120.600244067806,
    "itl": 181.2799152310658,
    "ttft": 2113415.0630998793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7620614482392547,
    "arrivals": 945185,
    "finished_requests": 78192,
    "scheduler_time": 96.16909730688336
}
#Debug simulation 
Total elapsed time: 14.886436643078923. Arrivals time: 0.3283344297669828 Scheduler time: 14.459837232716382 Scheduler overhead time: 0.03691437467932701 Adapter cache time: 0.009329287800937891 Engine time: 0.03675065562129021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 14.996567466761917,
    "estimated_duration": 3600.1626761909074,
    "input_throughput": 5371.725318940698,
    "output_throughput": 4748.7340261213885,
    "total_throughput": 10120.459345062087,
    "itl": 181.2817463927723,
    "ttft": 2113441.443272925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8119407509733025,
    "arrivals": 945185,
    "finished_requests": 78192,
    "scheduler_time": 96.16933947215868
}
#Debug simulation 
Total elapsed time: 14.996678578667343. Arrivals time: 0.3388372049666941 Scheduler time: 14.558316697366536 Scheduler overhead time: 0.037262510508298874 Adapter cache time: 0.009287677239626646 Engine time: 0.03742743283510208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.854593338444829,
    "estimated_duration": 3600.0083248931996,
    "input_throughput": 5356.9603899658,
    "output_throughput": 4737.081823416595,
    "total_throughput": 10094.042213382394,
    "itl": 179.34286165217054,
    "ttft": 2113366.5114714983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8932714322023139,
    "arrivals": 945185,
    "finished_requests": 77945,
    "scheduler_time": 96.27804076888057
}
#Debug simulation 
Total elapsed time: 13.854690551292151. Arrivals time: 0.31448940420523286 Scheduler time: 13.442815817892551 Scheduler overhead time: 0.03578582778573036 Adapter cache time: 0.009692214895039797 Engine time: 0.03654905874282122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 14.945130926091224,
    "estimated_duration": 3600.1285944839005,
    "input_throughput": 5371.7761720043145,
    "output_throughput": 4748.778981449367,
    "total_throughput": 10120.555153453683,
    "itl": 181.28052567731268,
    "ttft": 2113423.164665286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7780273395427524,
    "arrivals": 945185,
    "finished_requests": 78192,
    "scheduler_time": 96.16917117657579
}
#Debug simulation 
Total elapsed time: 14.94527124799788. Arrivals time: 0.3386334595270455 Scheduler time: 14.50763167720288 Scheduler overhead time: 0.03730084374547005 Adapter cache time: 0.009181684348732233 Engine time: 0.03713647276163101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 13.979362769052386,
    "estimated_duration": 3600.019943112252,
    "input_throughput": 5356.9431016339995,
    "output_throughput": 4737.066535597315,
    "total_throughput": 10094.009637231315,
    "itl": 179.34327468002556,
    "ttft": 2113372.7002514065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.904840780552481,
    "arrivals": 945185,
    "finished_requests": 77945,
    "scheduler_time": 96.27808963958373
}
#Debug simulation 
Total elapsed time: 13.979534068144858. Arrivals time: 0.31824866496026516 Scheduler time: 13.562618578784168 Scheduler overhead time: 0.03687003580853343 Adapter cache time: 0.009794574230909348 Engine time: 0.03649719525128603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 14.97582110017538,
    "estimated_duration": 3600.0949325786555,
    "input_throughput": 5371.826399629943,
    "output_throughput": 4748.823383875164,
    "total_throughput": 10120.649783505107,
    "itl": 181.2792594955169,
    "ttft": 2113406.087054735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7445225234306427,
    "arrivals": 945185,
    "finished_requests": 78192,
    "scheduler_time": 96.16901408743634
}
#Debug simulation 
Total elapsed time: 14.975949581246823. Arrivals time: 0.39754276210442185 Scheduler time: 14.47971157450229 Scheduler overhead time: 0.036581601947546005 Adapter cache time: 0.009388459380716085 Engine time: 0.03714327933266759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 14.00436230096966,
    "estimated_duration": 3600.031686024124,
    "input_throughput": 5356.925627868146,
    "output_throughput": 4737.051083801412,
    "total_throughput": 10093.976711669557,
    "itl": 179.34371665275506,
    "ttft": 2113378.6896431455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9165358826890634,
    "arrivals": 945185,
    "finished_requests": 77945,
    "scheduler_time": 96.27813744932182
}
#Debug simulation 
Total elapsed time: 14.004477621987462. Arrivals time: 0.335023395717144 Scheduler time: 13.570851024240255 Scheduler overhead time: 0.036747457925230265 Adapter cache time: 0.009716465137898922 Engine time: 0.03650739835575223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 13.818708220031112,
    "estimated_duration": 3600.1386141037215,
    "input_throughput": 5396.771369825995,
    "output_throughput": 4746.231140395672,
    "total_throughput": 10143.002510221668,
    "itl": 180.62562584990334,
    "ttft": 2111409.7903325353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8508155928133044,
    "arrivals": 935737,
    "finished_requests": 78554,
    "scheduler_time": 96.17363007994855
}
#Debug simulation 
Total elapsed time: 13.81877793604508. Arrivals time: 0.6568265724927187 Scheduler time: 13.065875705331564 Scheduler overhead time: 0.03549579996615648 Adapter cache time: 0.009592972230166197 Engine time: 0.035784715320914984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.469375398010015,
    "estimated_duration": 3600.0222473053445,
    "input_throughput": 5393.811945060746,
    "output_throughput": 4745.084565181887,
    "total_throughput": 10138.896510242634,
    "itl": 180.65437307174832,
    "ttft": 2111402.1146198777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9238173439609866,
    "arrivals": 935737,
    "finished_requests": 78536,
    "scheduler_time": 96.16693888940006
}
#Debug simulation 
Total elapsed time: 13.469498705118895. Arrivals time: 0.3128924136981368 Scheduler time: 13.059778499417007 Scheduler overhead time: 0.03602250013500452 Adapter cache time: 0.009626777842640877 Engine time: 0.03576697921380401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.807832885999233,
    "estimated_duration": 3600.058725140438,
    "input_throughput": 5385.086322235959,
    "output_throughput": 4737.7196602076665,
    "total_throughput": 10122.805982443626,
    "itl": 179.11841649420387,
    "ttft": 2112638.8064410873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9551452602632393,
    "arrivals": 935737,
    "finished_requests": 78394,
    "scheduler_time": 96.26596336965034
}
#Debug simulation 
Total elapsed time: 13.807928856927902. Arrivals time: 0.3183632083237171 Scheduler time: 13.39118369948119 Scheduler overhead time: 0.036691907327622175 Adapter cache time: 0.009731105994433165 Engine time: 0.036340035032480955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 13.442965817637742,
    "estimated_duration": 3600.181972248536,
    "input_throughput": 5393.827631404916,
    "output_throughput": 4745.265692592227,
    "total_throughput": 10139.093323997144,
    "itl": 180.65071043190926,
    "ttft": 2111544.6144920327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8833664074353901,
    "arrivals": 935737,
    "finished_requests": 78543,
    "scheduler_time": 96.17215690836977
}
#Debug simulation 
Total elapsed time: 13.443087491672486. Arrivals time: 0.31448360346257687 Scheduler time: 13.031832124572247 Scheduler overhead time: 0.03584810672327876 Adapter cache time: 0.009571683127433062 Engine time: 0.03595243487507105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 13.533578631933779,
    "estimated_duration": 3600.071604677029,
    "input_throughput": 5385.067056670175,
    "output_throughput": 4737.70271064654,
    "total_throughput": 10122.769767316715,
    "itl": 179.1189067762155,
    "ttft": 2112644.7929640207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9679721464775553,
    "arrivals": 935737,
    "finished_requests": 78394,
    "scheduler_time": 96.26601602004152
}
#Debug simulation 
Total elapsed time: 13.533687369897962. Arrivals time: 0.3324491158127785 Scheduler time: 13.103471991140395 Scheduler overhead time: 0.0366078563965857 Adapter cache time: 0.009596556890755892 Engine time: 0.03601251868531108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 13.49794106790796,
    "estimated_duration": 3600.118952787924,
    "input_throughput": 5396.800843192731,
    "output_throughput": 4746.257060969554,
    "total_throughput": 10143.057904162284,
    "itl": 180.62488722888654,
    "ttft": 2111400.752631098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.831233981982806,
    "arrivals": 935737,
    "finished_requests": 78554,
    "scheduler_time": 96.17355037494829
}
#Debug simulation 
Total elapsed time: 13.498095823917538. Arrivals time: 0.3115409919992089 Scheduler time: 13.089236893691123 Scheduler overhead time: 0.03627535654231906 Adapter cache time: 0.009764764923602343 Engine time: 0.035928495693951845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.526110246311873,
    "estimated_duration": 3600.083603937115,
    "input_throughput": 5385.0491079702815,
    "output_throughput": 4737.686919644639,
    "total_throughput": 10122.736027614921,
    "itl": 179.11934923675565,
    "ttft": 2112650.1012457763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9799187561869673,
    "arrivals": 935737,
    "finished_requests": 78394,
    "scheduler_time": 96.26606867043272
}
#Debug simulation 
Total elapsed time: 13.526248867157847. Arrivals time: 0.32689270935952663 Scheduler time: 13.101672747638077 Scheduler overhead time: 0.036268227733671665 Adapter cache time: 0.009623559657484293 Engine time: 0.03624138981103897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.905457907821983,
    "estimated_duration": 3600.1386414108247,
    "input_throughput": 5376.044071573684,
    "output_throughput": 4747.213288792265,
    "total_throughput": 10123.25736036595,
    "itl": 180.95329015888223,
    "ttft": 2112555.3153372603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7314565707999272,
    "arrivals": 931056,
    "finished_requests": 78175,
    "scheduler_time": 96.1600760111316
}
#Debug simulation 
Total elapsed time: 12.905555645935237. Arrivals time: 0.31054804427549243 Scheduler time: 12.498154661152512 Scheduler overhead time: 0.036869713105261326 Adapter cache time: 0.008770087733864784 Engine time: 0.03586798021569848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.97328690206632,
    "estimated_duration": 3600.186193650137,
    "input_throughput": 5375.973063320084,
    "output_throughput": 4747.15058630683,
    "total_throughput": 10123.123649626914,
    "itl": 180.9549364053926,
    "ttft": 2112579.7255337783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7787714854767575,
    "arrivals": 931056,
    "finished_requests": 78175,
    "scheduler_time": 96.16031333575384
}
#Debug simulation 
Total elapsed time: 12.973432213999331. Arrivals time: 0.38631435902789235 Scheduler time: 12.491303876508027 Scheduler overhead time: 0.036271174903959036 Adapter cache time: 0.008788011502474546 Engine time: 0.03546156361699104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.138496562838554,
    "estimated_duration": 3600.174874051929,
    "input_throughput": 5357.602803969179,
    "output_throughput": 4735.90133715101,
    "total_throughput": 10093.50414112019,
    "itl": 179.1689103152844,
    "ttft": 2114187.4848182374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8361356518603902,
    "arrivals": 931056,
    "finished_requests": 77975,
    "scheduler_time": 96.26624488305984
}
#Debug simulation 
Total elapsed time: 12.138595279771835. Arrivals time: 0.3055646540597081 Scheduler time: 11.738049276173115 Scheduler overhead time: 0.035083805210888386 Adapter cache time: 0.009118379559367895 Engine time: 0.035700110252946615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 12.984431824181229,
    "estimated_duration": 3600.1541584341676,
    "input_throughput": 5376.020900287766,
    "output_throughput": 4747.192827829714,
    "total_throughput": 10123.21372811748,
    "itl": 180.95380675909786,
    "ttft": 2112562.7337646335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7469010506384094,
    "arrivals": 931056,
    "finished_requests": 78175,
    "scheduler_time": 96.16014855461769
}
#Debug simulation 
Total elapsed time: 12.984556138049811. Arrivals time: 0.32114050537347794 Scheduler time: 12.566632724367082 Scheduler overhead time: 0.03602425614371896 Adapter cache time: 0.008929119445383549 Engine time: 0.03649086644873023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 12.369207603856921,
    "estimated_duration": 3600.185362210966,
    "input_throughput": 5357.587196053304,
    "output_throughput": 4735.887540392951,
    "total_throughput": 10093.474736446256,
    "itl": 179.16928678895897,
    "ttft": 2114192.8398379874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8465732161328233,
    "arrivals": 931056,
    "finished_requests": 77975,
    "scheduler_time": 96.26629547783122
}
#Debug simulation 
Total elapsed time: 12.369314495008439. Arrivals time: 0.3271889849565923 Scheduler time: 11.944933134131134 Scheduler overhead time: 0.03632100438699126 Adapter cache time: 0.009216070640832186 Engine time: 0.03638135362416506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.834718755912036,
    "estimated_duration": 3600.12172675767,
    "input_throughput": 5376.069330142065,
    "output_throughput": 4747.235592889829,
    "total_throughput": 10123.304923031894,
    "itl": 180.95269897248872,
    "ttft": 2112547.004966745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7146220204816209,
    "arrivals": 931056,
    "finished_requests": 78175,
    "scheduler_time": 96.15999590826944
}
#Debug simulation 
Total elapsed time: 12.834854467771947. Arrivals time: 0.32360350992530584 Scheduler time: 12.416696235537529 Scheduler overhead time: 0.03540526004508138 Adapter cache time: 0.008597011212259531 Engine time: 0.03550291666761041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.235404789913446,
    "estimated_duration": 3600.196223520124,
    "input_throughput": 5357.571032931279,
    "output_throughput": 4735.8732528554065,
    "total_throughput": 10093.444285786685,
    "itl": 179.16966452858517,
    "ttft": 2114197.909059538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8573880417645015,
    "arrivals": 931056,
    "finished_requests": 77975,
    "scheduler_time": 96.26634196136298
}
#Debug simulation 
Total elapsed time: 12.235561984125525. Arrivals time: 0.3103061313740909 Scheduler time: 11.82936089253053 Scheduler overhead time: 0.03576921531930566 Adapter cache time: 0.009057145100086927 Engine time: 0.03571757208555937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.844502995256335,
    "estimated_duration": 3600.1705883659342,
    "input_throughput": 5402.120961392394,
    "output_throughput": 4743.168019643938,
    "total_throughput": 10145.288981036332,
    "itl": 180.63867255287053,
    "ttft": 2112971.025105562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.697791205616667,
    "arrivals": 928543,
    "finished_requests": 78418,
    "scheduler_time": 96.18859575318159
}
#Debug simulation 
Total elapsed time: 11.8446314628236. Arrivals time: 0.40330528654158115 Scheduler time: 11.348511361517012 Scheduler overhead time: 0.03479384537786245 Adapter cache time: 0.008378627710044384 Engine time: 0.03432741155847907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.913700230885297,
    "estimated_duration": 3600.0256424400604,
    "input_throughput": 5401.287360503503,
    "output_throughput": 4742.923716628579,
    "total_throughput": 10144.211077132082,
    "itl": 180.65265499686112,
    "ttft": 2113044.482453918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7608311837073446,
    "arrivals": 928543,
    "finished_requests": 78408,
    "scheduler_time": 96.18266430252588
}
#Debug simulation 
Total elapsed time: 11.913795887958258. Arrivals time: 0.40981314657256007 Scheduler time: 11.41008743783459 Scheduler overhead time: 0.03523019282147288 Adapter cache time: 0.008492148481309414 Engine time: 0.03493518382310867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.68383689923212,
    "estimated_duration": 3600.1912673870916,
    "input_throughput": 5387.038787546374,
    "output_throughput": 4732.011089056475,
    "total_throughput": 10119.049876602849,
    "itl": 178.82539983025478,
    "ttft": 2114160.5706583215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7363906076364253,
    "arrivals": 928543,
    "finished_requests": 78223,
    "scheduler_time": 96.29747206001751
}
#Debug simulation 
Total elapsed time: 11.683931208215654. Arrivals time: 0.463485689368099 Scheduler time: 11.126080041751266 Scheduler overhead time: 0.03552739787846804 Adapter cache time: 0.008489844389259815 Engine time: 0.03513724682852626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.973513762000948,
    "estimated_duration": 3600.1864829449623,
    "input_throughput": 5402.097111394915,
    "output_throughput": 4743.147078878984,
    "total_throughput": 10145.2441902739,
    "itl": 180.63922426735468,
    "ttft": 2112979.415842905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7136019020760448,
    "arrivals": 928543,
    "finished_requests": 78418,
    "scheduler_time": 96.18867963573177
}
#Debug simulation 
Total elapsed time: 11.973612425848842. Arrivals time: 0.4197699665091932 Scheduler time: 11.459622024558485 Scheduler overhead time: 0.03541472414508462 Adapter cache time: 0.008585888426750898 Engine time: 0.03501181723549962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.626740394625813,
    "estimated_duration": 3600.0010396728076,
    "input_throughput": 5386.894277609028,
    "output_throughput": 4731.893633438573,
    "total_throughput": 10118.7879110476,
    "itl": 178.8247271839228,
    "ttft": 2114165.2527314955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7459478954039548,
    "arrivals": 928543,
    "finished_requests": 78219,
    "scheduler_time": 96.29214880621198
}
#Debug simulation 
Total elapsed time: 11.626862377859652. Arrivals time: 0.4055463965050876 Scheduler time: 11.126941825728863 Scheduler overhead time: 0.03509236592799425 Adapter cache time: 0.008449815679341555 Engine time: 0.035349962301552296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.003461576998234,
    "estimated_duration": 3600.154447198511,
    "input_throughput": 5402.145181614097,
    "output_throughput": 4743.189285473014,
    "total_throughput": 10145.33446708711,
    "itl": 180.6381243626562,
    "ttft": 2112963.169002452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.681731467237697,
    "arrivals": 928543,
    "finished_requests": 78418,
    "scheduler_time": 96.1885143241131
}
#Debug simulation 
Total elapsed time: 12.003573427908123. Arrivals time: 0.42669472889974713 Scheduler time: 11.483412397559732 Scheduler overhead time: 0.03470849897712469 Adapter cache time: 0.0085851289331913 Engine time: 0.03505236329510808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.605044808238745,
    "estimated_duration": 3600.010769463124,
    "input_throughput": 5386.8797183882,
    "output_throughput": 4731.880844495483,
    "total_throughput": 10118.760562883683,
    "itl": 178.82506632511928,
    "ttft": 2114170.2349485494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7556309369578985,
    "arrivals": 928543,
    "finished_requests": 78219,
    "scheduler_time": 96.292195554985
}
#Debug simulation 
Total elapsed time: 11.605148452799767. Arrivals time: 0.4620405426248908 Scheduler time: 11.049487098585814 Scheduler overhead time: 0.034864773973822594 Adapter cache time: 0.00845647742971778 Engine time: 0.035079953260719776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.307185458019376,
    "estimated_duration": 3600.0577428158504,
    "input_throughput": 5375.497945447787,
    "output_throughput": 4741.305895457441,
    "total_throughput": 10116.803840905228,
    "itl": 180.72348495609342,
    "ttft": 2107966.877923085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.61209754878655,
    "arrivals": 927414,
    "finished_requests": 78445,
    "scheduler_time": 96.16282403355993
}
#Debug simulation 
Total elapsed time: 10.307347925845534. Arrivals time: 0.31487671518698335 Scheduler time: 9.901404300238937 Scheduler overhead time: 0.03418323816731572 Adapter cache time: 0.007988061290234327 Engine time: 0.03374158404767513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.313143773935735,
    "estimated_duration": 3600.1027214246337,
    "input_throughput": 5375.430785581024,
    "output_throughput": 4741.246658996846,
    "total_throughput": 10116.67744457787,
    "itl": 180.725012653097,
    "ttft": 2107991.187932063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.656847784835847,
    "arrivals": 927414,
    "finished_requests": 78445,
    "scheduler_time": 96.16305240628945
}
#Debug simulation 
Total elapsed time: 10.313236190006137. Arrivals time: 0.4013028549961746 Scheduler time: 9.820548944175243 Scheduler overhead time: 0.03436256665736437 Adapter cache time: 0.007950189057737589 Engine time: 0.03409861493855715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.806891109794378,
    "estimated_duration": 3600.0032365303546,
    "input_throughput": 5364.434343845947,
    "output_throughput": 4733.587966555239,
    "total_throughput": 10098.022310401186,
    "itl": 178.99937673189706,
    "ttft": 2108149.40775952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6437443435937193,
    "arrivals": 927414,
    "finished_requests": 78283,
    "scheduler_time": 96.26774859607754
}
#Debug simulation 
Total elapsed time: 10.80701557174325. Arrivals time: 0.3996327272616327 Scheduler time: 10.315265164710581 Scheduler overhead time: 0.034358213655650616 Adapter cache time: 0.00806303322315216 Engine time: 0.03456472046673298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 10.3064244100824,
    "estimated_duration": 3600.0715089030577,
    "input_throughput": 5375.477390419,
    "output_throughput": 4741.28776547578,
    "total_throughput": 10116.76515589478,
    "itl": 180.72396954725332,
    "ttft": 2107974.2668583933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6257945406343799,
    "arrivals": 927414,
    "finished_requests": 78445,
    "scheduler_time": 96.16289312890957
}
#Debug simulation 
Total elapsed time: 10.306539022829384. Arrivals time: 0.41515483474358916 Scheduler time: 9.800582258496433 Scheduler overhead time: 0.033949818927794695 Adapter cache time: 0.0078123570419847965 Engine time: 0.03400798514485359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 10.744773227255791,
    "estimated_duration": 3600.012593236868,
    "input_throughput": 5364.420401273119,
    "output_throughput": 4733.5756636001215,
    "total_throughput": 10097.99606487324,
    "itl": 178.99967447697426,
    "ttft": 2108154.3832340846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6530501237884185,
    "arrivals": 927414,
    "finished_requests": 78283,
    "scheduler_time": 96.26779952240051
}
#Debug simulation 
Total elapsed time: 10.744892958085984. Arrivals time: 0.39586388459429145 Scheduler time: 10.256822316441685 Scheduler overhead time: 0.03436276502907276 Adapter cache time: 0.007921510376036167 Engine time: 0.034855125937610865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.250762738287449,
    "estimated_duration": 3600.0435734328175,
    "input_throughput": 5375.519102827642,
    "output_throughput": 4741.324556725823,
    "total_throughput": 10116.843659553466,
    "itl": 180.7230199357064,
    "ttft": 2107959.0737181036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5980100589804359,
    "arrivals": 927414,
    "finished_requests": 78445,
    "scheduler_time": 96.16274214031924
}
#Debug simulation 
Total elapsed time: 10.250871664378792. Arrivals time: 0.410265177488327 Scheduler time: 9.750811276491731 Scheduler overhead time: 0.03333630785346031 Adapter cache time: 0.007796313147991896 Engine time: 0.033744262997061014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.80977314338088,
    "estimated_duration": 3600.0209380114734,
    "input_throughput": 5364.40796665679,
    "output_throughput": 4733.56469126894,
    "total_throughput": 10097.972657925731,
    "itl": 178.99995527957785,
    "ttft": 2108158.886483975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6613498736917987,
    "arrivals": 927414,
    "finished_requests": 78283,
    "scheduler_time": 96.26784454710531
}
#Debug simulation 
Total elapsed time: 10.809870750177652. Arrivals time: 0.4001496550627053 Scheduler time: 10.317512492649257 Scheduler overhead time: 0.03437070734798908 Adapter cache time: 0.0080077457241714 Engine time: 0.03460352402180433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.753794215153903,
    "estimated_duration": 3600.11258167748,
    "input_throughput": 5368.554888634721,
    "output_throughput": 4744.780506847574,
    "total_throughput": 10113.335395482294,
    "itl": 180.92333327784613,
    "ttft": 2111496.8654695097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.477436088053509,
    "arrivals": 926784,
    "finished_requests": 78329,
    "scheduler_time": 96.15057223361258
}
#Debug simulation 
Total elapsed time: 11.75390173587948. Arrivals time: 0.41960860369727015 Scheduler time: 11.24241253407672 Scheduler overhead time: 0.034495190251618624 Adapter cache time: 0.007377073634415865 Engine time: 0.034903219901025295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.660832961089909,
    "estimated_duration": 3600.145963232,
    "input_throughput": 5368.505109900875,
    "output_throughput": 4744.736511923258,
    "total_throughput": 10113.241621824132,
    "itl": 180.92418155727339,
    "ttft": 2111516.970536654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5105761403962972,
    "arrivals": 926784,
    "finished_requests": 78329,
    "scheduler_time": 96.15081373578474
}
#Debug simulation 
Total elapsed time: 11.661014745011926. Arrivals time: 0.4610199467279017 Scheduler time: 11.108631835319102 Scheduler overhead time: 0.03429156728088856 Adapter cache time: 0.007228249683976173 Engine time: 0.03479265281930566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.290424695238471,
    "estimated_duration": 3600.15312691754,
    "input_throughput": 5356.625210136987,
    "output_throughput": 4732.937849948925,
    "total_throughput": 10089.563060085913,
    "itl": 178.75095334861868,
    "ttft": 2113623.021118521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.531496267877521,
    "arrivals": 926784,
    "finished_requests": 78151,
    "scheduler_time": 96.27974494920898
}
#Debug simulation 
Total elapsed time: 11.290557160042226. Arrivals time: 0.4030620874837041 Scheduler time: 10.795908102300018 Scheduler overhead time: 0.03459267318248749 Adapter cache time: 0.007437276188284159 Engine time: 0.03443002933636308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.838286167941988,
    "estimated_duration": 3600.1237439186666,
    "input_throughput": 5368.538243344516,
    "output_throughput": 4744.765795579806,
    "total_throughput": 10113.304038924322,
    "itl": 180.92364378393296,
    "ttft": 2111503.371928591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48851199320051847,
    "arrivals": 926784,
    "finished_requests": 78329,
    "scheduler_time": 96.15065856964448
}
#Debug simulation 
Total elapsed time: 11.838385705836117. Arrivals time: 0.4089055461809039 Scheduler time: 11.33661957969889 Scheduler overhead time: 0.035208314657211304 Adapter cache time: 0.007512691896408796 Engine time: 0.03507708618417382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.232222326099873,
    "estimated_duration": 3600.1605950627063,
    "input_throughput": 5356.614098395271,
    "output_throughput": 4732.9280319794225,
    "total_throughput": 10089.542130374693,
    "itl": 178.751165082605,
    "ttft": 2113627.239840816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5389157412759972,
    "arrivals": 926784,
    "finished_requests": 78151,
    "scheduler_time": 96.27979362098118
}
#Debug simulation 
Total elapsed time: 11.232349249068648. Arrivals time: 0.300463798455894 Scheduler time: 10.84142714412883 Scheduler overhead time: 0.0338759096339345 Adapter cache time: 0.0072885677218437195 Engine time: 0.034249198623001575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.685773683246225,
    "estimated_duration": 3600.1015205604035,
    "input_throughput": 5368.571383229058,
    "output_throughput": 4744.795084928883,
    "total_throughput": 10113.366468157941,
    "itl": 180.92303708859268,
    "ttft": 2111490.250969291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 156,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46644784600474,
    "arrivals": 926784,
    "finished_requests": 78329,
    "scheduler_time": 96.1504993585749
}
#Debug simulation 
Total elapsed time: 11.68587096221745. Arrivals time: 0.4050078089348972 Scheduler time: 11.189128646627069 Scheduler overhead time: 0.034803712740540504 Adapter cache time: 0.00740183237940073 Engine time: 0.03458293993026018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.255029551219195,
    "estimated_duration": 3600.167560259038,
    "input_throughput": 5356.603735025165,
    "output_throughput": 4732.918875246461,
    "total_throughput": 10089.522610271626,
    "itl": 178.75135429722232,
    "ttft": 2113631.2969262707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5458321995288138,
    "arrivals": 926784,
    "finished_requests": 78151,
    "scheduler_time": 96.27984235906371
}
#Debug simulation 
Total elapsed time: 11.255145529285073. Arrivals time: 0.39943971764296293 Scheduler time: 10.763569217640907 Scheduler overhead time: 0.034346087370067835 Adapter cache time: 0.007423451170325279 Engine time: 0.035072491969913244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.623429576866329,
    "estimated_duration": 3600.155294541477,
    "input_throughput": 5356.15783831234,
    "output_throughput": 4747.22493941104,
    "total_throughput": 10103.38277772338,
    "itl": 181.4781909359246,
    "ttft": 2099735.7995508215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4139453376969453,
    "arrivals": 850231,
    "finished_requests": 78332,
    "scheduler_time": 96.08271806807394
}
#Debug simulation 
Total elapsed time: 11.62352527724579. Arrivals time: 0.3152871164493263 Scheduler time: 11.211510646156967 Scheduler overhead time: 0.03510852763429284 Adapter cache time: 0.012358422856777906 Engine time: 0.034268325194716454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.602735366672277,
    "estimated_duration": 3600.0464786648618,
    "input_throughput": 5356.319734836154,
    "output_throughput": 4747.368430181599,
    "total_throughput": 10103.688165017753,
    "itl": 181.48199247778138,
    "ttft": 2099733.374984488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5043904270953543,
    "arrivals": 850231,
    "finished_requests": 78332,
    "scheduler_time": 96.07758810859808
}
#Debug simulation 
Total elapsed time: 11.60282965702936. Arrivals time: 0.40188451251015067 Scheduler time: 11.103807691019028 Scheduler overhead time: 0.0348352063447237 Adapter cache time: 0.01262480253353715 Engine time: 0.034554836340248585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.121679697185755,
    "estimated_duration": 3600.0902604596367,
    "input_throughput": 5347.630644555571,
    "output_throughput": 4736.490133950957,
    "total_throughput": 10084.120778506527,
    "itl": 179.86573391327568,
    "ttft": 2101268.3928955505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6584109908528726,
    "arrivals": 850231,
    "finished_requests": 78148,
    "scheduler_time": 96.17130092094581
}
#Debug simulation 
Total elapsed time: 11.121827451977879. Arrivals time: 0.40143375331535935 Scheduler time: 10.623024623375386 Scheduler overhead time: 0.03437724802643061 Adapter cache time: 0.013337932992726564 Engine time: 0.034379586577415466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.643087854608893,
    "estimated_duration": 3600.1869789576494,
    "input_throughput": 5356.110700001183,
    "output_throughput": 4747.183160178039,
    "total_throughput": 10103.293860179221,
    "itl": 181.47952069907012,
    "ttft": 2099749.969353557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4455527012399358,
    "arrivals": 850231,
    "finished_requests": 78332,
    "scheduler_time": 96.08279512066156
}
#Debug simulation 
Total elapsed time: 11.643206842709333. Arrivals time: 0.39805773459374905 Scheduler time: 11.147950587328523 Scheduler overhead time: 0.03496124409139156 Adapter cache time: 0.012535672169178724 Engine time: 0.03468609880656004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.13452515937388,
    "estimated_duration": 3600.1101792906084,
    "input_throughput": 5347.601056974746,
    "output_throughput": 4736.463927712348,
    "total_throughput": 10084.064984687095,
    "itl": 179.86657936707041,
    "ttft": 2101276.802649187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6782800891064151,
    "arrivals": 850231,
    "finished_requests": 78148,
    "scheduler_time": 96.17135065368308
}
#Debug simulation 
Total elapsed time: 11.13464133720845. Arrivals time: 0.40903957979753613 Scheduler time: 10.627967641223222 Scheduler overhead time: 0.03450005268678069 Adapter cache time: 0.013250969350337982 Engine time: 0.03472767025232315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.577074581757188,
    "estimated_duration": 3600.122670745772,
    "input_throughput": 5356.206375047073,
    "output_throughput": 4747.267958083112,
    "total_throughput": 10103.474333130185,
    "itl": 181.4768001519803,
    "ttft": 2099721.482619334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381403236244793,
    "arrivals": 850231,
    "finished_requests": 78332,
    "scheduler_time": 96.0826363737642
}
#Debug simulation 
Total elapsed time: 11.577193513978273. Arrivals time: 0.30776624707505107 Scheduler time: 11.172945593483746 Scheduler overhead time: 0.034699440468102694 Adapter cache time: 0.012087960727512836 Engine time: 0.03456141287460923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.144835578743368,
    "estimated_duration": 3600.1322352728457,
    "input_throughput": 5347.568295235394,
    "output_throughput": 4736.434910065931,
    "total_throughput": 10084.003205301326,
    "itl": 179.8675220889047,
    "ttft": 2101285.9335302277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 508,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7002870017290097,
    "arrivals": 850231,
    "finished_requests": 78148,
    "scheduler_time": 96.17139972331718
}
#Debug simulation 
Total elapsed time: 11.144945494830608. Arrivals time: 0.4159412160515785 Scheduler time: 10.630837986245751 Scheduler overhead time: 0.034783591981977224 Adapter cache time: 0.013532816432416439 Engine time: 0.03458251990377903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.94650124758482,
    "estimated_duration": 3600.060918811848,
    "input_throughput": 5401.161657680338,
    "output_throughput": 4742.848075369576,
    "total_throughput": 10144.009733049914,
    "itl": 180.44602674959475,
    "ttft": 2089311.4760458458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8913814257504715,
    "arrivals": 793158,
    "finished_requests": 78538,
    "scheduler_time": 96.06413834016963
}
#Debug simulation 
Total elapsed time: 7.946594062726945. Arrivals time: 0.28424782026559114 Scheduler time: 7.567689756397158 Scheduler overhead time: 0.03264816431328654 Adapter cache time: 0.014274363406002522 Engine time: 0.03290186636149883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.955164553131908,
    "estimated_duration": 3600.176987515886,
    "input_throughput": 5400.987525731803,
    "output_throughput": 4742.695167267706,
    "total_throughput": 10143.682692999508,
    "itl": 180.45107787268222,
    "ttft": 2089362.2988669716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.007203256441283,
    "arrivals": 793158,
    "finished_requests": 78538,
    "scheduler_time": 96.06438521347742
}
#Debug simulation 
Total elapsed time: 7.955263754352927. Arrivals time: 0.2959060682915151 Scheduler time: 7.565336737781763 Scheduler overhead time: 0.032352668698877096 Adapter cache time: 0.014386020600795746 Engine time: 0.03259341465309262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.780323934741318,
    "estimated_duration": 3600.114590505365,
    "input_throughput": 5389.11068307871,
    "output_throughput": 4732.404919813513,
    "total_throughput": 10121.515602892223,
    "itl": 178.93148675814618,
    "ttft": 2090436.212966949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.055359391383837,
    "arrivals": 793158,
    "finished_requests": 78374,
    "scheduler_time": 96.1489539850984
}
#Debug simulation 
Total elapsed time: 7.780445829033852. Arrivals time: 0.2827915004454553 Scheduler time: 7.40284608816728 Scheduler overhead time: 0.03260092902928591 Adapter cache time: 0.014881687704473734 Engine time: 0.03263702429831028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.962557424325496,
    "estimated_duration": 3600.1012331313395,
    "input_throughput": 5401.101174893162,
    "output_throughput": 4742.794964448458,
    "total_throughput": 10143.89613934162,
    "itl": 180.44780335929286,
    "ttft": 2089329.6158348403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.931613122529803,
    "arrivals": 793158,
    "finished_requests": 78538,
    "scheduler_time": 96.06422096282381
}
#Debug simulation 
Total elapsed time: 7.962704887147993. Arrivals time: 0.291012829169631 Scheduler time: 7.576999437995255 Scheduler overhead time: 0.032699234783649445 Adapter cache time: 0.014542233664542437 Engine time: 0.03268487099558115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.811515436973423,
    "estimated_duration": 3600.138158318073,
    "input_throughput": 5389.075404001726,
    "output_throughput": 4732.373939771108,
    "total_throughput": 10121.449343772834,
    "itl": 178.9324981884743,
    "ttft": 2090446.3743171073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.07887534944341,
    "arrivals": 793158,
    "finished_requests": 78374,
    "scheduler_time": 96.14900583976579
}
#Debug simulation 
Total elapsed time: 7.811629476025701. Arrivals time: 0.2833216502331197 Scheduler time: 7.433268832974136 Scheduler overhead time: 0.032802749425172806 Adapter cache time: 0.01459860848262906 Engine time: 0.03271219087764621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.9577371729537845,
    "estimated_duration": 3600.017299413671,
    "input_throughput": 5401.227100538349,
    "output_throughput": 4742.9055418097305,
    "total_throughput": 10144.132642348079,
    "itl": 180.4441092837252,
    "ttft": 2089292.6748077834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8478510822495158,
    "arrivals": 793158,
    "finished_requests": 78538,
    "scheduler_time": 96.06404928541477
}
#Debug simulation 
Total elapsed time: 7.957832532003522. Arrivals time: 0.2820000280626118 Scheduler time: 7.5813960786908865 Scheduler overhead time: 0.0327090029604733 Adapter cache time: 0.014326414559036493 Engine time: 0.0326444311067462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.802901655901223,
    "estimated_duration": 3600.1649960607774,
    "input_throughput": 5389.035230670986,
    "output_throughput": 4732.338661878479,
    "total_throughput": 10121.373892549465,
    "itl": 178.9336747848285,
    "ttft": 2090457.8624729412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.10566090594977,
    "arrivals": 793158,
    "finished_requests": 78374,
    "scheduler_time": 96.14905802598476
}
#Debug simulation 
Total elapsed time: 7.803007078822702. Arrivals time: 0.29308502143248916 Scheduler time: 7.415031507611275 Scheduler overhead time: 0.03255105996504426 Adapter cache time: 0.014676006510853767 Engine time: 0.03294486226513982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.1773545308969915,
    "estimated_duration": 3600.18953470773,
    "input_throughput": 5410.415705122397,
    "output_throughput": 4741.581751580149,
    "total_throughput": 10151.997456702546,
    "itl": 180.3028079223536,
    "ttft": 2088503.3945821289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.903623376726203,
    "arrivals": 783512,
    "finished_requests": 78595,
    "scheduler_time": 96.0669533798422
}
#Debug simulation 
Total elapsed time: 7.177457324694842. Arrivals time: 0.3609282118268311 Scheduler time: 6.723173769656569 Scheduler overhead time: 0.03251751931384206 Adapter cache time: 0.014001054223626852 Engine time: 0.03214060561731458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.222249913960695,
    "estimated_duration": 3600.126797846663,
    "input_throughput": 5410.097780903096,
    "output_throughput": 4741.216339993757,
    "total_throughput": 10151.314120896854,
    "itl": 180.3066200586968,
    "ttft": 2088496.0402543552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.02651817335282,
    "arrivals": 783512,
    "finished_requests": 78588,
    "scheduler_time": 96.0622588724541
}
#Debug simulation 
Total elapsed time: 7.222342357039452. Arrivals time: 0.36734251491725445 Scheduler time: 6.76111563295126 Scheduler overhead time: 0.03256817627698183 Adapter cache time: 0.014213755261152983 Engine time: 0.03236023290082812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.08974690688774,
    "estimated_duration": 3600.113449815898,
    "input_throughput": 5398.895137872377,
    "output_throughput": 4732.098373419645,
    "total_throughput": 10130.993511292021,
    "itl": 178.67223609520056,
    "ttft": 2089490.8548692092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.098289319965997,
    "arrivals": 783512,
    "finished_requests": 78428,
    "scheduler_time": 96.15934138044409
}
#Debug simulation 
Total elapsed time: 7.089865960180759. Arrivals time: 0.3606179994530976 Scheduler time: 6.634450157638639 Scheduler overhead time: 0.032856634352356195 Adapter cache time: 0.01451932918280363 Engine time: 0.03257959755137563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.165601235348731,
    "estimated_duration": 3600.027693031622,
    "input_throughput": 5410.246714962956,
    "output_throughput": 4741.346860480961,
    "total_throughput": 10151.593575443916,
    "itl": 180.3026723305068,
    "ttft": 2088460.3526064374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9447991096647335,
    "arrivals": 783512,
    "finished_requests": 78588,
    "scheduler_time": 96.06158556829278
}
#Debug simulation 
Total elapsed time: 7.165693951305002. Arrivals time: 0.3629514374770224 Scheduler time: 6.709215325303376 Scheduler overhead time: 0.03242905344814062 Adapter cache time: 0.014197179581969976 Engine time: 0.03222762793302536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.105007944162935,
    "estimated_duration": 3600.1390366517703,
    "input_throughput": 5398.8567669532595,
    "output_throughput": 4732.064741545104,
    "total_throughput": 10130.921508498364,
    "itl": 178.673343704276,
    "ttft": 2089502.1266820978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.123817338608209,
    "arrivals": 783512,
    "finished_requests": 78428,
    "scheduler_time": 96.1594001976947
}
#Debug simulation 
Total elapsed time: 7.105119256302714. Arrivals time: 0.37503072200343013 Scheduler time: 6.635271072387695 Scheduler overhead time: 0.03277168981730938 Adapter cache time: 0.014692844823002815 Engine time: 0.032470029313117266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.204662600997835,
    "estimated_duration": 3600.1456354164466,
    "input_throughput": 5410.481678402108,
    "output_throughput": 4741.639569262969,
    "total_throughput": 10152.121247665078,
    "itl": 180.30085899032983,
    "ttft": 2088484.4921178445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.859811283429124,
    "arrivals": 783512,
    "finished_requests": 78595,
    "scheduler_time": 96.06686618177619
}
#Debug simulation 
Total elapsed time: 7.204757400788367. Arrivals time: 0.2777928183786571 Scheduler time: 6.832726101391017 Scheduler overhead time: 0.03266163310036063 Adapter cache time: 0.0144372652284801 Engine time: 0.032374249305576086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.081852616742253,
    "estimated_duration": 3600.066731355706,
    "input_throughput": 5398.33493383092,
    "output_throughput": 4731.981452350697,
    "total_throughput": 10130.316386181616,
    "itl": 178.64537790890756,
    "ttft": 2089629.8880932746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 649,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.172019277252258,
    "arrivals": 783512,
    "finished_requests": 78427,
    "scheduler_time": 96.15798596358472
}
#Debug simulation 
Total elapsed time: 7.081946831662208. Arrivals time: 0.36231345497071743 Scheduler time: 6.62511640181765 Scheduler overhead time: 0.0326887178234756 Adapter cache time: 0.014673840720206499 Engine time: 0.03234154684469104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.760482011828572,
    "estimated_duration": 3600.0140237719315,
    "input_throughput": 5373.554067362027,
    "output_throughput": 4744.955960505496,
    "total_throughput": 10118.510027867524,
    "itl": 180.8956213527935,
    "ttft": 2089644.5833775473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.726115087578097,
    "arrivals": 778725,
    "finished_requests": 78472,
    "scheduler_time": 96.01753722879783
}
#Debug simulation 
Total elapsed time: 6.760604628827423. Arrivals time: 0.3636085847392678 Scheduler time: 6.304842408746481 Scheduler overhead time: 0.03233301779255271 Adapter cache time: 0.013088229577988386 Engine time: 0.032167562283575535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.7559147868305445,
    "estimated_duration": 3600.125315397592,
    "input_throughput": 5373.387953264505,
    "output_throughput": 4744.809278427438,
    "total_throughput": 10118.197231691944,
    "itl": 180.9004465860599,
    "ttft": 2089693.8035865077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8371600388293203,
    "arrivals": 778725,
    "finished_requests": 78472,
    "scheduler_time": 96.01778390317466
}
#Debug simulation 
Total elapsed time: 6.756007495801896. Arrivals time: 0.35490341670811176 Scheduler time: 6.30874185776338 Scheduler overhead time: 0.03220530226826668 Adapter cache time: 0.013282250612974167 Engine time: 0.03220902103930712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.004556146916002,
    "estimated_duration": 3600.103690114513,
    "input_throughput": 5357.98011956329,
    "output_throughput": 4731.251782211365,
    "total_throughput": 10089.231901774654,
    "itl": 178.88181265196616,
    "ttft": 2092026.0001311654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9102988119237243,
    "arrivals": 778725,
    "finished_requests": 78258,
    "scheduler_time": 96.1321567868432
}
#Debug simulation 
Total elapsed time: 7.004659773781896. Arrivals time: 0.599469680339098 Scheduler time: 6.311232483014464 Scheduler overhead time: 0.03270140616223216 Adapter cache time: 0.013806415256112814 Engine time: 0.03261784231290221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.773665242828429,
    "estimated_duration": 3600.055683974892,
    "input_throughput": 5373.491884059124,
    "output_throughput": 4744.901051402497,
    "total_throughput": 10118.392935461621,
    "itl": 180.8974090526523,
    "ttft": 2089662.3220804317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7676988346944484,
    "arrivals": 778725,
    "finished_requests": 78472,
    "scheduler_time": 96.01761368459259
}
#Debug simulation 
Total elapsed time: 6.773760397918522. Arrivals time: 0.3577878959476948 Scheduler time: 6.323249036446214 Scheduler overhead time: 0.032378461211919785 Adapter cache time: 0.013444582931697369 Engine time: 0.03232987690716982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.715371321886778,
    "estimated_duration": 3600.1265027414,
    "input_throughput": 5357.9461680892955,
    "output_throughput": 4731.221802075518,
    "total_throughput": 10089.167970164814,
    "itl": 178.88279716245583,
    "ttft": 2092035.967927834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9330602472648062,
    "arrivals": 778725,
    "finished_requests": 78258,
    "scheduler_time": 96.13220797840742
}
#Debug simulation 
Total elapsed time: 6.715481103863567. Arrivals time: 0.3737656455487013 Scheduler time: 6.247948296833783 Scheduler overhead time: 0.03264193935319781 Adapter cache time: 0.01364271342754364 Engine time: 0.03265931736677885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.750140917953104,
    "estimated_duration": 3600.1730297513614,
    "input_throughput": 5373.707830186182,
    "output_throughput": 4745.026102587019,
    "total_throughput": 10118.7339327732,
    "itl": 180.89487471595675,
    "ttft": 2089668.9435236272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.686388366324804,
    "arrivals": 778725,
    "finished_requests": 78476,
    "scheduler_time": 96.02280040618429
}
#Debug simulation 
Total elapsed time: 6.750237973872572. Arrivals time: 0.3565616053529084 Scheduler time: 6.30119598377496 Scheduler overhead time: 0.03238224145025015 Adapter cache time: 0.013407895341515541 Engine time: 0.03207794949412346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.677906016819179,
    "estimated_duration": 3600.1522059813033,
    "input_throughput": 5357.907915102236,
    "output_throughput": 4731.188023578928,
    "total_throughput": 10089.095938681165,
    "itl": 178.88393675671978,
    "ttft": 2092047.0718724332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9587140196934298,
    "arrivals": 778725,
    "finished_requests": 78258,
    "scheduler_time": 96.13225744590342
}
#Debug simulation 
Total elapsed time: 6.677995980717242. Arrivals time: 0.36460809921845794 Scheduler time: 6.2204295988194644 Scheduler overhead time: 0.03224638197571039 Adapter cache time: 0.013553914614021778 Engine time: 0.03244090732187033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.38602151395753,
    "estimated_duration": 3600.1151778490766,
    "input_throughput": 5336.073167380574,
    "output_throughput": 4749.6674843098135,
    "total_throughput": 10085.740651690387,
    "itl": 181.77190776670065,
    "ttft": 2088805.5735050852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4598526538559382,
    "arrivals": 776317,
    "finished_requests": 78066,
    "scheduler_time": 96.01074651556446
}
#Debug simulation 
Total elapsed time: 6.386142285075039. Arrivals time: 0.27135133277624846 Scheduler time: 6.023905208799988 Scheduler overhead time: 0.03215498337522149 Adapter cache time: 0.01195992436259985 Engine time: 0.03210644889622927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.348131505772471,
    "estimated_duration": 3600.0373671824723,
    "input_throughput": 5335.596562163795,
    "output_throughput": 4749.388757978792,
    "total_throughput": 10084.985320142587,
    "itl": 181.7763270684905,
    "ttft": 2088846.7099814042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.554552920658611,
    "arrivals": 776317,
    "finished_requests": 78059,
    "scheduler_time": 96.00643544359245
}
#Debug simulation 
Total elapsed time: 6.348224632907659. Arrivals time: 0.27509855199605227 Scheduler time: 5.9831593739800155 Scheduler overhead time: 0.03190763061866164 Adapter cache time: 0.011984858196228743 Engine time: 0.03180225333198905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.363400768954307,
    "estimated_duration": 3600.095629209396,
    "input_throughput": 5324.533283081038,
    "output_throughput": 4738.321077250422,
    "total_throughput": 10062.85436033146,
    "itl": 179.72699264847012,
    "ttft": 2090022.8901183615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5836885924078619,
    "arrivals": 776317,
    "finished_requests": 77879,
    "scheduler_time": 96.12813164324315
}
#Debug simulation 
Total elapsed time: 6.3635091269388795. Arrivals time: 0.28438896453008056 Scheduler time: 5.987444636877626 Scheduler overhead time: 0.03250245517119765 Adapter cache time: 0.012224058620631695 Engine time: 0.0321899326518178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.355993336997926,
    "estimated_duration": 3600.172160224903,
    "input_throughput": 5336.1314806678265,
    "output_throughput": 4749.692025542407,
    "total_throughput": 10085.823506210232,
    "itl": 181.77403978870882,
    "ttft": 2088814.9185630772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4895862650265854,
    "arrivals": 776317,
    "finished_requests": 78067,
    "scheduler_time": 96.01162886179915
}
#Debug simulation 
Total elapsed time: 6.356086642947048. Arrivals time: 0.2745120511390269 Scheduler time: 5.9913851246237755 Scheduler overhead time: 0.03210958279669285 Adapter cache time: 0.011825066059827805 Engine time: 0.03173985658213496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.421371757052839,
    "estimated_duration": 3600.1160543047217,
    "input_throughput": 5324.503074582692,
    "output_throughput": 4738.294194600467,
    "total_throughput": 10062.797269183158,
    "itl": 179.72784754834194,
    "ttft": 2090031.9289669865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6040607058070642,
    "arrivals": 776317,
    "finished_requests": 77879,
    "scheduler_time": 96.12818462518592
}
#Debug simulation 
Total elapsed time: 6.421464730985463. Arrivals time: 0.2806938122957945 Scheduler time: 6.049333423841745 Scheduler overhead time: 0.03225425397977233 Adapter cache time: 0.012044361792504787 Engine time: 0.03240137314423919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.4361429968848825,
    "estimated_duration": 3600.0814963643634,
    "input_throughput": 5336.123090380094,
    "output_throughput": 4749.711921040739,
    "total_throughput": 10085.835011420833,
    "itl": 181.77049755677754,
    "ttft": 2088790.6807962991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.426253990668324,
    "arrivals": 776317,
    "finished_requests": 78066,
    "scheduler_time": 96.01066369397934
}
#Debug simulation 
Total elapsed time: 6.436237236019224. Arrivals time: 0.32310418505221605 Scheduler time: 6.021460150368512 Scheduler overhead time: 0.03222772665321827 Adapter cache time: 0.01207743538543582 Engine time: 0.03278869856148958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.654469442088157,
    "estimated_duration": 3600.1358462546377,
    "input_throughput": 5324.473802826658,
    "output_throughput": 4738.268145560821,
    "total_throughput": 10062.74194838748,
    "itl": 179.72867021195788,
    "ttft": 2090041.2323592973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6238040502741928,
    "arrivals": 776317,
    "finished_requests": 77879,
    "scheduler_time": 96.12823323064781
}
#Debug simulation 
Total elapsed time: 6.654705545864999. Arrivals time: 0.27522185118868947 Scheduler time: 6.288359454832971 Scheduler overhead time: 0.032330750953406096 Adapter cache time: 0.012089214287698269 Engine time: 0.0318957744166255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.2152323932386935,
    "estimated_duration": 3600.1535666674167,
    "input_throughput": 5388.497918425634,
    "output_throughput": 4743.105449195267,
    "total_throughput": 10131.603367620903,
    "itl": 180.7707848518321,
    "ttft": 2088776.109728549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3221307053789595,
    "arrivals": 775108,
    "finished_requests": 78527,
    "scheduler_time": 96.06024665198727
}
#Debug simulation 
Total elapsed time: 6.215356190223247. Arrivals time: 0.354567913338542 Scheduler time: 5.77052943315357 Scheduler overhead time: 0.03221564553678036 Adapter cache time: 0.011436563450843096 Engine time: 0.03195461304858327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.187823597341776,
    "estimated_duration": 3600.047396638782,
    "input_throughput": 5388.383780200097,
    "output_throughput": 4743.028387887991,
    "total_throughput": 10131.412168088087,
    "itl": 180.77544390537201,
    "ttft": 2088777.730135871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.412645941656089,
    "arrivals": 775108,
    "finished_requests": 78523,
    "scheduler_time": 96.05516828048248
}
#Debug simulation 
Total elapsed time: 6.187919080257416. Arrivals time: 0.26790169440209866 Scheduler time: 5.830226041842252 Scheduler overhead time: 0.03197554871439934 Adapter cache time: 0.011402262840420008 Engine time: 0.03190650139003992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.170929474756122,
    "estimated_duration": 3600.0112743724208,
    "input_throughput": 5379.641485532889,
    "output_throughput": 4734.089896140955,
    "total_throughput": 10113.731381673844,
    "itl": 179.33526189201004,
    "ttft": 2089829.1105103367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.434909999109813,
    "arrivals": 775108,
    "finished_requests": 78361,
    "scheduler_time": 96.13897553970722
}
#Debug simulation 
Total elapsed time: 6.1710274438373744. Arrivals time: 0.2715546735562384 Scheduler time: 5.808353789616376 Scheduler overhead time: 0.0324810934253037 Adapter cache time: 0.01166184013709426 Engine time: 0.032385529950261116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.195369256194681,
    "estimated_duration": 3600.1820431844694,
    "input_throughput": 5388.455296788445,
    "output_throughput": 4743.067932447062,
    "total_throughput": 10131.523229235507,
    "itl": 180.7719903910975,
    "ttft": 2088789.1934347176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3505394532531478,
    "arrivals": 775108,
    "finished_requests": 78527,
    "scheduler_time": 96.06031442113058
}
#Debug simulation 
Total elapsed time: 6.195490578189492. Arrivals time: 0.3513092640787363 Scheduler time: 5.754304414149374 Scheduler overhead time: 0.031955634243786335 Adapter cache time: 0.011534938123077154 Engine time: 0.031800638884305954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.179067620076239,
    "estimated_duration": 3600.0309444145487,
    "input_throughput": 5379.612091959254,
    "output_throughput": 4734.064029766712,
    "total_throughput": 10113.676121725966,
    "itl": 179.33610297610008,
    "ttft": 2089838.125885145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4545275897905277,
    "arrivals": 775108,
    "finished_requests": 78361,
    "scheduler_time": 96.13902799116745
}
#Debug simulation 
Total elapsed time: 6.179164735134691. Arrivals time: 0.27426311280578375 Scheduler time: 5.814025241415948 Scheduler overhead time: 0.032283542677760124 Adapter cache time: 0.011626878753304482 Engine time: 0.03231206675991416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.214077920187265,
    "estimated_duration": 3600.123060769517,
    "input_throughput": 5388.543578244635,
    "output_throughput": 4743.14564023544,
    "total_throughput": 10131.689218480074,
    "itl": 180.7694771820459,
    "ttft": 2088761.9998524806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.291701727397731,
    "arrivals": 775108,
    "finished_requests": 78527,
    "scheduler_time": 96.0601697320203
}
#Debug simulation 
Total elapsed time: 6.214187649078667. Arrivals time: 0.360467332880944 Scheduler time: 5.764036685694009 Scheduler overhead time: 0.03191217873245478 Adapter cache time: 0.011326496489346027 Engine time: 0.03190440312027931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.149400112684816,
    "estimated_duration": 3600.0493518129156,
    "input_throughput": 5379.584585485548,
    "output_throughput": 4734.039824042297,
    "total_throughput": 10113.624409527845,
    "itl": 179.33686612061484,
    "ttft": 2089846.5248297458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4728876426070927,
    "arrivals": 775108,
    "finished_requests": 78361,
    "scheduler_time": 96.13907533673333
}
#Debug simulation 
Total elapsed time: 6.149495734833181. Arrivals time: 0.2697176611982286 Scheduler time: 5.789975368883461 Scheduler overhead time: 0.031818793155252934 Adapter cache time: 0.011407959274947643 Engine time: 0.03198331734165549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.246150416787714,
    "estimated_duration": 3600.1724985275882,
    "input_throughput": 5376.031289588411,
    "output_throughput": 4745.929537261884,
    "total_throughput": 10121.960826850294,
    "itl": 181.0249708548344,
    "ttft": 2088763.2549606569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.202771683365578,
    "arrivals": 774530,
    "finished_requests": 78373,
    "scheduler_time": 96.05157265908018
}
#Debug simulation 
Total elapsed time: 6.2462440598756075. Arrivals time: 0.369106984231621 Scheduler time: 5.787486094515771 Scheduler overhead time: 0.03206483228132129 Adapter cache time: 0.01076046610251069 Engine time: 0.03219288308173418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.181859005708247,
    "estimated_duration": 3600.056104521558,
    "input_throughput": 5375.813442377453,
    "output_throughput": 4745.879648525819,
    "total_throughput": 10121.69309090327,
    "itl": 181.02973648116478,
    "ttft": 2088762.4086976743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2803774749883499,
    "arrivals": 774530,
    "finished_requests": 78368,
    "scheduler_time": 96.04657737209789
}
#Debug simulation 
Total elapsed time: 6.181952328886837. Arrivals time: 0.2730075800791383 Scheduler time: 5.819546776358038 Scheduler overhead time: 0.03199344454333186 Adapter cache time: 0.010610299184918404 Engine time: 0.03213764913380146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.4685720931738615,
    "estimated_duration": 3600.1430732992208,
    "input_throughput": 5363.1324108199515,
    "output_throughput": 4734.811826347449,
    "total_throughput": 10097.9442371674,
    "itl": 179.21674728641997,
    "ttft": 2090529.4795324467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3033890678174869,
    "arrivals": 774530,
    "finished_requests": 78178,
    "scheduler_time": 96.15205273313174
}
#Debug simulation 
Total elapsed time: 6.468641201034188. Arrivals time: 0.3633365756832063 Scheduler time: 6.015585826244205 Scheduler overhead time: 0.03222930245101452 Adapter cache time: 0.010937332175672054 Engine time: 0.03191586397588253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.188037054147571,
    "estimated_duration": 3600.00322172722,
    "input_throughput": 5375.892411205858,
    "output_throughput": 4745.949363846042,
    "total_throughput": 10121.841775051898,
    "itl": 181.02760255878917,
    "ttft": 2088737.0359651772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2276686789095401,
    "arrivals": 774530,
    "finished_requests": 78368,
    "scheduler_time": 96.04640337382777
}
#Debug simulation 
Total elapsed time: 6.188160398975015. Arrivals time: 0.355281637981534 Scheduler time: 5.743551438674331 Scheduler overhead time: 0.03215615777298808 Adapter cache time: 0.010714688804000616 Engine time: 0.03190083196386695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.090936867985874,
    "estimated_duration": 3600.159851402126,
    "input_throughput": 5363.107416599918,
    "output_throughput": 4734.7897603383435,
    "total_throughput": 10097.89717693826,
    "itl": 179.2174324548514,
    "ttft": 2090537.5048853385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3201143214106617,
    "arrivals": 774530,
    "finished_requests": 78178,
    "scheduler_time": 96.15210558245387
}
#Debug simulation 
Total elapsed time: 6.0910306381993. Arrivals time: 0.2683888361789286 Scheduler time: 5.733722636010498 Scheduler overhead time: 0.031864102464169264 Adapter cache time: 0.010817821603268385 Engine time: 0.03182957787066698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.189914149232209,
    "estimated_duration": 3600.14473604313,
    "input_throughput": 5376.072746806402,
    "output_throughput": 4745.9661354557575,
    "total_throughput": 10122.038882262159,
    "itl": 181.02381958777235,
    "ttft": 2088750.7349060334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1750897658965502,
    "arrivals": 774530,
    "finished_requests": 78373,
    "scheduler_time": 96.0514920920458
}
#Debug simulation 
Total elapsed time: 6.1900209952145815. Arrivals time: 0.36043607257306576 Scheduler time: 5.740146329626441 Scheduler overhead time: 0.032049663830548525 Adapter cache time: 0.010831880383193493 Engine time: 0.03194733150303364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.127806554082781,
    "estimated_duration": 3600.177501824294,
    "input_throughput": 5363.081123143557,
    "output_throughput": 4734.766547305624,
    "total_throughput": 10097.847670449182,
    "itl": 179.21817686105726,
    "ttft": 2090545.4152919238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3377198515087392,
    "arrivals": 774530,
    "finished_requests": 78178,
    "scheduler_time": 96.15215047453805
}
#Debug simulation 
Total elapsed time: 6.1279267449863255. Arrivals time: 0.27084869472309947 Scheduler time: 5.767442523501813 Scheduler overhead time: 0.032190670259296894 Adapter cache time: 0.01090073212981224 Engine time: 0.031829803716391325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.552521965000778,
    "estimated_duration": 3600.095664156256,
    "input_throughput": 5373.904419435532,
    "output_throughput": 4746.20248848431,
    "total_throughput": 10120.106907919842,
    "itl": 181.1981087974314,
    "ttft": 2082978.7597195604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1055,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2288145698491313,
    "arrivals": 716829,
    "finished_requests": 78289,
    "scheduler_time": 95.93764251518671
}
#Debug simulation 
Total elapsed time: 6.5526172812096775. Arrivals time: 0.35051034670323133 Scheduler time: 6.1027106824330986 Scheduler overhead time: 0.03242047177627683 Adapter cache time: 0.020150646567344666 Engine time: 0.03220361517742276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.508829740807414,
    "estimated_duration": 3600.0992613073827,
    "input_throughput": 5373.509893995191,
    "output_throughput": 4745.757480529989,
    "total_throughput": 10119.267374525181,
    "itl": 181.20620127656292,
    "ttft": 2083027.988035249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1055,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.433165068298115,
    "arrivals": 716829,
    "finished_requests": 78283,
    "scheduler_time": 95.93250851187284
}
#Debug simulation 
Total elapsed time: 6.509003818035126. Arrivals time: 0.34648479614406824 Scheduler time: 6.063408999238163 Scheduler overhead time: 0.032288494519889355 Adapter cache time: 0.020075432490557432 Engine time: 0.031997231766581535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.465332586783916,
    "estimated_duration": 3600.1523115911227,
    "input_throughput": 5360.651530732194,
    "output_throughput": 4735.437704985692,
    "total_throughput": 10096.089235717885,
    "itl": 179.4061892053607,
    "ttft": 2084512.0781071056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1076,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5095505374669593,
    "arrivals": 716829,
    "finished_requests": 78104,
    "scheduler_time": 96.03905377512342
}
#Debug simulation 
Total elapsed time: 6.465426206123084. Arrivals time: 0.33924967143684626 Scheduler time: 6.026395697612315 Scheduler overhead time: 0.03253134759142995 Adapter cache time: 0.02030507940798998 Engine time: 0.03224125690758228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.592919143848121,
    "estimated_duration": 3600.167707222085,
    "input_throughput": 5373.796882070239,
    "output_throughput": 4746.1075120815085,
    "total_throughput": 10119.904394151747,
    "itl": 181.20157978943854,
    "ttft": 2083007.8283774888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1055,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.300780185123405,
    "arrivals": 716829,
    "finished_requests": 78289,
    "scheduler_time": 95.9377199656362
}
#Debug simulation 
Total elapsed time: 6.593029044102877. Arrivals time: 0.3609887296333909 Scheduler time: 6.1326249041594565 Scheduler overhead time: 0.032304422464221716 Adapter cache time: 0.020209201145917177 Engine time: 0.03217410854995251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.462205772288144,
    "estimated_duration": 3600.1941176814576,
    "input_throughput": 5360.589281899264,
    "output_throughput": 4735.382716246197,
    "total_throughput": 10095.971998145462,
    "itl": 179.40811056321914,
    "ttft": 2084528.5857115518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1076,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.551300794556738,
    "arrivals": 716829,
    "finished_requests": 78104,
    "scheduler_time": 96.03910960840979
}
#Debug simulation 
Total elapsed time: 6.462299596052617. Arrivals time: 0.2710639010183513 Scheduler time: 6.091837821993977 Scheduler overhead time: 0.03240595571696758 Adapter cache time: 0.020199851132929325 Engine time: 0.03223270829766989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.838868879247457,
    "estimated_duration": 3600.0212608741945,
    "input_throughput": 5374.015484370241,
    "output_throughput": 4746.300580416797,
    "total_throughput": 10120.316064787039,
    "itl": 181.19458776704622,
    "ttft": 2082949.1510616909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1055,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.15450306112172,
    "arrivals": 716829,
    "finished_requests": 78289,
    "scheduler_time": 95.93755074170888
}
#Debug simulation 
Total elapsed time: 6.838941041380167. Arrivals time: 0.5966703994199634 Scheduler time: 6.143614978063852 Scheduler overhead time: 0.032073562033474445 Adapter cache time: 0.02024516509845853 Engine time: 0.03181736497208476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.529005669057369,
    "estimated_duration": 3600.180719788258,
    "input_throughput": 5360.785611099851,
    "output_throughput": 4735.758654082998,
    "total_throughput": 10096.544265182849,
    "itl": 179.44186911422267,
    "ttft": 2084377.2228897978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.569486013054895,
    "arrivals": 716829,
    "finished_requests": 78104,
    "scheduler_time": 96.03669731922648
}
#Debug simulation 
Total elapsed time: 6.529127569869161. Arrivals time: 0.3481379337608814 Scheduler time: 6.080437622964382 Scheduler overhead time: 0.03261365694925189 Adapter cache time: 0.02046130131930113 Engine time: 0.032644692342728376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.100554023869336,
    "estimated_duration": 3600.0257101088837,
    "input_throughput": 5351.379282071106,
    "output_throughput": 4748.56553162865,
    "total_throughput": 10099.944813699756,
    "itl": 181.6660261739617,
    "ttft": 2075750.529764401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1075,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2900243247277885,
    "arrivals": 707393,
    "finished_requests": 78072,
    "scheduler_time": 95.90461562861157
}
#Debug simulation 
Total elapsed time: 6.100647130049765. Arrivals time: 0.2702550175599754 Scheduler time: 5.7322100177407265 Scheduler overhead time: 0.03189863124862313 Adapter cache time: 0.02028943132609129 Engine time: 0.03154302528128028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.135937795042992,
    "estimated_duration": 3600.069032184302,
    "input_throughput": 5350.9473923370615,
    "output_throughput": 4748.002843053519,
    "total_throughput": 10098.950235390581,
    "itl": 181.67716097043152,
    "ttft": 2075850.3077335004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1074,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.501825288136028,
    "arrivals": 707393,
    "finished_requests": 78066,
    "scheduler_time": 95.90051558132559
}
#Debug simulation 
Total elapsed time: 6.13605041615665. Arrivals time: 0.3551565706729889 Scheduler time: 5.682001537177712 Scheduler overhead time: 0.031945581547915936 Adapter cache time: 0.02044224925339222 Engine time: 0.031948677729815245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.077033489942551,
    "estimated_duration": 3600.0317610233465,
    "input_throughput": 5334.517658405586,
    "output_throughput": 4734.60406225829,
    "total_throughput": 10069.121720663876,
    "itl": 179.59593216140408,
    "ttft": 2076715.1596763602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6062354800477165,
    "arrivals": 707393,
    "finished_requests": 77827,
    "scheduler_time": 96.0186908243588
}
#Debug simulation 
Total elapsed time: 6.077148439362645. Arrivals time: 0.3428756268694997 Scheduler time: 5.634705826174468 Scheduler overhead time: 0.032119082286953926 Adapter cache time: 0.020695066079497337 Engine time: 0.032147298101335764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.134948806837201,
    "estimated_duration": 3600.0975746544805,
    "input_throughput": 5351.272458733002,
    "output_throughput": 4748.4707415578005,
    "total_throughput": 10099.743200290803,
    "itl": 181.66948049181752,
    "ttft": 2075779.5942407846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1075,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.361806976976768,
    "arrivals": 707393,
    "finished_requests": 78072,
    "scheduler_time": 95.90469752185226
}
#Debug simulation 
Total elapsed time: 6.135039407759905. Arrivals time: 0.2713008029386401 Scheduler time: 5.7648281692527235 Scheduler overhead time: 0.03190894052386284 Adapter cache time: 0.020582607481628656 Engine time: 0.03186245774850249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.074670614209026,
    "estimated_duration": 3600.076587459108,
    "input_throughput": 5334.45123553726,
    "output_throughput": 4734.545109227793,
    "total_throughput": 10068.996344765052,
    "itl": 179.59803140446257,
    "ttft": 2076733.4669309498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6510038280114516,
    "arrivals": 707393,
    "finished_requests": 77827,
    "scheduler_time": 96.01874891219593
}
#Debug simulation 
Total elapsed time: 6.074794820975512. Arrivals time: 0.3431314346380532 Scheduler time: 5.6325141419656575 Scheduler overhead time: 0.03210977744311094 Adapter cache time: 0.020535429008305073 Engine time: 0.03192645590752363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.079361228272319,
    "estimated_duration": 3600.191183960519,
    "input_throughput": 5351.413859862193,
    "output_throughput": 4748.367552301486,
    "total_throughput": 10099.78141216368,
    "itl": 181.66270457348566,
    "ttft": 2075797.4346122183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2442045699687823,
    "arrivals": 707393,
    "finished_requests": 78072,
    "scheduler_time": 95.9101474848507
}
#Debug simulation 
Total elapsed time: 6.079453187063336. Arrivals time: 0.34253862546756864 Scheduler time: 5.638513069599867 Scheduler overhead time: 0.0317729520611465 Adapter cache time: 0.020398407708853483 Engine time: 0.03177246730774641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.102971084881574,
    "estimated_duration": 3600.1236712298532,
    "input_throughput": 5334.381469578653,
    "output_throughput": 4734.483189067025,
    "total_throughput": 10068.864658645678,
    "itl": 179.60027306724828,
    "ttft": 2076751.8657352643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.698035744130662,
    "arrivals": 707393,
    "finished_requests": 77827,
    "scheduler_time": 96.01880076686332
}
#Debug simulation 
Total elapsed time: 6.103081269655377. Arrivals time: 0.3516820170916617 Scheduler time: 5.652114141732454 Scheduler overhead time: 0.03207396389916539 Adapter cache time: 0.02064409665763378 Engine time: 0.03185806795954704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.790204932913184,
    "estimated_duration": 3600.1431829378257,
    "input_throughput": 5355.955310717708,
    "output_throughput": 4743.224680876507,
    "total_throughput": 10099.179991594214,
    "itl": 181.41783774186018,
    "ttft": 2077995.1270117445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.369597006070043,
    "arrivals": 702770,
    "finished_requests": 77922,
    "scheduler_time": 95.92783819435209
}
#Debug simulation 
Total elapsed time: 5.790305366739631. Arrivals time: 0.267717644572258 Scheduler time: 5.424259153660387 Scheduler overhead time: 0.031836656387895346 Adapter cache time: 0.020289919804781675 Engine time: 0.031723570078611374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.789744470734149,
    "estimated_duration": 3600.0143978311494,
    "input_throughput": 5355.754135765632,
    "output_throughput": 4743.287696373515,
    "total_throughput": 10099.041832139148,
    "itl": 181.42861067939114,
    "ttft": 2078003.10668772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1101,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.592771529059397,
    "arrivals": 702770,
    "finished_requests": 77919,
    "scheduler_time": 95.91884974503846
}
#Debug simulation 
Total elapsed time: 5.789839082863182. Arrivals time: 0.268328913487494 Scheduler time: 5.422683365177363 Scheduler overhead time: 0.03169280430302024 Adapter cache time: 0.020684837829321623 Engine time: 0.031833821441978216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.741624797228724,
    "estimated_duration": 3600.064101996357,
    "input_throughput": 5343.861791053038,
    "output_throughput": 4733.043500684101,
    "total_throughput": 10076.90529173714,
    "itl": 179.47534422659365,
    "ttft": 2079328.8721354029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6677637692167857,
    "arrivals": 702770,
    "finished_requests": 77737,
    "scheduler_time": 96.03760422229234
}
#Debug simulation 
Total elapsed time: 5.741747659165412. Arrivals time: 0.2664693552069366 Scheduler time: 5.375348469242454 Scheduler overhead time: 0.031939221546053886 Adapter cache time: 0.021274919155985117 Engine time: 0.032069888431578875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.733286241069436,
    "estimated_duration": 3600.040656049271,
    "input_throughput": 5356.107844948765,
    "output_throughput": 4743.359764925469,
    "total_throughput": 10099.467609874233,
    "itl": 181.42106506429843,
    "ttft": 2077982.2830635544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4486672647157315,
    "arrivals": 702770,
    "finished_requests": 77922,
    "scheduler_time": 95.92313579167028
}
#Debug simulation 
Total elapsed time: 5.733379852026701. Arrivals time: 0.26240012887865305 Scheduler time: 5.3729760576970875 Scheduler overhead time: 0.0315753067843616 Adapter cache time: 0.020345348864793777 Engine time: 0.031631463672965765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.7494169869460166,
    "estimated_duration": 3600.110690709192,
    "input_throughput": 5343.792636612022,
    "output_throughput": 4732.982250788352,
    "total_throughput": 10076.774887400374,
    "itl": 179.4774964288882,
    "ttft": 2079347.3744236012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7142926701903325,
    "arrivals": 702770,
    "finished_requests": 77737,
    "scheduler_time": 96.03766403419769
}
#Debug simulation 
Total elapsed time: 5.749527489300817. Arrivals time: 0.2806904399767518 Scheduler time: 5.369091067928821 Scheduler overhead time: 0.031991145107895136 Adapter cache time: 0.0209817704744637 Engine time: 0.032088952139019966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.776723149698228,
    "estimated_duration": 3600.019487419238,
    "input_throughput": 5356.139339629775,
    "output_throughput": 4743.387656560036,
    "total_throughput": 10099.52699618981,
    "itl": 181.413117377715,
    "ttft": 2077968.9931309444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3010155255719216,
    "arrivals": 702770,
    "finished_requests": 77922,
    "scheduler_time": 95.92624386608898
}
#Debug simulation 
Total elapsed time: 5.776820871978998. Arrivals time: 0.3071965607814491 Scheduler time: 5.3712478540837765 Scheduler overhead time: 0.03148490656167269 Adapter cache time: 0.020427885465323925 Engine time: 0.03204590128734708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.7662461791187525,
    "estimated_duration": 3600.158530129928,
    "input_throughput": 5343.721627532247,
    "output_throughput": 4732.919358244222,
    "total_throughput": 10076.640985776468,
    "itl": 179.47974289282803,
    "ttft": 2079365.9057622335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.762079109028035,
    "arrivals": 702770,
    "finished_requests": 77737,
    "scheduler_time": 96.03771701614046
}
#Debug simulation 
Total elapsed time: 5.766341011971235. Arrivals time: 0.2682094620540738 Scheduler time: 5.398130862507969 Scheduler overhead time: 0.03201017761602998 Adapter cache time: 0.0211676056496799 Engine time: 0.03214431740343571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.690979901235551,
    "estimated_duration": 3600.1331551898247,
    "input_throughput": 5381.187351937964,
    "output_throughput": 4744.361184357194,
    "total_throughput": 10125.548536295159,
    "itl": 181.26344237314115,
    "ttft": 2075323.561049984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1020,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.121697498811481,
    "arrivals": 700393,
    "finished_requests": 78235,
    "scheduler_time": 95.93851203491023
}
#Debug simulation 
Total elapsed time: 5.691105088219047. Arrivals time: 0.3409613589756191 Scheduler time: 5.2534874100238085 Scheduler overhead time: 0.03134608780965209 Adapter cache time: 0.01914811274036765 Engine time: 0.03167007863521576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.727360013406724,
    "estimated_duration": 3600.176638374289,
    "input_throughput": 5380.770430405209,
    "output_throughput": 4743.891679634974,
    "total_throughput": 10124.662110040183,
    "itl": 181.27192466591035,
    "ttft": 2075411.383443402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1021,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3364065020927303,
    "arrivals": 700393,
    "finished_requests": 78229,
    "scheduler_time": 95.93429090583591
}
#Debug simulation 
Total elapsed time: 5.7274537570774555. Arrivals time: 0.3395756739191711 Scheduler time: 5.290139867924154 Scheduler overhead time: 0.0318789747543633 Adapter cache time: 0.019299412611871958 Engine time: 0.03191348351538181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.728166463319212,
    "estimated_duration": 3600.107388505717,
    "input_throughput": 5369.656766828777,
    "output_throughput": 4733.463244570917,
    "total_throughput": 10103.120011399693,
    "itl": 179.54515987831044,
    "ttft": 2076846.162762116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3911822584643545,
    "arrivals": 700393,
    "finished_requests": 78063,
    "scheduler_time": 96.02790371560457
}
#Debug simulation 
Total elapsed time: 5.728274233173579. Arrivals time: 0.3426158158108592 Scheduler time: 5.286734219174832 Scheduler overhead time: 0.032435675617307425 Adapter cache time: 0.01993123022839427 Engine time: 0.03188054170459509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.734052497893572,
    "estimated_duration": 3600.0065637034754,
    "input_throughput": 5380.945744741031,
    "output_throughput": 4743.6627400012185,
    "total_throughput": 10124.60848474225,
    "itl": 181.265460882925,
    "ttft": 2075366.6210325796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1020,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1924510669358677,
    "arrivals": 700393,
    "finished_requests": 78225,
    "scheduler_time": 95.93326583211598
}
#Debug simulation 
Total elapsed time: 5.734151359181851. Arrivals time: 0.3388395211659372 Scheduler time: 5.298263077624142 Scheduler overhead time: 0.0316449785605073 Adapter cache time: 0.01922589633613825 Engine time: 0.03168216207996011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.718001309782267,
    "estimated_duration": 3600.008395221007,
    "input_throughput": 5369.397200756618,
    "output_throughput": 4732.9961848475,
    "total_throughput": 10102.393385604119,
    "itl": 179.5358492215353,
    "ttft": 2076812.3740555074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.441746122688054,
    "arrivals": 700393,
    "finished_requests": 78056,
    "scheduler_time": 96.02457285655291
}
#Debug simulation 
Total elapsed time: 5.71814423892647. Arrivals time: 0.34271389711648226 Scheduler time: 5.277067081537098 Scheduler overhead time: 0.031977507285773754 Adapter cache time: 0.019710455555468798 Engine time: 0.0320007735863328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.6815710249356925,
    "estimated_duration": 3600.0434958355922,
    "input_throughput": 5381.188872414865,
    "output_throughput": 4744.38212198201,
    "total_throughput": 10125.570994396876,
    "itl": 181.26081930148626,
    "ttft": 2075312.8516598153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1024,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.061811501979756,
    "arrivals": 700393,
    "finished_requests": 78233,
    "scheduler_time": 95.93752721482669
}
#Debug simulation 
Total elapsed time: 5.6816944177262485. Arrivals time: 0.3336002677679062 Scheduler time: 5.251335827168077 Scheduler overhead time: 0.031636434607207775 Adapter cache time: 0.0191575875505805 Engine time: 0.03145690308883786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.676018713973463,
    "estimated_duration": 3600.0532143626306,
    "input_throughput": 5369.330354029849,
    "output_throughput": 4732.937261044523,
    "total_throughput": 10102.267615074372,
    "itl": 179.53797073270144,
    "ttft": 2076830.003956557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.486514470651791,
    "arrivals": 700393,
    "finished_requests": 78056,
    "scheduler_time": 96.02462365025524
}
#Debug simulation 
Total elapsed time: 5.676115422975272. Arrivals time: 0.2639376977458596 Scheduler time: 5.314845911692828 Scheduler overhead time: 0.03176563046872616 Adapter cache time: 0.019389279186725616 Engine time: 0.03174220118671656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.583101645112038,
    "estimated_duration": 3600.0151086291767,
    "input_throughput": 5400.649834328983,
    "output_throughput": 4740.795659187884,
    "total_throughput": 10141.445493516867,
    "itl": 180.66877819580185,
    "ttft": 2069847.89214732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.020701403261697,
    "arrivals": 699127,
    "finished_requests": 78798,
    "scheduler_time": 95.94634709033548
}
#Debug simulation 
Total elapsed time: 5.583213119767606. Arrivals time: 0.2754462016746402 Scheduler time: 5.211642513051629 Scheduler overhead time: 0.03134091617539525 Adapter cache time: 0.018803379498422146 Engine time: 0.031664370093494654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.598522132728249,
    "estimated_duration": 3600.054638746231,
    "input_throughput": 5400.59025514432,
    "output_throughput": 4740.716103671072,
    "total_throughput": 10141.306358815393,
    "itl": 180.67825771098254,
    "ttft": 2069902.352097251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2286158622894514,
    "arrivals": 699127,
    "finished_requests": 78797,
    "scheduler_time": 95.94234199651271
}
#Debug simulation 
Total elapsed time: 5.5986143280752. Arrivals time: 0.3171351277269423 Scheduler time: 5.184853377752006 Scheduler overhead time: 0.03146146144717932 Adapter cache time: 0.018917137291282415 Engine time: 0.03182885982096195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.597344828769565,
    "estimated_duration": 3600.047497895339,
    "input_throughput": 5390.590266196589,
    "output_throughput": 4732.57311464931,
    "total_throughput": 10123.1633808459,
    "itl": 179.32860894974996,
    "ttft": 2071097.36379894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1014,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.321864111330322,
    "arrivals": 699127,
    "finished_requests": 78654,
    "scheduler_time": 96.01494708503617
}
#Debug simulation 
Total elapsed time: 5.597437949851155. Arrivals time: 0.2689624927006662 Scheduler time: 5.230900562833995 Scheduler overhead time: 0.03187301242724061 Adapter cache time: 0.019255835562944412 Engine time: 0.031948510091751814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.529795801267028,
    "estimated_duration": 3600.0858017209853,
    "input_throughput": 5400.543784458066,
    "output_throughput": 4740.702566544753,
    "total_throughput": 10141.24635100282,
    "itl": 180.67211794427485,
    "ttft": 2069876.414384641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.091327835293457,
    "arrivals": 699127,
    "finished_requests": 78798,
    "scheduler_time": 95.94644151622597
}
#Debug simulation 
Total elapsed time: 5.529888211283833. Arrivals time: 0.26221313094720244 Scheduler time: 5.1719936723820865 Scheduler overhead time: 0.031132827047258615 Adapter cache time: 0.018859886098653078 Engine time: 0.03141248878091574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.533170602284372,
    "estimated_duration": 3600.0917348604157,
    "input_throughput": 5390.524028064088,
    "output_throughput": 4732.514962055706,
    "total_throughput": 10123.038990119794,
    "itl": 179.33065778699293,
    "ttft": 2071115.9341663239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1014,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3657521827891475,
    "arrivals": 699127,
    "finished_requests": 78654,
    "scheduler_time": 96.01501505286792
}
#Debug simulation 
Total elapsed time: 5.533290697261691. Arrivals time: 0.2651925259269774 Scheduler time: 5.171462005469948 Scheduler overhead time: 0.03147814096882939 Adapter cache time: 0.019054079428315163 Engine time: 0.03169527277350426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.564879182726145,
    "estimated_duration": 3600.131054459413,
    "input_throughput": 5401.014214722724,
    "output_throughput": 4741.162680413314,
    "total_throughput": 10142.176895136037,
    "itl": 180.66765540003095,
    "ttft": 2069834.706461662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 989,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9571597416581836,
    "arrivals": 699127,
    "finished_requests": 78806,
    "scheduler_time": 95.95108960918253
}
#Debug simulation 
Total elapsed time: 5.564995750784874. Arrivals time: 0.2600311920978129 Scheduler time: 5.209624463226646 Scheduler overhead time: 0.031117388047277927 Adapter cache time: 0.01882682740688324 Engine time: 0.031195399817079306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.515192722901702,
    "estimated_duration": 3600.168523814126,
    "input_throughput": 5390.409052140787,
    "output_throughput": 4732.4140209831,
    "total_throughput": 10122.823073123887,
    "itl": 179.32630604194372,
    "ttft": 2071151.0459982692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.403215969055933,
    "arrivals": 699127,
    "finished_requests": 78654,
    "scheduler_time": 96.01664089389368
}
#Debug simulation 
Total elapsed time: 5.515283461194485. Arrivals time: 0.25629651127383113 Scheduler time: 5.162685415241867 Scheduler overhead time: 0.03148489352315664 Adapter cache time: 0.018989649135619402 Engine time: 0.03141749370843172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.446598347276449,
    "estimated_duration": 3600.1024981112,
    "input_throughput": 5376.949686892521,
    "output_throughput": 4741.71805079332,
    "total_throughput": 10118.66773768584,
    "itl": 181.05011167521448,
    "ttft": 2075637.7364152276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.990096525822368,
    "arrivals": 698509,
    "finished_requests": 78345,
    "scheduler_time": 95.94655997611852
}
#Debug simulation 
Total elapsed time: 5.4466950902715325. Arrivals time: 0.24349311692640185 Scheduler time: 5.109330573584884 Scheduler overhead time: 0.030620936304330826 Adapter cache time: 0.01809551240876317 Engine time: 0.031176269985735416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.52518252376467,
    "estimated_duration": 3600.0945647989724,
    "input_throughput": 5376.820706119671,
    "output_throughput": 4741.5693373468885,
    "total_throughput": 10118.39004346656,
    "itl": 181.06074056884603,
    "ttft": 2075645.605139649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.185640309150335,
    "arrivals": 698509,
    "finished_requests": 78341,
    "scheduler_time": 95.94135469115469
}
#Debug simulation 
Total elapsed time: 5.525275452062488. Arrivals time: 0.28131837118417025 Scheduler time: 5.149732068646699 Scheduler overhead time: 0.031016613356769085 Adapter cache time: 0.018157738726586103 Engine time: 0.03085806453600526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.434732781723142,
    "estimated_duration": 3600.1919796339284,
    "input_throughput": 5364.220049721817,
    "output_throughput": 4731.340188622944,
    "total_throughput": 10095.56023834476,
    "itl": 179.14672981905315,
    "ttft": 2077010.1697207883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1001,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.270494703985716,
    "arrivals": 698509,
    "finished_requests": 78158,
    "scheduler_time": 96.06133378978065
}
#Debug simulation 
Total elapsed time: 5.434823815710843. Arrivals time: 0.2392599917948246 Scheduler time: 5.100854471325874 Scheduler overhead time: 0.03098257863894105 Adapter cache time: 0.0186041253618896 Engine time: 0.030888899695128202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.7361690937541425,
    "estimated_duration": 3600.1616549408227,
    "input_throughput": 5376.86133438866,
    "output_throughput": 4741.640136234549,
    "total_throughput": 10118.501470623209,
    "itl": 181.05286387372274,
    "ttft": 2075662.8443779827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0491694727912204,
    "arrivals": 698509,
    "finished_requests": 78345,
    "scheduler_time": 95.94664385866871
}
#Debug simulation 
Total elapsed time: 5.7362334378995. Arrivals time: 0.5301572009921074 Scheduler time: 5.112534446641803 Scheduler overhead time: 0.03083033813163638 Adapter cache time: 0.018044530414044857 Engine time: 0.030594756826758385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.412991584744304,
    "estimated_duration": 3600.0375788875212,
    "input_throughput": 5364.216505197586,
    "output_throughput": 4731.225890498451,
    "total_throughput": 10095.442395696038,
    "itl": 179.14788420327147,
    "ttft": 2076898.9810641147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1001,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.313502498939637,
    "arrivals": 698509,
    "finished_requests": 78154,
    "scheduler_time": 96.05607180500984
}
#Debug simulation 
Total elapsed time: 5.413104954641312. Arrivals time: 0.24197778292000294 Scheduler time: 5.076791203580797 Scheduler overhead time: 0.030883115716278553 Adapter cache time: 0.01837461395189166 Engine time: 0.030965565238147974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.529731843154877,
    "estimated_duration": 3600.1932885598612,
    "input_throughput": 5377.044077470543,
    "output_throughput": 4741.9870633749,
    "total_throughput": 10119.031140845444,
    "itl": 181.04718068435167,
    "ttft": 2075636.1820891297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 973,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9093189369397505,
    "arrivals": 698509,
    "finished_requests": 78348,
    "scheduler_time": 95.95100224924953
}
#Debug simulation 
Total elapsed time: 5.529844041913748. Arrivals time: 0.2841704632155597 Scheduler time: 5.15163582470268 Scheduler overhead time: 0.03099015960469842 Adapter cache time: 0.01811815705150366 Engine time: 0.030718158930540085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.435569372959435,
    "estimated_duration": 3600.0781285672756,
    "input_throughput": 5364.1560850473425,
    "output_throughput": 4731.172600073118,
    "total_throughput": 10095.32868512046,
    "itl": 179.14977615465614,
    "ttft": 2076915.7084412505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1001,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3539952181652586,
    "arrivals": 698509,
    "finished_requests": 78154,
    "scheduler_time": 96.05612876557159
}
#Debug simulation 
Total elapsed time: 5.435662518255413. Arrivals time: 0.24052943056449294 Scheduler time: 5.10051689716056 Scheduler overhead time: 0.03104384057223797 Adapter cache time: 0.01848679454997182 Engine time: 0.03088987059891224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.590149996802211,
    "estimated_duration": 3600.1632890870833,
    "input_throughput": 5560.3250165567115,
    "output_throughput": 4946.1623182418025,
    "total_throughput": 10506.487334798514,
    "itl": 174.57030262825026,
    "ttft": 2040900.7707165498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.218911095671536,
    "arrivals": 650482,
    "finished_requests": 81311,
    "scheduler_time": 99.78432763574203
}
#Debug simulation 
Total elapsed time: 5.590251274872571. Arrivals time: 0.24171616416424513 Scheduler time: 5.240407805424184 Scheduler overhead time: 0.030729726888239384 Adapter cache time: 0.03236396284773946 Engine time: 0.030838595237582922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.623854630161077,
    "estimated_duration": 3600.15740744366,
    "input_throughput": 5560.099110836796,
    "output_throughput": 4945.631255785206,
    "total_throughput": 10505.730366622003,
    "itl": 174.5866521687263,
    "ttft": 2040998.9692331625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6118592570347285,
    "arrivals": 650482,
    "finished_requests": 81304,
    "scheduler_time": 99.77367732706392
}
#Debug simulation 
Total elapsed time: 5.6239435421302915. Arrivals time: 0.24187403032556176 Scheduler time: 5.274197875987738 Scheduler overhead time: 0.030691300053149462 Adapter cache time: 0.032216033432632685 Engine time: 0.03079993650317192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.8670111247338355,
    "estimated_duration": 3600.100254724846,
    "input_throughput": 5556.116103641339,
    "output_throughput": 4942.235143770983,
    "total_throughput": 10498.351247412322,
    "itl": 172.37696559746243,
    "ttft": 2041532.8023355692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.626899013519232,
    "arrivals": 650482,
    "finished_requests": 81246,
    "scheduler_time": 100.06854440968984
}
#Debug simulation 
Total elapsed time: 5.867096038069576. Arrivals time: 0.502907445654273 Scheduler time: 5.25337487179786 Scheduler overhead time: 0.031230162363499403 Adapter cache time: 0.03284345427528024 Engine time: 0.032297773752361536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.5954656773246825,
    "estimated_duration": 3600.088906570679,
    "input_throughput": 5560.276570799462,
    "output_throughput": 4945.9928524269135,
    "total_throughput": 10506.269423226377,
    "itl": 174.5750879372162,
    "ttft": 2040911.3887369952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.344637918774776,
    "arrivals": 650482,
    "finished_requests": 81307,
    "scheduler_time": 99.77896702384793
}
#Debug simulation 
Total elapsed time: 5.595546067226678. Arrivals time: 0.24046138999983668 Scheduler time: 5.24750259751454 Scheduler overhead time: 0.030625414568930864 Adapter cache time: 0.03219822561368346 Engine time: 0.03069747146219015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.598606224171817,
    "estimated_duration": 3600.1828125350125,
    "input_throughput": 5555.98869322847,
    "output_throughput": 4942.121810606517,
    "total_throughput": 10498.110503834987,
    "itl": 172.3807670873145,
    "ttft": 2041564.4119382938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.709393497407298,
    "arrivals": 650482,
    "finished_requests": 81246,
    "scheduler_time": 100.06860773604195
}
#Debug simulation 
Total elapsed time: 5.5986866359598935. Arrivals time: 0.24483647430315614 Scheduler time: 5.244244785048068 Scheduler overhead time: 0.03128678258508444 Adapter cache time: 0.03259772202000022 Engine time: 0.03136679995805025 
