INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 34.86993405595422,
    "estimated_duration": 3600.0200232831776,
    "input_throughput": 6340.434734355513,
    "output_throughput": 5562.175451940512,
    "total_throughput": 11902.610186296024,
    "itl": 153.77216866965622,
    "ttft": 1474708.5294899049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3239274077489969,
    "arrivals": 196098,
    "finished_requests": 91967,
    "scheduler_time": 141.70007470226395
}
#Debug simulation 
Total elapsed time: 34.87012147530913. Arrivals time: 0.3455088068731129 Scheduler time: 34.38513093069196 Scheduler overhead time: 0.056529171764850616 Adapter cache time: 0.009908740874379873 Engine time: 0.05254287691786885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 26.49822763679549,
    "estimated_duration": 3600.1229750409007,
    "input_throughput": 6323.93647601478,
    "output_throughput": 5564.957680306015,
    "total_throughput": 11888.894156320795,
    "itl": 154.0881139333443,
    "ttft": 1487182.243928445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2356575562828218,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73356098901965
}
#Debug simulation 
Total elapsed time: 26.498385197948664. Arrivals time: 0.31973485043272376 Scheduler time: 26.051859146449715 Scheduler overhead time: 0.05071659106761217 Adapter cache time: 0.008540772832930088 Engine time: 0.04847989184781909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.89342899993062,
    "estimated_duration": 3600.0811155636757,
    "input_throughput": 6324.010006767669,
    "output_throughput": 5565.022386131189,
    "total_throughput": 11889.032392898858,
    "itl": 154.08932922599578,
    "ttft": 1487178.4625600406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25107223394792527,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73182042168008
}
#Debug simulation 
Total elapsed time: 26.893606553785503. Arrivals time: 0.3396387309767306 Scheduler time: 26.423909295815974 Scheduler overhead time: 0.05204363400116563 Adapter cache time: 0.008736224379390478 Engine time: 0.05001636827364564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.929238917771727,
    "estimated_duration": 3600.0819070156736,
    "input_throughput": 6324.0086164797585,
    "output_throughput": 5565.021162701223,
    "total_throughput": 11889.02977918098,
    "itl": 154.0892926569376,
    "ttft": 1487179.0700395214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2515505735762416,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73186159140957
}
#Debug simulation 
Total elapsed time: 26.929362820927054. Arrivals time: 0.3293807930313051 Scheduler time: 26.47035346319899 Scheduler overhead time: 0.05144320102408528 Adapter cache time: 0.008712154813110828 Engine time: 0.05018870206549764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 26.892041434999555,
    "estimated_duration": 3600.1349364661187,
    "input_throughput": 6323.915464776431,
    "output_throughput": 5564.939190769842,
    "total_throughput": 11888.854655546273,
    "itl": 154.0878063408264,
    "ttft": 1487186.7092172583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.241265946305357,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73411891065004
}
#Debug simulation 
Total elapsed time: 26.89217655407265. Arrivals time: 0.33029611967504025 Scheduler time: 26.43179490044713 Scheduler overhead time: 0.05149740679189563 Adapter cache time: 0.008747739251703024 Engine time: 0.05051335459575057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 26.957222991157323,
    "estimated_duration": 3600.0888867360495,
    "input_throughput": 6323.996355723653,
    "output_throughput": 5565.010373442173,
    "total_throughput": 11889.006729165825,
    "itl": 154.0891805199887,
    "ttft": 1487181.6709386585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25456866445019827,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.7321890195718
}
#Debug simulation 
Total elapsed time: 26.957406184170395. Arrivals time: 0.33251862972974777 Scheduler time: 26.494038090109825 Scheduler overhead time: 0.05243542976677418 Adapter cache time: 0.00878342567011714 Engine time: 0.049876557663083076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 26.820234979037195,
    "estimated_duration": 3600.1081246221124,
    "input_throughput": 6323.962562204919,
    "output_throughput": 5564.980635714362,
    "total_throughput": 11888.94319791928,
    "itl": 154.08828758603622,
    "ttft": 1487177.6073933102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23023387270746776,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73294026810154
}
#Debug simulation 
Total elapsed time: 26.820361351128668. Arrivals time: 0.33949489798396826 Scheduler time: 26.35147381061688 Scheduler overhead time: 0.050987588707357645 Adapter cache time: 0.008841150905936956 Engine time: 0.04973347345367074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.796494570095092,
    "estimated_duration": 3600.1089338771117,
    "input_throughput": 6323.961140665067,
    "output_throughput": 5564.979384783213,
    "total_throughput": 11888.94052544828,
    "itl": 154.0891396839306,
    "ttft": 1487181.8216268613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25796401668339936,
    "arrivals": 192525,
    "finished_requests": 91639,
    "scheduler_time": 141.73303866604093
}
#Debug simulation 
Total elapsed time: 26.796722887083888. Arrivals time: 0.3245584601536393 Scheduler time: 26.342537120450288 Scheduler overhead time: 0.051750284153968096 Adapter cache time: 0.008745966013520956 Engine time: 0.04932146426290274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 21.910961292684078,
    "estimated_duration": 3600.1096514888245,
    "input_throughput": 6290.616451261249,
    "output_throughput": 5568.68760697586,
    "total_throughput": 11859.304058237109,
    "itl": 154.38509410671617,
    "ttft": 1479676.3896790047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26932292146608205,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36791755197984
}
#Debug simulation 
Total elapsed time: 21.91110794758424. Arrivals time: 0.3183321272954345 Scheduler time: 21.471244839951396 Scheduler overhead time: 0.04748161230236292 Adapter cache time: 0.008524836972355843 Engine time: 0.046629156451672316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.107158141210675,
    "estimated_duration": 3600.0554046388343,
    "input_throughput": 6290.711240393254,
    "output_throughput": 5568.771517840362,
    "total_throughput": 11859.482758233617,
    "itl": 154.38629757473853,
    "ttft": 1479662.2372223719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.287231549739372,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36573972488722
}
#Debug simulation 
Total elapsed time: 22.10726701002568. Arrivals time: 0.322819484397769 Scheduler time: 21.661942709237337 Scheduler overhead time: 0.049076908733695745 Adapter cache time: 0.007720919791609049 Engine time: 0.04696218203753233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.036599074024707,
    "estimated_duration": 3600.0533127426897,
    "input_throughput": 6290.714895759841,
    "output_throughput": 5568.774753706794,
    "total_throughput": 11859.489649466635,
    "itl": 154.38622797748764,
    "ttft": 1479660.106791452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28772726086899647,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.3656714668579
}
#Debug simulation 
Total elapsed time: 22.036756130401045. Arrivals time: 0.321555086877197 Scheduler time: 21.593767954036593 Scheduler overhead time: 0.04822957282885909 Adapter cache time: 0.008177478797733784 Engine time: 0.046400261111557484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 21.95993290375918,
    "estimated_duration": 3600.030101366999,
    "input_throughput": 6290.755455461482,
    "output_throughput": 5568.810658662948,
    "total_throughput": 11859.56611412443,
    "itl": 154.38680448680805,
    "ttft": 1479634.791375704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2761994761414827,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36472062581026
}
#Debug simulation 
Total elapsed time: 21.96005883673206. Arrivals time: 0.32134808553382754 Scheduler time: 21.514660419430584 Scheduler overhead time: 0.049041097052395344 Adapter cache time: 0.007972183171659708 Engine time: 0.04758768994361162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 21.870561433956027,
    "estimated_duration": 3600.060471890347,
    "input_throughput": 6290.70238592642,
    "output_throughput": 5568.763679537056,
    "total_throughput": 11859.466065463477,
    "itl": 154.38613458168206,
    "ttft": 1479663.7795480352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29112261310219756,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36597602470732
}
#Debug simulation 
Total elapsed time: 21.870689623057842. Arrivals time: 0.31440975377336144 Scheduler time: 21.433741569984704 Scheduler overhead time: 0.048447934444993734 Adapter cache time: 0.008274121675640345 Engine time: 0.04634712543338537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 22.907808425836265,
    "estimated_duration": 3600.0887232029286,
    "input_throughput": 6290.653020309868,
    "output_throughput": 5568.719979257563,
    "total_throughput": 11859.372999567431,
    "itl": 154.38511625708566,
    "ttft": 1479675.3710312329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26312442595139174,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36705901413956
}
#Debug simulation 
Total elapsed time: 22.907962743658572. Arrivals time: 0.3272297731600702 Scheduler time: 22.45902919769287 Scheduler overhead time: 0.04870185162872076 Adapter cache time: 0.007841147016733885 Engine time: 0.04648525035008788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.058080760296434,
    "estimated_duration": 3600.0681571749624,
    "input_throughput": 6290.6889567811495,
    "output_throughput": 5568.751791558284,
    "total_throughput": 11859.440748339433,
    "itl": 154.3857916577065,
    "ttft": 1479665.4739477432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2951467342674729,
    "arrivals": 191098,
    "finished_requests": 91850,
    "scheduler_time": 141.36624580081073
}
#Debug simulation 
Total elapsed time: 22.058188560418785. Arrivals time: 0.3266493733972311 Scheduler time: 21.607635957654566 Scheduler overhead time: 0.04984543053433299 Adapter cache time: 0.008513803128153086 Engine time: 0.04630116419866681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 20.139271514024585,
    "estimated_duration": 3600.1186881731674,
    "input_throughput": 6242.069761150907,
    "output_throughput": 5572.643498089861,
    "total_throughput": 11814.713259240769,
    "itl": 155.04892439105862,
    "ttft": 1487035.6980857393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 83,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2540204827464183,
    "arrivals": 190328,
    "finished_requests": 91197,
    "scheduler_time": 141.19713444494224
}
#Debug simulation 
Total elapsed time: 20.13945740321651. Arrivals time: 0.31536783976480365 Scheduler time: 19.703045011032373 Scheduler overhead time: 0.04830376571044326 Adapter cache time: 0.007976144086569548 Engine time: 0.046110411174595356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 20.082387779373676,
    "estimated_duration": 3600.049683403058,
    "input_throughput": 6242.140519226844,
    "output_throughput": 5572.507538572754,
    "total_throughput": 11814.648057799599,
    "itl": 155.0489678200624,
    "ttft": 1487016.1954331538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2724111813306809,
    "arrivals": 190328,
    "finished_requests": 91195,
    "scheduler_time": 141.1942434439322
}
#Debug simulation 
Total elapsed time: 20.082497101277113. Arrivals time: 0.311588391661644 Scheduler time: 19.651697874069214 Scheduler overhead time: 0.04783230507746339 Adapter cache time: 0.008034614380449057 Engine time: 0.04475871380418539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 20.1282664607279,
    "estimated_duration": 3600.053478746969,
    "input_throughput": 6242.133938471821,
    "output_throughput": 5572.501663775983,
    "total_throughput": 11814.635602247805,
    "itl": 155.04901690547348,
    "ttft": 1487017.082564463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27319245301187034,
    "arrivals": 190328,
    "finished_requests": 91195,
    "scheduler_time": 141.19435801916939
}
#Debug simulation 
Total elapsed time: 20.12843676796183. Arrivals time: 0.31532795215025544 Scheduler time: 19.692887477576733 Scheduler overhead time: 0.04807096626609564 Adapter cache time: 0.00754041550680995 Engine time: 0.045872692950069904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 20.419284593313932,
    "estimated_duration": 3600.171434054386,
    "input_throughput": 6241.9819199255735,
    "output_throughput": 5572.663515472171,
    "total_throughput": 11814.645435397744,
    "itl": 155.04950794179473,
    "ttft": 1487036.9344616402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26301348900655297,
    "arrivals": 190328,
    "finished_requests": 91198,
    "scheduler_time": 141.19934473896862
}
#Debug simulation 
Total elapsed time: 20.419368540402502. Arrivals time: 0.5347030088305473 Scheduler time: 19.76436521951109 Scheduler overhead time: 0.04773115925490856 Adapter cache time: 0.00782776065170765 Engine time: 0.046070782002061605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 20.26674326090142,
    "estimated_duration": 3600.0595652696156,
    "input_throughput": 6242.138384817814,
    "output_throughput": 5572.6197403896385,
    "total_throughput": 11814.758125207452,
    "itl": 155.04909564012584,
    "ttft": 1487002.4519888086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2760847900994121,
    "arrivals": 190328,
    "finished_requests": 91196,
    "scheduler_time": 141.19461235283757
}
#Debug simulation 
Total elapsed time: 20.266906073782593. Arrivals time: 0.3199477712623775 Scheduler time: 19.826565283350646 Scheduler overhead time: 0.04715439910069108 Adapter cache time: 0.007888342253863811 Engine time: 0.046586669981479645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 20.051710768137127,
    "estimated_duration": 3600.1056051740197,
    "input_throughput": 6242.058557311059,
    "output_throughput": 5572.548475013489,
    "total_throughput": 11814.60703232455,
    "itl": 155.04923614683653,
    "ttft": 1487048.0070269716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 83,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24817417447688084,
    "arrivals": 190328,
    "finished_requests": 91196,
    "scheduler_time": 141.19659586767222
}
#Debug simulation 
Total elapsed time: 20.051831791177392. Arrivals time: 0.31531435949727893 Scheduler time: 19.61808349052444 Scheduler overhead time: 0.046326511073857546 Adapter cache time: 0.008237013593316078 Engine time: 0.0449833613820374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 20.20329543715343,
    "estimated_duration": 3600.064508617397,
    "input_throughput": 6242.129813565587,
    "output_throughput": 5572.612088471911,
    "total_throughput": 11814.741902037498,
    "itl": 155.0486463105904,
    "ttft": 1487003.1979025253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2797316499054429,
    "arrivals": 190328,
    "finished_requests": 91196,
    "scheduler_time": 141.19490852015656
}
#Debug simulation 
Total elapsed time: 20.20342150190845. Arrivals time: 0.3146201428025961 Scheduler time: 19.769603989552706 Scheduler overhead time: 0.04748992249369621 Adapter cache time: 0.007380086462944746 Engine time: 0.04538491880521178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 15.661898178979754,
    "estimated_duration": 3600.0689867976193,
    "input_throughput": 6310.97822939503,
    "output_throughput": 5567.640529530437,
    "total_throughput": 11878.618758925466,
    "itl": 154.1440892737085,
    "ttft": 1478319.0888704066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20199219109956157,
    "arrivals": 188114,
    "finished_requests": 91572,
    "scheduler_time": 141.128527791039
}
#Debug simulation 
Total elapsed time: 15.662019130773842. Arrivals time: 0.3009441373869777 Scheduler time: 15.249556973110884 Scheduler overhead time: 0.043606894090771675 Adapter cache time: 0.007084939628839493 Engine time: 0.04257909441366792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 15.719008532818407,
    "estimated_duration": 3600.106278219321,
    "input_throughput": 6310.912857616445,
    "output_throughput": 5567.582857557771,
    "total_throughput": 11878.495715174216,
    "itl": 154.14396703399663,
    "ttft": 1478326.8548328888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21409572751959785,
    "arrivals": 188114,
    "finished_requests": 91572,
    "scheduler_time": 141.13009715869302
}
#Debug simulation 
Total elapsed time: 15.71921277185902. Arrivals time: 0.29582251561805606 Scheduler time: 15.30869225366041 Scheduler overhead time: 0.044740989338606596 Adapter cache time: 0.0073270793072879314 Engine time: 0.044254946522414684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 15.654190444387496,
    "estimated_duration": 3600.110035852741,
    "input_throughput": 6310.965990965435,
    "output_throughput": 5567.57760190302,
    "total_throughput": 11878.543592868455,
    "itl": 154.143159091447,
    "ttft": 1478330.972984376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469939129427065,
    "arrivals": 188114,
    "finished_requests": 91573,
    "scheduler_time": 141.13024682887377
}
#Debug simulation 
Total elapsed time: 15.654296140186489. Arrivals time: 0.2999655092135072 Scheduler time: 15.241347976960242 Scheduler overhead time: 0.04432318406179547 Adapter cache time: 0.007117525674402714 Engine time: 0.04306796658784151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 15.783188488334417,
    "estimated_duration": 3600.0892192138117,
    "input_throughput": 6310.942761846772,
    "output_throughput": 5567.609239522455,
    "total_throughput": 11878.552001369228,
    "itl": 154.14444108386516,
    "ttft": 1478320.6300831868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20633241646923134,
    "arrivals": 188114,
    "finished_requests": 91572,
    "scheduler_time": 141.12933866362866
}
#Debug simulation 
Total elapsed time: 15.783348691184074. Arrivals time: 0.3073935811407864 Scheduler time: 15.362021102569997 Scheduler overhead time: 0.04483992885798216 Adapter cache time: 0.007196541875600815 Engine time: 0.04350174032151699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 15.65919325593859,
    "estimated_duration": 3600.1184395605123,
    "input_throughput": 6310.951259362896,
    "output_throughput": 5567.564605581942,
    "total_throughput": 11878.515864944839,
    "itl": 154.14303752038603,
    "ttft": 1478333.8578831258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21708871323615295,
    "arrivals": 188114,
    "finished_requests": 91573,
    "scheduler_time": 141.13058499074063
}
#Debug simulation 
Total elapsed time: 15.659299429971725. Arrivals time: 0.3102936213836074 Scheduler time: 15.236818974372 Scheduler overhead time: 0.043883275240659714 Adapter cache time: 0.007109265774488449 Engine time: 0.042852410580962896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 15.643819560296834,
    "estimated_duration": 3600.056626921928,
    "input_throughput": 6310.999896528215,
    "output_throughput": 5567.659644603329,
    "total_throughput": 11878.659541131543,
    "itl": 154.14436685387844,
    "ttft": 1478315.607995865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19734331946354378,
    "arrivals": 188114,
    "finished_requests": 91572,
    "scheduler_time": 141.12797555812372
}
#Debug simulation 
Total elapsed time: 15.643935919273645. Arrivals time: 0.3033242542296648 Scheduler time: 15.228554472792894 Scheduler overhead time: 0.043810497503727674 Adapter cache time: 0.007075676694512367 Engine time: 0.04282995406538248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 15.710357611998916,
    "estimated_duration": 3600.1729725607693,
    "input_throughput": 6310.930939476267,
    "output_throughput": 5567.623598302445,
    "total_throughput": 11878.554537778711,
    "itl": 154.14390830679554,
    "ttft": 1478327.7562068284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 66,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2198552965372798,
    "arrivals": 188114,
    "finished_requests": 91574,
    "scheduler_time": 141.13296243527438
}
#Debug simulation 
Total elapsed time: 15.710481626912951. Arrivals time: 0.2983456146903336 Scheduler time: 15.298083856701851 Scheduler overhead time: 0.04498446732759476 Adapter cache time: 0.007182392757385969 Engine time: 0.043515614699572325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 13.951269717887044,
    "estimated_duration": 3600.0518930601284,
    "input_throughput": 6292.596238312512,
    "output_throughput": 5573.171886404498,
    "total_throughput": 11865.76812471701,
    "itl": 154.58306103117013,
    "ttft": 1481923.347585172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21423414207529257,
    "arrivals": 187444,
    "finished_requests": 91532,
    "scheduler_time": 140.9099935397445
}
#Debug simulation 
Total elapsed time: 13.951371785253286. Arrivals time: 0.28932161536067724 Scheduler time: 13.551738919690251 Scheduler overhead time: 0.04312681593000889 Adapter cache time: 0.00700351083651185 Engine time: 0.04195609921589494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.040043514687568,
    "estimated_duration": 3600.1266560103727,
    "input_throughput": 6292.502504649306,
    "output_throughput": 5573.075871234623,
    "total_throughput": 11865.57837588393,
    "itl": 154.58282381353956,
    "ttft": 1481949.809539036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2280989052914083,
    "arrivals": 187444,
    "finished_requests": 91533,
    "scheduler_time": 140.9131295756004
}
#Debug simulation 
Total elapsed time: 14.040188936050981. Arrivals time: 0.292058196850121 Scheduler time: 13.637185864150524 Scheduler overhead time: 0.04302856791764498 Adapter cache time: 0.006975866854190826 Engine time: 0.04261940810829401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.102724908851087,
    "estimated_duration": 3600.1291028301766,
    "input_throughput": 6292.498227963858,
    "output_throughput": 5573.0720835059,
    "total_throughput": 11865.570311469759,
    "itl": 154.58285830237384,
    "ttft": 1481949.802931581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2285597041621803,
    "arrivals": 187444,
    "finished_requests": 91533,
    "scheduler_time": 140.91318096522807
}
#Debug simulation 
Total elapsed time: 14.10288114566356. Arrivals time: 0.2922769160941243 Scheduler time: 13.699036656413227 Scheduler overhead time: 0.04380537150427699 Adapter cache time: 0.0072108907625079155 Engine time: 0.04181448882445693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 13.911073618102819,
    "estimated_duration": 3600.107856575282,
    "input_throughput": 6292.535363523847,
    "output_throughput": 5573.104973328858,
    "total_throughput": 11865.640336852706,
    "itl": 154.5836402805965,
    "ttft": 1481944.4151582026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21910980828572074,
    "arrivals": 187444,
    "finished_requests": 91533,
    "scheduler_time": 140.9122900768366
}
#Debug simulation 
Total elapsed time: 13.911186897195876. Arrivals time: 0.29099793126806617 Scheduler time: 13.510251740925014 Scheduler overhead time: 0.043123151175677776 Adapter cache time: 0.007001979276537895 Engine time: 0.04164746217429638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 14.063921357970685,
    "estimated_duration": 3600.136419540216,
    "input_throughput": 6292.485439452648,
    "output_throughput": 5573.060757114978,
    "total_throughput": 11865.546196567626,
    "itl": 154.58271517590484,
    "ttft": 1481953.864717721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23132628746330716,
    "arrivals": 187444,
    "finished_requests": 91533,
    "scheduler_time": 140.91355545244573
}
#Debug simulation 
Total elapsed time: 14.064046925865114. Arrivals time: 0.3526899302378297 Scheduler time: 13.601870600599796 Scheduler overhead time: 0.04239823156967759 Adapter cache time: 0.007021448574960232 Engine time: 0.04202732676640153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 13.924477694090456,
    "estimated_duration": 3600.0433712859103,
    "input_throughput": 6292.611133712055,
    "output_throughput": 5573.185078832365,
    "total_throughput": 11865.79621254442,
    "itl": 154.58337589204987,
    "ttft": 1481921.0602374799,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2093035206431525,
    "arrivals": 187444,
    "finished_requests": 91532,
    "scheduler_time": 140.90966501396082
}
#Debug simulation 
Total elapsed time: 13.924575598910451. Arrivals time: 0.29025029484182596 Scheduler time: 13.524379946291447 Scheduler overhead time: 0.04311591014266014 Adapter cache time: 0.006863264832645655 Engine time: 0.04180995933711529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.120722423773259,
    "estimated_duration": 3600.166222573346,
    "input_throughput": 6292.472513618354,
    "output_throughput": 5573.06823062702,
    "total_throughput": 11865.540744245374,
    "itl": 154.58345875129964,
    "ttft": 1481954.1185196459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 70,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23434437833726376,
    "arrivals": 187444,
    "finished_requests": 91534,
    "scheduler_time": 140.91485032803118
}
#Debug simulation 
Total elapsed time: 14.120862389914691. Arrivals time: 0.2918736436404288 Scheduler time: 13.717194048687816 Scheduler overhead time: 0.04365036869421601 Adapter cache time: 0.007003906648606062 Engine time: 0.04271016037091613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.799107905942947,
    "estimated_duration": 3600.0138814359343,
    "input_throughput": 6284.223823874887,
    "output_throughput": 5571.104629185557,
    "total_throughput": 11855.328453060443,
    "itl": 154.57549272292135,
    "ttft": 1481035.4998513786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22035511756315806,
    "arrivals": 185958,
    "finished_requests": 91583,
    "scheduler_time": 140.7918257001392
}
#Debug simulation 
Total elapsed time: 10.799213835969567. Arrivals time: 0.28417354822158813 Scheduler time: 10.409342992119491 Scheduler overhead time: 0.040741012431681156 Adapter cache time: 0.006718276534229517 Engine time: 0.040421025827527046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.850699638947845,
    "estimated_duration": 3600.0569316622855,
    "input_throughput": 6284.303673373769,
    "output_throughput": 5571.078008130076,
    "total_throughput": 11855.381681503844,
    "itl": 154.57512749711637,
    "ttft": 1481044.860595257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23407900588121264,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79367991035926
}
#Debug simulation 
Total elapsed time: 10.850819039158523. Arrivals time: 0.2877298966050148 Scheduler time: 10.458096738439053 Scheduler overhead time: 0.04021526547148824 Adapter cache time: 0.0066826059482991695 Engine time: 0.04025251558050513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.869400549214333,
    "estimated_duration": 3600.0604064342747,
    "input_throughput": 6284.297607774887,
    "output_throughput": 5571.072630935356,
    "total_throughput": 11855.370238710242,
    "itl": 154.5752098572404,
    "ttft": 1481044.9861738419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23464674185961468,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79380463767058
}
#Debug simulation 
Total elapsed time: 10.869558482896537. Arrivals time: 0.2839641598984599 Scheduler time: 10.47973487759009 Scheduler overhead time: 0.040518681053072214 Adapter cache time: 0.006679643411189318 Engine time: 0.04061502777040005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.861010344233364,
    "estimated_duration": 3600.03588269656,
    "input_throughput": 6284.34041692215,
    "output_throughput": 5571.110581535972,
    "total_throughput": 11855.450998458124,
    "itl": 154.57570230455585,
    "ttft": 1481038.6477478368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2250899088755251,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79274790996516
}
#Debug simulation 
Total elapsed time: 10.861113594379276. Arrivals time: 0.2814962239935994 Scheduler time: 10.471859506797045 Scheduler overhead time: 0.04264117777347565 Adapter cache time: 0.00672692758962512 Engine time: 0.040457616560161114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 10.947607005015016,
    "estimated_duration": 3600.0630330524973,
    "input_throughput": 6284.293022729998,
    "output_throughput": 5571.0685662618325,
    "total_throughput": 11855.36158899183,
    "itl": 154.57468752078387,
    "ttft": 1481045.3248055435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23741332516074157,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79392375855915
}
#Debug simulation 
Total elapsed time: 10.94771222025156. Arrivals time: 0.2840396580286324 Scheduler time: 10.557274689432234 Scheduler overhead time: 0.04106439929455519 Adapter cache time: 0.006705532781779766 Engine time: 0.040612636134028435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.79615651583299,
    "estimated_duration": 3600.1242973221692,
    "input_throughput": 6284.186081249469,
    "output_throughput": 5570.973761910977,
    "total_throughput": 11855.159843160447,
    "itl": 154.57453409230413,
    "ttft": 1481057.599632627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21528362123295686,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79649946078314
}
#Debug simulation 
Total elapsed time: 10.796279715839773. Arrivals time: 0.2734080902300775 Scheduler time: 10.418299723882228 Scheduler overhead time: 0.039842247031629086 Adapter cache time: 0.006602051202207804 Engine time: 0.040261590387672186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.883573114871979,
    "estimated_duration": 3600.095416135755,
    "input_throughput": 6284.236495121519,
    "output_throughput": 5571.018454151913,
    "total_throughput": 11855.25494927343,
    "itl": 154.57515776580854,
    "ttft": 1481064.1283191035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 72,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24043141603469814,
    "arrivals": 185958,
    "finished_requests": 91584,
    "scheduler_time": 140.79533895077446
}
#Debug simulation 
Total elapsed time: 10.883670802693814. Arrivals time: 0.27611223654821515 Scheduler time: 10.500903544947505 Scheduler overhead time: 0.04132285388186574 Adapter cache time: 0.0068223681300878525 Engine time: 0.0405860198661685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.67805302608758,
    "estimated_duration": 3600.1169996777867,
    "input_throughput": 6233.110202254091,
    "output_throughput": 5482.6420924004915,
    "total_throughput": 11715.752294654583,
    "itl": 145.14624371620846,
    "ttft": 1135159.7064290976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8661180315329682,
    "arrivals": 126417,
    "finished_requests": 90464,
    "scheduler_time": 125.1776519221504
}
#Debug simulation 
Total elapsed time: 112.67822274193168. Arrivals time: 0.39178500324487686 Scheduler time: 112.1154328873381 Scheduler overhead time: 0.06808645883575082 Adapter cache time: 0.014715169090777636 Engine time: 0.06406119745224714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 110.2904500211589,
    "estimated_duration": 3600.1418029724136,
    "input_throughput": 6239.787272115996,
    "output_throughput": 5485.798915946571,
    "total_throughput": 11725.586188062567,
    "itl": 145.00222437203934,
    "ttft": 1144525.6022910583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9243558223452468,
    "arrivals": 126417,
    "finished_requests": 90520,
    "scheduler_time": 125.33246230741496
}
#Debug simulation 
Total elapsed time: 110.29061257792637. Arrivals time: 0.3886129749007523 Scheduler time: 109.7289237040095 Scheduler overhead time: 0.0695407991297543 Adapter cache time: 0.01510734437033534 Engine time: 0.06492634117603302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 110.0372485825792,
    "estimated_duration": 3600.141196014161,
    "input_throughput": 6239.788324099842,
    "output_throughput": 5485.799840813332,
    "total_throughput": 11725.588164913173,
    "itl": 145.0018808551511,
    "ttft": 1144524.8783541427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9264128920435952,
    "arrivals": 126417,
    "finished_requests": 90520,
    "scheduler_time": 125.33232797431572
}
#Debug simulation 
Total elapsed time: 110.03741642460227. Arrivals time: 0.3909863270819187 Scheduler time: 109.47451360384002 Scheduler overhead time: 0.06911364989355206 Adapter cache time: 0.014413109049201012 Engine time: 0.06421823939308524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.36715488694608,
    "estimated_duration": 3600.153506948223,
    "input_throughput": 6233.961678768709,
    "output_throughput": 5482.639549092948,
    "total_throughput": 11716.601227861658,
    "itl": 145.14018835552645,
    "ttft": 1135270.982220778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8825492167985095,
    "arrivals": 126417,
    "finished_requests": 90468,
    "scheduler_time": 125.1822339664536
}
#Debug simulation 
Total elapsed time: 111.36732840491459. Arrivals time: 0.39143230812624097 Scheduler time: 110.80154487816617 Scheduler overhead time: 0.06996770156547427 Adapter cache time: 0.015018978621810675 Engine time: 0.06543958093971014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 110.75845087319613,
    "estimated_duration": 3600.101982037361,
    "input_throughput": 6240.104617060502,
    "output_throughput": 5486.1795856190365,
    "total_throughput": 11726.284202679539,
    "itl": 144.99882289155,
    "ttft": 1144498.1198540425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 285,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9418660218827476,
    "arrivals": 126417,
    "finished_requests": 90526,
    "scheduler_time": 125.33653879833999
}
#Debug simulation 
Total elapsed time: 110.75891332421452. Arrivals time: 0.38827696815133095 Scheduler time: 110.19936348591 Scheduler overhead time: 0.06781876413151622 Adapter cache time: 0.015250407624989748 Engine time: 0.06380563229322433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 57.274571161717176,
    "estimated_duration": 3600.1085263625873,
    "input_throughput": 6219.266123797117,
    "output_throughput": 5471.159231941506,
    "total_throughput": 11690.425355738624,
    "itl": 143.6754546615904,
    "ttft": 1183632.0550859154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.169109665306746,
    "arrivals": 126417,
    "finished_requests": 90296,
    "scheduler_time": 125.3097722935402
}
#Debug simulation 
Total elapsed time: 57.27474478399381. Arrivals time: 0.35501658357679844 Scheduler time: 56.765696512535214 Scheduler overhead time: 0.05970199778676033 Adapter cache time: 0.014680216554552317 Engine time: 0.05743643594905734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 57.17338203918189,
    "estimated_duration": 3600.0339390983568,
    "input_throughput": 6219.034980988916,
    "output_throughput": 5471.132031864402,
    "total_throughput": 11690.167012853317,
    "itl": 143.6827848054384,
    "ttft": 1183679.7474008491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3053031888231665,
    "arrivals": 126417,
    "finished_requests": 90293,
    "scheduler_time": 125.30476015441576
}
#Debug simulation 
Total elapsed time: 57.17355959303677. Arrivals time: 0.3658424778841436 Scheduler time: 56.649275452364236 Scheduler overhead time: 0.062175564002245665 Adapter cache time: 0.01515641063451767 Engine time: 0.058134866412729025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 60.964917024131864,
    "estimated_duration": 3600.0433789483013,
    "input_throughput": 6251.269674026665,
    "output_throughput": 5472.522668811686,
    "total_throughput": 11723.792342838351,
    "itl": 142.8190069093597,
    "ttft": 1093239.7066762443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0344448574492706,
    "arrivals": 120741,
    "finished_requests": 90377,
    "scheduler_time": 120.40121877507043
}
#Debug simulation 
Total elapsed time: 60.96509771514684. Arrivals time: 0.3616890599951148 Scheduler time: 60.44085832312703 Scheduler overhead time: 0.06379563407972455 Adapter cache time: 0.014238729141652584 Engine time: 0.06132413959130645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 52.99365762900561,
    "estimated_duration": 3600.040658337883,
    "input_throughput": 6256.878501588831,
    "output_throughput": 5451.8192605806735,
    "total_throughput": 11708.697762169504,
    "itl": 141.33744459881729,
    "ttft": 1138967.4853298655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9090700750937741,
    "arrivals": 120741,
    "finished_requests": 90264,
    "scheduler_time": 121.05720646201524
}
#Debug simulation 
Total elapsed time: 52.99382338905707. Arrivals time: 0.33630930120125413 Scheduler time: 52.50468564312905 Scheduler overhead time: 0.05816377466544509 Adapter cache time: 0.01667122496291995 Engine time: 0.05579673079773784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 56.900285355746746,
    "estimated_duration": 3600.1056994187093,
    "input_throughput": 6273.716353285642,
    "output_throughput": 5475.03730326101,
    "total_throughput": 11748.75365654665,
    "itl": 142.55297565109976,
    "ttft": 1140124.4951833866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4903761247731833,
    "arrivals": 120741,
    "finished_requests": 90570,
    "scheduler_time": 120.46457286957182
}
#Debug simulation 
Total elapsed time: 56.900459453929216. Arrivals time: 0.35504405479878187 Scheduler time: 56.38663764530793 Scheduler overhead time: 0.061809164471924305 Adapter cache time: 0.01618722779676318 Engine time: 0.05828271247446537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 62.39187290985137,
    "estimated_duration": 3600.097324127953,
    "input_throughput": 6245.073945451182,
    "output_throughput": 5468.016897228846,
    "total_throughput": 11713.090842680029,
    "itl": 142.87081899564126,
    "ttft": 1090822.158271016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.07474857243011,
    "arrivals": 120741,
    "finished_requests": 90328,
    "scheduler_time": 120.38484211266699
}
#Debug simulation 
Total elapsed time: 62.39205554081127. Arrivals time: 0.36289749341085553 Scheduler time: 61.86786602484062 Scheduler overhead time: 0.06355401035398245 Adapter cache time: 0.014615789521485567 Engine time: 0.06032580463215709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 56.93493781192228,
    "estimated_duration": 3600.120824952237,
    "input_throughput": 6273.689994918337,
    "output_throughput": 5475.014300460736,
    "total_throughput": 11748.704295379073,
    "itl": 142.5530193524595,
    "ttft": 1140129.9103408495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5092391927354083,
    "arrivals": 120741,
    "finished_requests": 90570,
    "scheduler_time": 120.46504704747984
}
#Debug simulation 
Total elapsed time: 56.93511513294652. Arrivals time: 0.35854965820908546 Scheduler time: 56.418378920294344 Scheduler overhead time: 0.06115121953189373 Adapter cache time: 0.01545200077816844 Engine time: 0.05918462947010994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 64.0835445499979,
    "estimated_duration": 3600.044028362036,
    "input_throughput": 6263.714505252798,
    "output_throughput": 5480.677692982869,
    "total_throughput": 11744.392198235668,
    "itl": 142.69989872139976,
    "ttft": 1107341.0133994091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.339542532116164,
    "arrivals": 120741,
    "finished_requests": 90513,
    "scheduler_time": 120.40090478919906
}
#Debug simulation 
Total elapsed time: 64.08381525613368. Arrivals time: 0.3528528609313071 Scheduler time: 63.5716602858156 Scheduler overhead time: 0.062099517323076725 Adapter cache time: 0.01570953382179141 Engine time: 0.05868311459198594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 57.59108192194253,
    "estimated_duration": 3600.0191980902505,
    "input_throughput": 6251.468606594859,
    "output_throughput": 5471.415544241829,
    "total_throughput": 11722.884150836688,
    "itl": 142.8839749785444,
    "ttft": 1104606.1020830974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.360944509617986,
    "arrivals": 120741,
    "finished_requests": 90323,
    "scheduler_time": 120.39766880646387
}
#Debug simulation 
Total elapsed time: 57.59126714710146. Arrivals time: 0.3614261783659458 Scheduler time: 57.0705242860131 Scheduler overhead time: 0.061902280896902084 Adapter cache time: 0.015463657211512327 Engine time: 0.05958300130441785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 79.02129176817834,
    "estimated_duration": 3600.050787061578,
    "input_throughput": 6164.984694039644,
    "output_throughput": 5477.688556748323,
    "total_throughput": 11642.673250787966,
    "itl": 143.67292448499106,
    "ttft": 1121573.491936739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2180741220852422,
    "arrivals": 117840,
    "finished_requests": 89883,
    "scheduler_time": 117.96026759237054
}
#Debug simulation 
Total elapsed time: 79.02145437244326. Arrivals time: 0.36142692249268293 Scheduler time: 78.49819872295484 Scheduler overhead time: 0.06268849270418286 Adapter cache time: 0.015443635173141956 Engine time: 0.06057152058929205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 77.34158317511901,
    "estimated_duration": 3600.1510395561445,
    "input_throughput": 6165.207724933802,
    "output_throughput": 5483.064400107415,
    "total_throughput": 11648.272125041218,
    "itl": 144.1171465554932,
    "ttft": 1124799.2657802873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3386929287994338,
    "arrivals": 117840,
    "finished_requests": 89904,
    "scheduler_time": 117.74691451429578
}
#Debug simulation 
Total elapsed time: 77.34175200900063. Arrivals time: 0.36476256465539336 Scheduler time: 76.81788093503565 Scheduler overhead time: 0.062449948862195015 Adapter cache time: 0.015182015486061573 Engine time: 0.059369240421801805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 76.65805480303243,
    "estimated_duration": 3600.0287417943296,
    "input_throughput": 6166.351602219579,
    "output_throughput": 5483.409554713987,
    "total_throughput": 11649.761156933566,
    "itl": 144.13055852470143,
    "ttft": 1124864.9014191374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.367294672243305,
    "arrivals": 117840,
    "finished_requests": 89899,
    "scheduler_time": 117.75541923032763
}
#Debug simulation 
Total elapsed time: 76.65821478096768. Arrivals time: 0.3668569172732532 Scheduler time: 76.13152798172086 Scheduler overhead time: 0.06210468430072069 Adapter cache time: 0.015431274194270372 Engine time: 0.05954418797045946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 76.45356164686382,
    "estimated_duration": 3600.115197746745,
    "input_throughput": 6169.064538240465,
    "output_throughput": 5487.484403933714,
    "total_throughput": 11656.548942174179,
    "itl": 144.5523374513405,
    "ttft": 1123996.364933888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3100344181456587,
    "arrivals": 117840,
    "finished_requests": 89965,
    "scheduler_time": 117.54858683638511
}
#Debug simulation 
Total elapsed time: 76.4537252439186. Arrivals time: 0.3681411133147776 Scheduler time: 75.92477869801223 Scheduler overhead time: 0.06276607001200318 Adapter cache time: 0.015793560538440943 Engine time: 0.05956008331850171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 107.36881204415113,
    "estimated_duration": 3600.0315098864626,
    "input_throughput": 6165.180204408816,
    "output_throughput": 5483.016175217457,
    "total_throughput": 11648.196379626273,
    "itl": 144.11855247465863,
    "ttft": 1124948.701348741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.358065565302973,
    "arrivals": 117840,
    "finished_requests": 89898,
    "scheduler_time": 117.74129792387009
}
#Debug simulation 
Total elapsed time: 107.36900695739314. Arrivals time: 0.6907348264940083 Scheduler time: 106.42650302732363 Scheduler overhead time: 0.09888536995276809 Adapter cache time: 0.02507269475609064 Engine time: 0.10009513888508081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.03125236742198,
    "estimated_duration": 3600.1695984648168,
    "input_throughput": 6168.487731652908,
    "output_throughput": 5478.781613069266,
    "total_throughput": 11647.269344722174,
    "itl": 143.72286148578635,
    "ttft": 1121594.7303720291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2139604197302771,
    "arrivals": 117840,
    "finished_requests": 89901,
    "scheduler_time": 117.9124220169303
}
#Debug simulation 
Total elapsed time: 110.03147029317915. Arrivals time: 0.6633708206936717 Scheduler time: 109.10699638724327 Scheduler overhead time: 0.10098700691014528 Adapter cache time: 0.025905923917889595 Engine time: 0.10494481958448887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 105.63165642227978,
    "estimated_duration": 3600.0932666454196,
    "input_throughput": 6165.2838846261175,
    "output_throughput": 5480.824672741289,
    "total_throughput": 11646.108557367406,
    "itl": 144.18945922136672,
    "ttft": 1124222.5905683143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3585614821687362,
    "arrivals": 117840,
    "finished_requests": 89894,
    "scheduler_time": 117.81556049368835
}
#Debug simulation 
Total elapsed time: 105.63199784699827. Arrivals time: 0.706866801250726 Scheduler time: 104.67221312783659 Scheduler overhead time: 0.10022051120176911 Adapter cache time: 0.025446058250963688 Engine time: 0.0991212441585958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 84.43796818517148,
    "estimated_duration": 3600.082729065703,
    "input_throughput": 6122.574301430096,
    "output_throughput": 5482.421512330693,
    "total_throughput": 11604.995813760788,
    "itl": 144.6795267715074,
    "ttft": 1125577.8101574073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2639814382442351,
    "arrivals": 116430,
    "finished_requests": 89615,
    "scheduler_time": 115.51841076757971
}
#Debug simulation 
Total elapsed time: 84.43817952508107. Arrivals time: 0.6300209132023156 Scheduler time: 83.5756071023643 Scheduler overhead time: 0.08911760058254004 Adapter cache time: 0.023703590035438538 Engine time: 0.09235358191654086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 84.47378194890916,
    "estimated_duration": 3600.117100284076,
    "input_throughput": 6121.9314222476005,
    "output_throughput": 5482.5174987898445,
    "total_throughput": 11604.448921037445,
    "itl": 144.68617751256787,
    "ttft": 1125583.407394947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3472544843657002,
    "arrivals": 116430,
    "finished_requests": 89615,
    "scheduler_time": 115.5164387339803
}
#Debug simulation 
Total elapsed time: 84.47397987404838. Arrivals time: 0.6293091382831335 Scheduler time: 83.61371216131374 Scheduler overhead time: 0.089580322150141 Adapter cache time: 0.023877921514213085 Engine time: 0.0903803389519453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 84.5093126441352,
    "estimated_duration": 3600.118368926724,
    "input_throughput": 6121.929264945396,
    "output_throughput": 5482.515566810169,
    "total_throughput": 11604.444831755565,
    "itl": 144.68614550135987,
    "ttft": 1125583.57316866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3497163455374617,
    "arrivals": 116430,
    "finished_requests": 89615,
    "scheduler_time": 115.5164411522135
}
#Debug simulation 
Total elapsed time: 84.50956022879109. Arrivals time: 0.6297228033654392 Scheduler time: 83.64065378252417 Scheduler overhead time: 0.09194826567545533 Adapter cache time: 0.023591664154082537 Engine time: 0.09609124576672912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 84.44210024783388,
    "estimated_duration": 3600.039790909929,
    "input_throughput": 6115.367684432024,
    "output_throughput": 5480.583311834189,
    "total_throughput": 11595.950996266214,
    "itl": 144.64151234605174,
    "ttft": 1126029.2779478023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3206389503041267,
    "arrivals": 116430,
    "finished_requests": 89533,
    "scheduler_time": 115.63335459309302
}
#Debug simulation 
Total elapsed time: 84.44232342299074. Arrivals time: 0.6255226098001003 Scheduler time: 83.58290425967425 Scheduler overhead time: 0.09038561070337892 Adapter cache time: 0.023821992799639702 Engine time: 0.09238030901178718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 78.66049178084359,
    "estimated_duration": 3600.056363860003,
    "input_throughput": 6106.589391402902,
    "output_throughput": 5471.041008614034,
    "total_throughput": 11577.630400016935,
    "itl": 144.56830797884538,
    "ttft": 1131463.1775027039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 431,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4270781874097933,
    "arrivals": 116430,
    "finished_requests": 89418,
    "scheduler_time": 115.95778396878495
}
#Debug simulation 
Total elapsed time: 78.66075768368319. Arrivals time: 0.6008580215275288 Scheduler time: 77.83328242320567 Scheduler overhead time: 0.0876414142549038 Adapter cache time: 0.022966931108385324 Engine time: 0.08946267981082201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 85.93648262694478,
    "estimated_duration": 3600.0584607544297,
    "input_throughput": 6116.76344705395,
    "output_throughput": 5481.783203005092,
    "total_throughput": 11598.546650059041,
    "itl": 144.68014484060856,
    "ttft": 1125614.7186791124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2677813250385144,
    "arrivals": 116430,
    "finished_requests": 89574,
    "scheduler_time": 115.55358657195492
}
#Debug simulation 
Total elapsed time: 85.93666767608374. Arrivals time: 0.6199622387066483 Scheduler time: 85.0829261308536 Scheduler overhead time: 0.09038479626178741 Adapter cache time: 0.02356379944831133 Engine time: 0.09252007631585002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 84.22447409434244,
    "estimated_duration": 3600.1230816530515,
    "input_throughput": 6117.441959758652,
    "output_throughput": 5481.742027258246,
    "total_throughput": 11599.183987016897,
    "itl": 144.67038141136882,
    "ttft": 1126157.751437979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4249878745153557,
    "arrivals": 116430,
    "finished_requests": 89579,
    "scheduler_time": 115.55579602348388
}
#Debug simulation 
Total elapsed time: 84.22477191500366. Arrivals time: 0.6236146837472916 Scheduler time: 83.36316775344312 Scheduler overhead time: 0.09185035387054086 Adapter cache time: 0.02438702154904604 Engine time: 0.0946024302393198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 165.23790325364098,
    "estimated_duration": 3600.0418191761723,
    "input_throughput": 6217.300277117887,
    "output_throughput": 5512.001247957986,
    "total_throughput": 11729.301525075873,
    "itl": 145.6431875671798,
    "ttft": 847165.5764971415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4590731615899125,
    "arrivals": 115680,
    "finished_requests": 90285,
    "scheduler_time": 113.04912160874756
}
#Debug simulation 
Total elapsed time: 165.2382622975856. Arrivals time: 0.730920041911304 Scheduler time: 164.2027235957794 Scheduler overhead time: 0.12339032394811511 Adapter cache time: 0.02533350419253111 Engine time: 0.12352755246683955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 163.58136630570516,
    "estimated_duration": 3600.0445125906217,
    "input_throughput": 6217.2956255736235,
    "output_throughput": 5511.997124091252,
    "total_throughput": 11729.292749664875,
    "itl": 145.6443861663713,
    "ttft": 847166.5700558533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4869155041687191,
    "arrivals": 115680,
    "finished_requests": 90285,
    "scheduler_time": 113.04931022093528
}
#Debug simulation 
Total elapsed time: 163.58159921970218. Arrivals time: 0.7322028186172247 Scheduler time: 162.54430490313098 Scheduler overhead time: 0.12131070392206311 Adapter cache time: 0.02568828919902444 Engine time: 0.12464230135083199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 164.596474442631,
    "estimated_duration": 3600.049869999019,
    "input_throughput": 6217.286373315184,
    "output_throughput": 5511.988921421638,
    "total_throughput": 11729.275294736823,
    "itl": 145.64435119067275,
    "ttft": 847190.9459200569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4882290918007508,
    "arrivals": 115680,
    "finished_requests": 90285,
    "scheduler_time": 113.04959461751967
}
#Debug simulation 
Total elapsed time: 164.59667750168592. Arrivals time: 0.7374764448031783 Scheduler time: 163.55478883674368 Scheduler overhead time: 0.12176508037373424 Adapter cache time: 0.02519265655428171 Engine time: 0.12481525959447026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 163.5876001943834,
    "estimated_duration": 3600.05781346373,
    "input_throughput": 6217.373764468714,
    "output_throughput": 5512.145923264331,
    "total_throughput": 11729.519687733045,
    "itl": 145.64273961912605,
    "ttft": 847152.4215456314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4668943335651423,
    "arrivals": 115680,
    "finished_requests": 90287,
    "scheduler_time": 113.04985679668464
}
#Debug simulation 
Total elapsed time: 163.5878011654131. Arrivals time: 0.72375293308869 Scheduler time: 162.55777854518965 Scheduler overhead time: 0.12335808435454965 Adapter cache time: 0.025668816175311804 Engine time: 0.12419074680656195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 163.18211671384051,
    "estimated_duration": 3600.0688007176914,
    "input_throughput": 6217.354789313432,
    "output_throughput": 5512.129100433856,
    "total_throughput": 11729.483889747287,
    "itl": 145.64442879983437,
    "ttft": 847155.9642651315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4943910273350781,
    "arrivals": 115680,
    "finished_requests": 90287,
    "scheduler_time": 113.05055962296935
}
#Debug simulation 
Total elapsed time: 163.18232262786478. Arrivals time: 0.7312722564674914 Scheduler time: 162.1464497372508 Scheduler overhead time: 0.12233988801017404 Adapter cache time: 0.02535374602302909 Engine time: 0.12430703453719616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 164.34358321409672,
    "estimated_duration": 3600.1506775042385,
    "input_throughput": 6217.554487335384,
    "output_throughput": 5512.25317429138,
    "total_throughput": 11729.807661626763,
    "itl": 145.64403600784846,
    "ttft": 847076.7735369335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4485075442353269,
    "arrivals": 115680,
    "finished_requests": 90291,
    "scheduler_time": 113.0536395844594
}
#Debug simulation 
Total elapsed time: 164.34377662604675. Arrivals time: 0.7229567291215062 Scheduler time: 163.3216150221415 Scheduler overhead time: 0.12149689579382539 Adapter cache time: 0.025280915200710297 Engine time: 0.12012901995331049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 161.82894383789971,
    "estimated_duration": 3600.0986691034414,
    "input_throughput": 6217.3032067407685,
    "output_throughput": 5512.083368798863,
    "total_throughput": 11729.386575539631,
    "itl": 145.64434523011028,
    "ttft": 847156.0238643041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5000499477237461,
    "arrivals": 115680,
    "finished_requests": 90287,
    "scheduler_time": 113.05179735432264
}
#Debug simulation 
Total elapsed time: 161.82919163210317. Arrivals time: 0.7190517731942236 Scheduler time: 160.80911471927539 Scheduler overhead time: 0.12091530812904239 Adapter cache time: 0.02488693594932556 Engine time: 0.12214693240821362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 70.1696844319813,
    "estimated_duration": 3600.1507847144653,
    "input_throughput": 6285.82283166643,
    "output_throughput": 5513.820444488518,
    "total_throughput": 11799.643276154948,
    "itl": 143.07889159571442,
    "ttft": 852949.2016095765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1935902201337794,
    "arrivals": 109355,
    "finished_requests": 90897,
    "scheduler_time": 103.81398668707178
}
#Debug simulation 
Total elapsed time: 70.16987598221749. Arrivals time: 0.5230584763921797 Scheduler time: 69.42988448729739 Scheduler overhead time: 0.08458789018914104 Adapter cache time: 0.02060551382601261 Engine time: 0.08641494298353791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.18566408893093,
    "estimated_duration": 3600.1599554076147,
    "input_throughput": 6284.633260812516,
    "output_throughput": 5515.327164887559,
    "total_throughput": 11799.960425700076,
    "itl": 143.60333647093418,
    "ttft": 850086.8737507011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2877132445992956,
    "arrivals": 109355,
    "finished_requests": 90948,
    "scheduler_time": 103.63281650052343
}
#Debug simulation 
Total elapsed time: 68.18588341819122. Arrivals time: 0.508055339101702 Scheduler time: 67.47136660898104 Scheduler overhead time: 0.08070300566032529 Adapter cache time: 0.019617144018411636 Engine time: 0.08116125455126166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.4935694327578,
    "estimated_duration": 3600.081987222564,
    "input_throughput": 6280.304748682444,
    "output_throughput": 5510.614222234805,
    "total_throughput": 11790.91897091725,
    "itl": 143.50882950151066,
    "ttft": 852506.9721741729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3033973592147305,
    "arrivals": 109355,
    "finished_requests": 90877,
    "scheduler_time": 103.61214712527419
}
#Debug simulation 
Total elapsed time: 68.49376923590899. Arrivals time: 0.5168072199448943 Scheduler time: 67.76127283787355 Scheduler overhead time: 0.08550092251971364 Adapter cache time: 0.020812963135540485 Engine time: 0.08381781773641706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 69.77445975597948,
    "estimated_duration": 3600.043601597719,
    "input_throughput": 6282.528908806073,
    "output_throughput": 5508.748836041477,
    "total_throughput": 11791.27774484755,
    "itl": 143.1244388199896,
    "ttft": 853290.8640675718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2413931444287298,
    "arrivals": 109355,
    "finished_requests": 90857,
    "scheduler_time": 103.75620073002277
}
#Debug simulation 
Total elapsed time: 69.77464167494327. Arrivals time: 0.5305053014308214 Scheduler time: 69.02728032693267 Scheduler overhead time: 0.08442093105986714 Adapter cache time: 0.020718570332974195 Engine time: 0.08714120090007782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 70.00611647218466,
    "estimated_duration": 3600.1737913508923,
    "input_throughput": 6282.795029045701,
    "output_throughput": 5508.998495474835,
    "total_throughput": 11791.793524520535,
    "itl": 143.12322811675048,
    "ttft": 853390.9453450991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 397,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3133210662566175,
    "arrivals": 109355,
    "finished_requests": 90857,
    "scheduler_time": 103.76378301293033
}
#Debug simulation 
Total elapsed time: 70.0062928921543. Arrivals time: 0.5320636127144098 Scheduler time: 69.26009009871632 Scheduler overhead time: 0.08446396701037884 Adapter cache time: 0.020400176290422678 Engine time: 0.0838188137859106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 70.34820765210316,
    "estimated_duration": 3600.000705618988,
    "input_throughput": 6288.5204340834935,
    "output_throughput": 5515.857807751666,
    "total_throughput": 11804.378241835158,
    "itl": 143.08541766148522,
    "ttft": 851239.8278610273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1840599167812564,
    "arrivals": 109355,
    "finished_requests": 90948,
    "scheduler_time": 103.85478952780095
}
#Debug simulation 
Total elapsed time: 70.34838074119762. Arrivals time: 0.5220951624214649 Scheduler time: 69.61033683316782 Scheduler overhead time: 0.08326039602980018 Adapter cache time: 0.021079289261251688 Engine time: 0.0858805887401104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.99540810380131,
    "estimated_duration": 3600.1016634217794,
    "input_throughput": 6286.343585777941,
    "output_throughput": 5513.630407073997,
    "total_throughput": 11799.973992851938,
    "itl": 143.10294954040074,
    "ttft": 851495.5768482396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3195267594605709,
    "arrivals": 109355,
    "finished_requests": 90940,
    "scheduler_time": 103.80692622597282
}
#Debug simulation 
Total elapsed time: 68.99558950401843. Arrivals time: 0.5321437967941165 Scheduler time: 68.25275781704113 Scheduler overhead time: 0.08156748581677675 Adapter cache time: 0.02046118024736643 Engine time: 0.08363323891535401 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 84.45090378355235,
    "estimated_duration": 3600.075140457727,
    "input_throughput": 6249.324006370466,
    "output_throughput": 5543.412073744281,
    "total_throughput": 11792.736080114746,
    "itl": 144.94302581872697,
    "ttft": 576756.0862409278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46825462482171076,
    "arrivals": 106484,
    "finished_requests": 90710,
    "scheduler_time": 99.43694285126955
}
#Debug simulation 
Total elapsed time: 84.451101930812. Arrivals time: 0.49845301592722535 Scheduler time: 83.74097531987354 Scheduler overhead time: 0.08466971525922418 Adapter cache time: 0.017382680904120207 Engine time: 0.08465631352737546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.08260122360662,
    "estimated_duration": 3600.138751118458,
    "input_throughput": 6214.856578249643,
    "output_throughput": 5513.2394532943135,
    "total_throughput": 11728.096031543955,
    "itl": 141.43684066914227,
    "ttft": 568978.4401427606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4545634293090552,
    "arrivals": 106484,
    "finished_requests": 90100,
    "scheduler_time": 100.41244715976494
}
#Debug simulation 
Total elapsed time: 112.08278293488547. Arrivals time: 0.5636300803162158 Scheduler time: 111.26315530203283 Scheduler overhead time: 0.10380504466593266 Adapter cache time: 0.020559520460665226 Engine time: 0.10238312231376767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.79749811626971,
    "estimated_duration": 3600.1388212596667,
    "input_throughput": 6214.856457166103,
    "output_throughput": 5513.239345880323,
    "total_throughput": 11728.095803046424,
    "itl": 141.43667260203597,
    "ttft": 568978.3781971644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45577041834592946,
    "arrivals": 106484,
    "finished_requests": 90100,
    "scheduler_time": 100.41242748201077
}
#Debug simulation 
Total elapsed time: 111.79769424814731. Arrivals time: 0.5629501533694565 Scheduler time: 110.98287270730361 Scheduler overhead time: 0.10172137105837464 Adapter cache time: 0.020430641248822212 Engine time: 0.09973118035122752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.55492723593488,
    "estimated_duration": 3600.14076739767,
    "input_throughput": 6216.766911638869,
    "output_throughput": 5512.280014079774,
    "total_throughput": 11729.046925718643,
    "itl": 141.3775925322935,
    "ttft": 568761.5140822813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4327779943658973,
    "arrivals": 106484,
    "finished_requests": 90107,
    "scheduler_time": 100.40592713318709
}
#Debug simulation 
Total elapsed time: 111.55507841985673. Arrivals time: 0.5676190191879869 Scheduler time: 110.73592983325943 Scheduler overhead time: 0.10274651041254401 Adapter cache time: 0.018986697774380445 Engine time: 0.10117394011467695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 111.68497853772715,
    "estimated_duration": 3600.008599520268,
    "input_throughput": 6214.678765762221,
    "output_throughput": 5512.94683091722,
    "total_throughput": 11727.62559667944,
    "itl": 141.43648132277306,
    "ttft": 569032.1697509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46155509252101223,
    "arrivals": 106484,
    "finished_requests": 90095,
    "scheduler_time": 100.40791057008157
}
#Debug simulation 
Total elapsed time: 111.68513542879373. Arrivals time: 0.5613545472733676 Scheduler time: 110.86986943613738 Scheduler overhead time: 0.10307207750156522 Adapter cache time: 0.020920698065310717 Engine time: 0.1009124438278377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 94.83605843270198,
    "estimated_duration": 3600.0767391223103,
    "input_throughput": 6249.321231270465,
    "output_throughput": 5543.4096121143775,
    "total_throughput": 11792.730843384843,
    "itl": 144.94446979993108,
    "ttft": 576757.5686539692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45747769512003345,
    "arrivals": 106484,
    "finished_requests": 90710,
    "scheduler_time": 99.43735445685279
}
#Debug simulation 
Total elapsed time: 94.8362387418747. Arrivals time: 0.5435273665934801 Scheduler time: 94.05923882778734 Scheduler overhead time: 0.09384463494643569 Adapter cache time: 0.018242419697344303 Engine time: 0.09399705892428756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 109.58758701942861,
    "estimated_duration": 3600.1092859070864,
    "input_throughput": 6214.713560942003,
    "output_throughput": 5513.050972562125,
    "total_throughput": 11727.764533504129,
    "itl": 141.43688760710282,
    "ttft": 568963.4508875181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 140,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4668367515504358,
    "arrivals": 106484,
    "finished_requests": 90097,
    "scheduler_time": 100.41113517545321
}
#Debug simulation 
Total elapsed time: 109.5877275983803. Arrivals time: 0.5553427524864674 Scheduler time: 108.78283009817824 Scheduler overhead time: 0.10077898995950818 Adapter cache time: 0.019105482380837202 Engine time: 0.10103461937978864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 85.79067631810904,
    "estimated_duration": 3600.0325725739217,
    "input_throughput": 6322.251963327937,
    "output_throughput": 5524.514181209292,
    "total_throughput": 11846.766144537229,
    "itl": 141.52104350184806,
    "ttft": 484530.7401368205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3703190170158628,
    "arrivals": 105023,
    "finished_requests": 91455,
    "scheduler_time": 96.405523015892
}
#Debug simulation 
Total elapsed time: 85.79082892322913. Arrivals time: 0.516801143065095 Scheduler time: 85.04585679015145 Scheduler overhead time: 0.0922830174677074 Adapter cache time: 0.01721633318811655 Engine time: 0.09133924497291446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 86.18762653507292,
    "estimated_duration": 3600.04810866272,
    "input_throughput": 6322.286345349608,
    "output_throughput": 5524.563116848982,
    "total_throughput": 11846.84946219859,
    "itl": 141.52473292556186,
    "ttft": 484497.8525460792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3928493298846297,
    "arrivals": 105023,
    "finished_requests": 91456,
    "scheduler_time": 96.40619100624768
}
#Debug simulation 
Total elapsed time: 86.18778576189652. Arrivals time: 0.5328661943785846 Scheduler time: 85.42353187687695 Scheduler overhead time: 0.09392909007146955 Adapter cache time: 0.01737208990380168 Engine time: 0.09268898889422417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 86.04517262801528,
    "estimated_duration": 3600.0482915266875,
    "input_throughput": 6322.286024209927,
    "output_throughput": 5524.562836229544,
    "total_throughput": 11846.84886043947,
    "itl": 141.52455837339133,
    "ttft": 484497.8218524873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3938965902850041,
    "arrivals": 105023,
    "finished_requests": 91456,
    "scheduler_time": 96.40617183725034
}
#Debug simulation 
Total elapsed time: 86.04533851891756. Arrivals time: 0.533042885363102 Scheduler time: 85.28185111517087 Scheduler overhead time: 0.09299394115805626 Adapter cache time: 0.017498725093901157 Engine time: 0.09310023766011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 85.91527422470972,
    "estimated_duration": 3600.0546722633603,
    "input_throughput": 6322.386205787859,
    "output_throughput": 5524.618599054718,
    "total_throughput": 11847.004804842578,
    "itl": 141.52165285637815,
    "ttft": 484461.7209388885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37568832651013534,
    "arrivals": 105023,
    "finished_requests": 91457,
    "scheduler_time": 96.40628404532174
}
#Debug simulation 
Total elapsed time: 85.9154850160703. Arrivals time: 0.5304268556647003 Scheduler time: 85.15529903071001 Scheduler overhead time: 0.09150036703795195 Adapter cache time: 0.018509273417294025 Engine time: 0.09255907451733947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 85.79021479794756,
    "estimated_duration": 3600.0495062552254,
    "input_throughput": 6322.283890944469,
    "output_throughput": 5524.560972131807,
    "total_throughput": 11846.844863076276,
    "itl": 141.52305125414995,
    "ttft": 484497.92821876425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39917824931442764,
    "arrivals": 105023,
    "finished_requests": 91456,
    "scheduler_time": 96.40624271344201
}
#Debug simulation 
Total elapsed time: 85.79036541189998. Arrivals time: 0.5220784363336861 Scheduler time: 85.04028629511595 Scheduler overhead time: 0.09243659162893891 Adapter cache time: 0.016430691815912724 Engine time: 0.0912752472795546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 86.0416491148062,
    "estimated_duration": 3600.162763440124,
    "input_throughput": 6322.502757694712,
    "output_throughput": 5524.495781683784,
    "total_throughput": 11846.998539378497,
    "itl": 141.51976498228765,
    "ttft": 484439.5275215511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3617960856831637,
    "arrivals": 105023,
    "finished_requests": 91461,
    "scheduler_time": 96.40988387538695
}
#Debug simulation 
Total elapsed time: 86.04181569488719. Arrivals time: 0.5222686887718737 Scheduler time: 85.28898519743234 Scheduler overhead time: 0.093008600641042 Adapter cache time: 0.01751293707638979 Engine time: 0.09310732549056411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 85.83262499328703,
    "estimated_duration": 3600.071366801343,
    "input_throughput": 6322.356887114449,
    "output_throughput": 5524.592979852862,
    "total_throughput": 11846.949866967312,
    "itl": 141.52375690481213,
    "ttft": 484491.808190836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40345387805253247,
    "arrivals": 105023,
    "finished_requests": 91457,
    "scheduler_time": 96.4069362194541
}
#Debug simulation 
Total elapsed time: 85.83277284540236. Arrivals time: 0.5370723456144333 Scheduler time: 85.06548623228446 Scheduler overhead time: 0.09337587561458349 Adapter cache time: 0.017125030979514122 Engine time: 0.09312539966776967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 69.42054535169154,
    "estimated_duration": 3600.028130789614,
    "input_throughput": 6327.510000596498,
    "output_throughput": 5526.305983513621,
    "total_throughput": 11853.815984110119,
    "itl": 143.00854873111348,
    "ttft": 517157.86979722045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.550887793907895,
    "arrivals": 104293,
    "finished_requests": 91375,
    "scheduler_time": 95.5930354513292
}
#Debug simulation 
Total elapsed time: 69.42068984592333. Arrivals time: 0.5194965368136764 Scheduler time: 68.68997899908572 Scheduler overhead time: 0.08404517965391278 Adapter cache time: 0.01674450049176812 Engine time: 0.0843357308767736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 70.59006557799876,
    "estimated_duration": 3600.0505233309855,
    "input_throughput": 6327.470643085111,
    "output_throughput": 5526.271609541765,
    "total_throughput": 11853.742252626877,
    "itl": 143.01079160721866,
    "ttft": 517188.2711882776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5864233006583537,
    "arrivals": 104293,
    "finished_requests": 91375,
    "scheduler_time": 95.5936842291784
}
#Debug simulation 
Total elapsed time: 70.59022654732689. Arrivals time: 0.5125317680649459 Scheduler time: 69.85984516935423 Scheduler overhead time: 0.08687259908765554 Adapter cache time: 0.018008760642260313 Engine time: 0.08783007226884365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.54468044592068,
    "estimated_duration": 3600.050762036397,
    "input_throughput": 6327.470223535059,
    "output_throughput": 5526.271243116116,
    "total_throughput": 11853.741466651176,
    "itl": 143.01080100786444,
    "ttft": 517188.41793550225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5876285971328639,
    "arrivals": 104293,
    "finished_requests": 91375,
    "scheduler_time": 95.59369230364692
}
#Debug simulation 
Total elapsed time: 68.5448382277973. Arrivals time: 0.5158127723261714 Scheduler time: 67.81540940050036 Scheduler overhead time: 0.08326306147500873 Adapter cache time: 0.017748982645571232 Engine time: 0.08616409683600068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 69.00557955773547,
    "estimated_duration": 3600.0877710435807,
    "input_throughput": 6327.58822804941,
    "output_throughput": 5526.338596526168,
    "total_throughput": 11853.926824575577,
    "itl": 143.0101103854501,
    "ttft": 517178.2650422329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5623161768703728,
    "arrivals": 104293,
    "finished_requests": 91377,
    "scheduler_time": 95.5948229763412
}
#Debug simulation 
Total elapsed time: 69.00575365079567. Arrivals time: 0.5153923891484737 Scheduler time: 68.2767558679916 Scheduler overhead time: 0.08388073882088065 Adapter cache time: 0.01705253589898348 Engine time: 0.08721436746418476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 70.00473575294018,
    "estimated_duration": 3600.115798231441,
    "input_throughput": 6327.539245040569,
    "output_throughput": 5526.382792957308,
    "total_throughput": 11853.922037997878,
    "itl": 143.0134431443601,
    "ttft": 517175.6655829658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.59504807053134,
    "arrivals": 104293,
    "finished_requests": 91378,
    "scheduler_time": 95.59587000747725
}
#Debug simulation 
Total elapsed time: 70.00495251081884. Arrivals time: 0.5090591968037188 Scheduler time: 69.27877424703911 Scheduler overhead time: 0.08350455248728395 Adapter cache time: 0.018799735233187675 Engine time: 0.08968547265976667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 69.7865890506655,
    "estimated_duration": 3600.130049686051,
    "input_throughput": 6327.514196879226,
    "output_throughput": 5526.360916249399,
    "total_throughput": 11853.875113128626,
    "itl": 143.00976310468965,
    "ttft": 517173.3427040207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5382090530823923,
    "arrivals": 104293,
    "finished_requests": 91378,
    "scheduler_time": 95.59619949086517
}
#Debug simulation 
Total elapsed time: 69.78673722688109. Arrivals time: 0.5074991188012064 Scheduler time: 69.06651175674051 Scheduler overhead time: 0.08445218903943896 Adapter cache time: 0.0181731591001153 Engine time: 0.08529299404472113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 70.72035618312657,
    "estimated_duration": 3600.149093592717,
    "input_throughput": 6327.48072587937,
    "output_throughput": 5526.331683154115,
    "total_throughput": 11853.812409033486,
    "itl": 143.01370940706263,
    "ttft": 517179.79669545754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6024675439298164,
    "arrivals": 104293,
    "finished_requests": 91378,
    "scheduler_time": 95.59718898378367
}
#Debug simulation 
Total elapsed time: 70.72055042302236. Arrivals time: 0.5191273395903409 Scheduler time: 69.98194282874465 Scheduler overhead time: 0.08473914209753275 Adapter cache time: 0.018796435557305813 Engine time: 0.0895938347093761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 55.643627190962434,
    "estimated_duration": 3600.0480496342316,
    "input_throughput": 6325.526683543481,
    "output_throughput": 5531.761166916464,
    "total_throughput": 11857.287850459947,
    "itl": 141.36871098380755,
    "ttft": 365224.02883544075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5570087693957605,
    "arrivals": 100777,
    "finished_requests": 91438,
    "scheduler_time": 90.70887369635847
}
#Debug simulation 
Total elapsed time: 55.643840193748474. Arrivals time: 0.46657954761758447 Scheduler time: 54.98846539482474 Scheduler overhead time: 0.0737061663530767 Adapter cache time: 0.014918215572834015 Engine time: 0.07589559257030487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 54.9843156831339,
    "estimated_duration": 3600.164421923046,
    "input_throughput": 6325.55331676648,
    "output_throughput": 5531.623188854923,
    "total_throughput": 11857.176505621403,
    "itl": 141.37361267041678,
    "ttft": 365284.8272018443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5936291872034792,
    "arrivals": 100777,
    "finished_requests": 91440,
    "scheduler_time": 90.71235542028946
}
#Debug simulation 
Total elapsed time: 54.98447290388867. Arrivals time: 0.4677177518606186 Scheduler time: 54.32860057428479 Scheduler overhead time: 0.07430591434240341 Adapter cache time: 0.014859471470117569 Engine time: 0.0742905093356967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 55.256991683039814,
    "estimated_duration": 3600.0006978591077,
    "input_throughput": 6325.609884893202,
    "output_throughput": 5531.833927655364,
    "total_throughput": 11857.443812548567,
    "itl": 141.37412109946078,
    "ttft": 365163.97423628333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5947273773141228,
    "arrivals": 100777,
    "finished_requests": 91438,
    "scheduler_time": 90.7076634870039
}
#Debug simulation 
Total elapsed time: 55.25713176326826. Arrivals time: 0.4674127819016576 Scheduler time: 54.602997538633645 Scheduler overhead time: 0.0740075996145606 Adapter cache time: 0.014445866458117962 Engine time: 0.07400636468082666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 54.248899292200804,
    "estimated_duration": 3600.0661392157463,
    "input_throughput": 6325.494899091157,
    "output_throughput": 5531.7333709703125,
    "total_throughput": 11857.22827006147,
    "itl": 141.36851930554133,
    "ttft": 365257.2041134019,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5703392540523791,
    "arrivals": 100777,
    "finished_requests": 91438,
    "scheduler_time": 90.70933717658555
}
#Debug simulation 
Total elapsed time: 54.24909145897254. Arrivals time: 0.4588259574957192 Scheduler time: 53.60252664377913 Scheduler overhead time: 0.07354276301339269 Adapter cache time: 0.015100448857992887 Engine time: 0.07481746282428503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 55.992621436249465,
    "estimated_duration": 3600.1643313641134,
    "input_throughput": 6325.553475880149,
    "output_throughput": 5531.623327997986,
    "total_throughput": 11857.176803878134,
    "itl": 141.3727831939699,
    "ttft": 365283.6116965336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6018953431397694,
    "arrivals": 100777,
    "finished_requests": 91440,
    "scheduler_time": 90.71207729842055
}
#Debug simulation 
Total elapsed time: 55.992811053059995. Arrivals time: 0.46505766874179244 Scheduler time: 55.33610835345462 Scheduler overhead time: 0.07522550877183676 Adapter cache time: 0.01546866400167346 Engine time: 0.07674589427188039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 55.6780280158855,
    "estimated_duration": 3600.1558452343747,
    "input_throughput": 6325.568386197861,
    "output_throughput": 5531.636366898312,
    "total_throughput": 11857.204753096174,
    "itl": 141.36763410771403,
    "ttft": 365281.2032789944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5441891536721967,
    "arrivals": 100777,
    "finished_requests": 91440,
    "scheduler_time": 90.71185678005388
}
#Debug simulation 
Total elapsed time: 55.6782396659255. Arrivals time: 0.46650643507018685 Scheduler time: 55.02334654517472 Scheduler overhead time: 0.0728150992654264 Adapter cache time: 0.014178712852299213 Engine time: 0.07741963444277644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 270, 270, 270, 270, 4320, 270, 135, 4320, 135, 270, 270, 135, 4320, 4320, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 135, 270, 270, 4320, 270, 135, 135, 270, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 4320, 4320, 135, 270, 4320, 135, 270, 135, 270, 135, 135, 270, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 270, 135, 135, 135, 135, 135, 4320, 135, 270, 135, 4320, 4320, 135, 4320, 135, 270, 270, 4320, 135, 135, 135, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 135, 4320, 270, 270, 270, 4320, 270, 135, 270, 4320, 4320, 270, 4320, 270, 135, 4320, 270, 135, 270, 270, 135, 270, 4320, 135, 270, 4320, 135, 270, 4320, 4320, 270, 135, 4320, 270, 135, 135, 270, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 270, 135, 4320, 4320, 270, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 135, 4320, 270, 4320, 135, 4320, 270, 135, 270, 135, 135, 270, 270, 270, 4320, 270, 135, 135]
Prompts retrieved: 302400 . Total input tokens: 67494451 . Total output tokens: 60433434
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 55.63832750916481,
    "estimated_duration": 3600.0440500413065,
    "input_throughput": 6325.533711105206,
    "output_throughput": 5531.76731261705,
    "total_throughput": 11857.301023722255,
    "itl": 141.37452867782162,
    "ttft": 365225.6524256037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6099435854703199,
    "arrivals": 100777,
    "finished_requests": 91438,
    "scheduler_time": 90.70867916902631
}
#Debug simulation 
Total elapsed time: 55.63847443833947. Arrivals time: 0.46842564502730966 Scheduler time: 54.97953158337623 Scheduler overhead time: 0.07586014736443758 Adapter cache time: 0.015721160918474197 Engine time: 0.07463353872299194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 39.38464146107435,
    "estimated_duration": 3600.1531169480154,
    "input_throughput": 6357.765144001375,
    "output_throughput": 5547.708486613347,
    "total_throughput": 11905.473630614722,
    "itl": 143.3284753502953,
    "ttft": 307339.67793419404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47437560030957626,
    "arrivals": 99385,
    "finished_requests": 91509,
    "scheduler_time": 88.07081522927042
}
#Debug simulation 
Total elapsed time: 39.384797766804695. Arrivals time: 0.42739666905254126 Scheduler time: 38.800591888371855 Scheduler overhead time: 0.06294371234253049 Adapter cache time: 0.011563046835362911 Engine time: 0.06100396439433098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 40.055715129710734,
    "estimated_duration": 3600.0091419572973,
    "input_throughput": 6338.646958988938,
    "output_throughput": 5533.690114233688,
    "total_throughput": 11872.337073222627,
    "itl": 142.07322840535036,
    "ttft": 329603.6103350972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.542519619937521,
    "arrivals": 99385,
    "finished_requests": 91247,
    "scheduler_time": 88.44603930950163
}
#Debug simulation 
Total elapsed time: 40.05587269272655. Arrivals time: 0.42742723133414984 Scheduler time: 39.46945351688191 Scheduler overhead time: 0.06244019465520978 Adapter cache time: 0.011611801572144032 Engine time: 0.0631659310311079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 36.732626552693546,
    "estimated_duration": 3600.13021267431,
    "input_throughput": 6338.434070985744,
    "output_throughput": 5533.615959185375,
    "total_throughput": 11872.05003017112,
    "itl": 142.0719161017991,
    "ttft": 329704.5700866802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5433330957777818,
    "arrivals": 99385,
    "finished_requests": 91248,
    "scheduler_time": 88.44964031443159
}
#Debug simulation 
Total elapsed time: 36.73275093873963. Arrivals time: 0.2804996813647449 Scheduler time: 36.316594920586795 Scheduler overhead time: 0.05334764765575528 Adapter cache time: 0.010184316895902157 Engine time: 0.051617045886814594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 36.22833567811176,
    "estimated_duration": 3600.166923663876,
    "input_throughput": 6357.74076183835,
    "output_throughput": 5547.687211034638,
    "total_throughput": 11905.427972872989,
    "itl": 143.32801466919926,
    "ttft": 307340.54560716706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.487564919497818,
    "arrivals": 99385,
    "finished_requests": 91509,
    "scheduler_time": 88.07106364331719
}
#Debug simulation 
Total elapsed time: 36.228431642986834. Arrivals time: 0.2493980135768652 Scheduler time: 35.84754696581513 Scheduler overhead time: 0.051282749976962805 Adapter cache time: 0.009129196871072054 Engine time: 0.05086587555706501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 36.60887202527374,
    "estimated_duration": 3600.0711869213033,
    "input_throughput": 6338.537716392891,
    "output_throughput": 5533.594744562887,
    "total_throughput": 11872.132460955778,
    "itl": 142.07483961594622,
    "ttft": 329703.7742545814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5497465388849392,
    "arrivals": 99385,
    "finished_requests": 91247,
    "scheduler_time": 88.44798559760733
}
#Debug simulation 
Total elapsed time: 36.60900297621265. Arrivals time: 0.25298849446699023 Scheduler time: 36.220646975096315 Scheduler overhead time: 0.05199402570724487 Adapter cache time: 0.010508922394365072 Engine time: 0.052169899456202984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 36.654467700980604,
    "estimated_duration": 3600.0784038915535,
    "input_throughput": 6338.525009714592,
    "output_throughput": 5533.583651529856,
    "total_throughput": 11872.108661244447,
    "itl": 142.07045423386862,
    "ttft": 329702.93811928265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4963483489537618,
    "arrivals": 99385,
    "finished_requests": 91247,
    "scheduler_time": 88.44804244891327
}
#Debug simulation 
Total elapsed time: 36.65457343496382. Arrivals time: 0.25656207418069243 Scheduler time: 36.263855289667845 Scheduler overhead time: 0.053099234122782946 Adapter cache time: 0.009916710201650858 Engine time: 0.050853466149419546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 270, 270, 270, 270, 4320, 270, 66, 4320, 66, 270, 270, 66, 4320, 4320, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 66, 270, 270, 4320, 270, 66, 66, 270, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 4320, 4320, 66, 270, 4320, 66, 270, 66, 270, 66, 66, 270, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 270, 66, 66, 66, 66, 66, 4320, 66, 270, 66, 4320, 4320, 66, 4320, 66, 270, 270, 4320, 66, 66, 66, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 66, 4320, 270, 270, 270, 4320, 270, 66, 270, 4320, 4320, 270, 4320, 270, 66, 4320, 270, 66, 270, 270, 66, 270, 4320, 66, 270, 4320, 66, 270, 4320, 4320, 270, 66, 4320, 270, 66, 66, 270, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 270, 66, 4320, 4320, 270, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 66, 4320, 270, 4320, 66, 4320, 270, 66, 270, 66, 66, 270, 270, 270, 4320, 270, 66, 66]
Prompts retrieved: 297984 . Total input tokens: 66511279 . Total output tokens: 59553787
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 36.32322380738333,
    "estimated_duration": 3600.070911092935,
    "input_throughput": 6357.662269783315,
    "output_throughput": 5547.699890704854,
    "total_throughput": 11905.36216048817,
    "itl": 143.33135877530128,
    "ttft": 307380.12720620004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5203605560585856,
    "arrivals": 99385,
    "finished_requests": 91507,
    "scheduler_time": 88.06872336392118
}
#Debug simulation 
Total elapsed time: 36.32335685426369. Arrivals time: 0.2553669400513172 Scheduler time: 35.93363977270201 Scheduler overhead time: 0.05251121101900935 Adapter cache time: 0.009469398763030767 Engine time: 0.05188319040462375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 38.03316255193204,
    "estimated_duration": 3600.0837747053306,
    "input_throughput": 6318.7946235673235,
    "output_throughput": 5499.912568457009,
    "total_throughput": 11818.707192024332,
    "itl": 137.29134305870107,
    "ttft": 295379.3127954537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 98651,
    "finished_requests": 90958,
    "scheduler_time": 87.04358386344404
}
#Debug simulation 
Total elapsed time: 38.03327222680673. Arrivals time: 0.2503332202322781 Scheduler time: 37.6453036442399 Scheduler overhead time: 0.05417486885562539 Adapter cache time: 0.00974141526967287 Engine time: 0.05271103512495756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 37.89781287917867,
    "estimated_duration": 3600.004718049167,
    "input_throughput": 6318.676996714239,
    "output_throughput": 5499.620292366962,
    "total_throughput": 11818.2972890812,
    "itl": 137.28905573842084,
    "ttft": 295290.6414043243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41623125385958704,
    "arrivals": 98651,
    "finished_requests": 90954,
    "scheduler_time": 87.04128781480611
}
#Debug simulation 
Total elapsed time: 37.897925978992134. Arrivals time: 0.25115361204370856 Scheduler time: 37.50929168937728 Scheduler overhead time: 0.05386621365323663 Adapter cache time: 0.00956256315112114 Engine time: 0.052760543301701546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 37.80930738104507,
    "estimated_duration": 3600.0070579128655,
    "input_throughput": 6318.672889821477,
    "output_throughput": 5499.616717829003,
    "total_throughput": 11818.28960765048,
    "itl": 137.2891150108595,
    "ttft": 295290.7537083121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4172247071936737,
    "arrivals": 98651,
    "finished_requests": 90954,
    "scheduler_time": 87.04132922772324
}
#Debug simulation 
Total elapsed time: 37.80942023033276. Arrivals time: 0.2550512496381998 Scheduler time: 37.416957553476095 Scheduler overhead time: 0.0536799281835556 Adapter cache time: 0.009551997762173414 Engine time: 0.05312881711870432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 37.708044153638184,
    "estimated_duration": 3600.1145993228874,
    "input_throughput": 6318.740521281878,
    "output_throughput": 5499.865477538973,
    "total_throughput": 11818.60599882085,
    "itl": 137.29116398700037,
    "ttft": 295378.1798799287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39825305984821197,
    "arrivals": 98651,
    "finished_requests": 90958,
    "scheduler_time": 87.04429042900254
}
#Debug simulation 
Total elapsed time: 37.70816187886521. Arrivals time: 0.25257848715409636 Scheduler time: 37.31728709070012 Scheduler overhead time: 0.0545437908731401 Adapter cache time: 0.009072603657841682 Engine time: 0.053171108942478895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 37.83006997965276,
    "estimated_duration": 3600.010215280243,
    "input_throughput": 6318.6673480673,
    "output_throughput": 5499.611894423131,
    "total_throughput": 11818.279242490431,
    "itl": 137.28822616215126,
    "ttft": 295325.07382503565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42275787379592683,
    "arrivals": 98651,
    "finished_requests": 90954,
    "scheduler_time": 87.04138886936799
}
#Debug simulation 
Total elapsed time: 37.83018101891503. Arrivals time: 0.2482965849339962 Scheduler time: 37.44490996142849 Scheduler overhead time: 0.05423530889675021 Adapter cache time: 0.009899856057018042 Engine time: 0.05177214276045561 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 37.888195557985455,
    "estimated_duration": 3600.1661333760103,
    "input_throughput": 6318.650072592114,
    "output_throughput": 5499.786750516611,
    "total_throughput": 11818.436823108726,
    "itl": 137.28968458896702,
    "ttft": 295445.4215723906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 98651,
    "finished_requests": 90958,
    "scheduler_time": 87.04570092040063
}
#Debug simulation 
Total elapsed time: 37.88831225922331. Arrivals time: 0.2512397044338286 Scheduler time: 37.499981832224876 Scheduler overhead time: 0.05325560411438346 Adapter cache time: 0.009730249177664518 Engine time: 0.053091179579496384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 270, 270, 270, 270, 4320, 270, 33, 4320, 33, 270, 270, 33, 4320, 4320, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 33, 270, 270, 4320, 270, 33, 33, 270, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 4320, 4320, 33, 270, 4320, 33, 270, 33, 270, 33, 33, 270, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 270, 33, 33, 33, 33, 33, 4320, 33, 270, 33, 4320, 4320, 33, 4320, 33, 270, 270, 4320, 33, 33, 33, 4320, 4320, 270, 4320, 270, 4320, 4320, 270, 270, 33, 4320, 270, 270, 270, 4320, 270, 33, 270, 4320, 4320, 270, 4320, 270, 33, 4320, 270, 33, 270, 270, 33, 270, 4320, 33, 270, 4320, 33, 270, 4320, 4320, 270, 33, 4320, 270, 33, 33, 270, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 270, 33, 4320, 4320, 270, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 270, 270, 270, 270, 270, 270, 33, 4320, 270, 4320, 33, 4320, 270, 33, 270, 33, 33, 270, 270, 270, 4320, 270, 33, 33]
Prompts retrieved: 295872 . Total input tokens: 66041597 . Total output tokens: 59123757
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 37.8134225490503,
    "estimated_duration": 3600.1161886752925,
    "input_throughput": 6318.737731731509,
    "output_throughput": 5499.863049499441,
    "total_throughput": 11818.600781230949,
    "itl": 137.29288280245984,
    "ttft": 295378.8175152767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42753651767969103,
    "arrivals": 98651,
    "finished_requests": 90958,
    "scheduler_time": 87.04444633006496
}
#Debug simulation 
Total elapsed time: 37.81354049127549. Arrivals time: 0.2538146637380123 Scheduler time: 37.42185107897967 Scheduler overhead time: 0.054594905115664005 Adapter cache time: 0.009806444402784109 Engine time: 0.052560314536094666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 22.89396038092673,
    "estimated_duration": 3600.0184780731,
    "input_throughput": 6275.780565463319,
    "output_throughput": 5587.4132653803445,
    "total_throughput": 11863.193830843664,
    "itl": 145.167659666712,
    "ttft": 191637.04235044215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2601414582342838,
    "arrivals": 96558,
    "finished_requests": 91439,
    "scheduler_time": 83.96596781298797
}
#Debug simulation 
Total elapsed time: 22.894072067923844. Arrivals time: 0.22267565922811627 Scheduler time: 22.554649989586323 Scheduler overhead time: 0.045692816376686096 Adapter cache time: 0.0072439881041646 Engine time: 0.04457172378897667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.90666247997433,
    "estimated_duration": 3600.1340990411186,
    "input_throughput": 6275.705398312151,
    "output_throughput": 5587.274097750284,
    "total_throughput": 11862.979496062435,
    "itl": 145.16830435495035,
    "ttft": 191668.4333144284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2749926363071427,
    "arrivals": 96558,
    "finished_requests": 91440,
    "scheduler_time": 83.9689728533306
}
#Debug simulation 
Total elapsed time: 22.906769808847457. Arrivals time: 0.22403781861066818 Scheduler time: 22.56485860235989 Scheduler overhead time: 0.04655478149652481 Adapter cache time: 0.007463898975402117 Engine time: 0.044521025847643614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.73706751782447,
    "estimated_duration": 3600.133532281999,
    "input_throughput": 6275.70638627919,
    "output_throughput": 5587.274977339477,
    "total_throughput": 11862.981363618668,
    "itl": 145.16787508690328,
    "ttft": 191668.0080576824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27589872436597945,
    "arrivals": 96558,
    "finished_requests": 91440,
    "scheduler_time": 83.96887823151525
}
#Debug simulation 
Total elapsed time: 22.73716352088377. Arrivals time: 0.2210346427746117 Scheduler time: 22.401030351873487 Scheduler overhead time: 0.04434581799432635 Adapter cache time: 0.007329729851335287 Engine time: 0.04455064842477441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 22.77629441022873,
    "estimated_duration": 3600.023818579022,
    "input_throughput": 6275.771255568451,
    "output_throughput": 5587.404976653621,
    "total_throughput": 11863.176232222071,
    "itl": 145.16572781850925,
    "ttft": 191670.9631046512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2651863486645744,
    "arrivals": 96558,
    "finished_requests": 91439,
    "scheduler_time": 83.96592235214763
}
#Debug simulation 
Total elapsed time: 22.776407142169774. Arrivals time: 0.221431995742023 Scheduler time: 22.43836224731058 Scheduler overhead time: 0.045584783889353275 Adapter cache time: 0.0071443612687289715 Engine time: 0.04499713983386755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 22.813590141944587,
    "estimated_duration": 3600.013435656622,
    "input_throughput": 6275.789355735885,
    "output_throughput": 5587.421091480226,
    "total_throughput": 11863.21044721611,
    "itl": 145.16922908265713,
    "ttft": 191636.0970475421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27891681523993594,
    "arrivals": 96558,
    "finished_requests": 91439,
    "scheduler_time": 83.96604980963417
}
#Debug simulation 
Total elapsed time: 22.81368469586596. Arrivals time: 0.2219847054220736 Scheduler time: 22.474610787350684 Scheduler overhead time: 0.04563549254089594 Adapter cache time: 0.007298850454390049 Engine time: 0.045154675375670195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 22.825104027986526,
    "estimated_duration": 3600.1369360193544,
    "input_throughput": 6275.700452933698,
    "output_throughput": 5587.2696948691455,
    "total_throughput": 11862.970147802844,
    "itl": 145.16672103620775,
    "ttft": 191669.26122932407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2541542750666852,
    "arrivals": 96558,
    "finished_requests": 91440,
    "scheduler_time": 83.96888718279895
}
#Debug simulation 
Total elapsed time: 22.82519969623536. Arrivals time: 0.2236757380887866 Scheduler time: 22.484484393615276 Scheduler overhead time: 0.045368282590061426 Adapter cache time: 0.007622566074132919 Engine time: 0.044765499886125326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 135, 135, 135, 135, 4320, 135, 66, 4320, 66, 135, 135, 66, 4320, 4320, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 66, 135, 135, 4320, 135, 66, 66, 135, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 4320, 4320, 66, 135, 4320, 66, 135, 66, 135, 66, 66, 135, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 135, 66, 66, 66, 66, 66, 4320, 66, 135, 66, 4320, 4320, 66, 4320, 66, 135, 135, 4320, 66, 66, 66, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 66, 4320, 135, 135, 135, 4320, 135, 66, 135, 4320, 4320, 135, 4320, 135, 66, 4320, 135, 66, 135, 135, 66, 135, 4320, 66, 135, 4320, 66, 135, 4320, 4320, 135, 66, 4320, 135, 66, 66, 135, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 135, 66, 4320, 4320, 135, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 66, 4320, 135, 4320, 66, 4320, 135, 66, 135, 66, 66, 135, 135, 135, 4320, 135, 66, 66]
Prompts retrieved: 289344 . Total input tokens: 64591017 . Total output tokens: 57818491
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.942042652051896,
    "estimated_duration": 3600.161661952165,
    "input_throughput": 6275.759291250461,
    "output_throughput": 5587.272152965687,
    "total_throughput": 11863.031444216147,
    "itl": 145.16826514481747,
    "ttft": 191631.8934142847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 85,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2823121674731371,
    "arrivals": 96558,
    "finished_requests": 91441,
    "scheduler_time": 83.96961343116124
}
#Debug simulation 
Total elapsed time: 22.94213284412399. Arrivals time: 0.21845383243635297 Scheduler time: 22.607033668551594 Scheduler overhead time: 0.045584785751998425 Adapter cache time: 0.007665849290788174 Engine time: 0.04438397940248251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 21.59695133427158,
    "estimated_duration": 3600.1039221495757,
    "input_throughput": 6322.691370089366,
    "output_throughput": 5516.16743000646,
    "total_throughput": 11838.858800095824,
    "itl": 137.3211371851881,
    "ttft": 171903.54503452618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 105,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3213512131129388,
    "arrivals": 95849,
    "finished_requests": 91311,
    "scheduler_time": 82.19946480374821
}
#Debug simulation 
Total elapsed time: 21.597043700981885. Arrivals time: 0.2168535660021007 Scheduler time: 21.260407101828605 Scheduler overhead time: 0.04649133374914527 Adapter cache time: 0.007796196267008781 Engine time: 0.04595844866707921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 21.542804803233594,
    "estimated_duration": 3600.016141760023,
    "input_throughput": 6322.804705223271,
    "output_throughput": 5516.148877680157,
    "total_throughput": 11838.953582903428,
    "itl": 137.32042817227727,
    "ttft": 171802.3492313449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33793252168688925,
    "arrivals": 95849,
    "finished_requests": 91309,
    "scheduler_time": 82.19731461264163
}
#Debug simulation 
Total elapsed time: 21.542924279347062. Arrivals time: 0.21617724420502782 Scheduler time: 21.205724989064038 Scheduler overhead time: 0.04765128716826439 Adapter cache time: 0.007746225688606501 Engine time: 0.04604804562404752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 22.02205483522266,
    "estimated_duration": 3600.032125817983,
    "input_throughput": 6322.776909894402,
    "output_throughput": 5516.139385975032,
    "total_throughput": 11838.916295869434,
    "itl": 137.32109219233982,
    "ttft": 171836.73582117868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3387842949107294,
    "arrivals": 95849,
    "finished_requests": 91310,
    "scheduler_time": 82.19783263097221
}
#Debug simulation 
Total elapsed time: 22.022143833804876. Arrivals time: 0.2195342849008739 Scheduler time: 21.677359832450747 Scheduler overhead time: 0.04792766971513629 Adapter cache time: 0.008522127754986286 Engine time: 0.048383479937911034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 21.595553448889405,
    "estimated_duration": 3600.1052227756577,
    "input_throughput": 6322.689085862435,
    "output_throughput": 5516.165437156032,
    "total_throughput": 11838.854523018466,
    "itl": 137.32053055073737,
    "ttft": 171904.203315009,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3224058995861562,
    "arrivals": 95849,
    "finished_requests": 91311,
    "scheduler_time": 82.19949998578258
}
#Debug simulation 
Total elapsed time: 21.59563463088125. Arrivals time: 0.2184866084717214 Scheduler time: 21.258144442923367 Scheduler overhead time: 0.04628138896077871 Adapter cache time: 0.007648037746548653 Engine time: 0.04516463028267026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 21.515410864260048,
    "estimated_duration": 3600.0974655354235,
    "input_throughput": 6322.702709554192,
    "output_throughput": 5516.177323006589,
    "total_throughput": 11838.88003256078,
    "itl": 137.3224752273842,
    "ttft": 171907.0504350964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3435629387944937,
    "arrivals": 95849,
    "finished_requests": 91311,
    "scheduler_time": 82.19968940284262
}
#Debug simulation 
Total elapsed time: 21.51548883598298. Arrivals time: 0.21659347135573626 Scheduler time: 21.178983957041055 Scheduler overhead time: 0.04626868711784482 Adapter cache time: 0.007650592364370823 Engine time: 0.04639808973297477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 21.641586067155004,
    "estimated_duration": 3600.035163734521,
    "input_throughput": 6322.771574371923,
    "output_throughput": 5516.134731139648,
    "total_throughput": 11838.90630551157,
    "itl": 137.31944554855863,
    "ttft": 171836.25134317944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 105,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3139552809647288,
    "arrivals": 95849,
    "finished_requests": 91310,
    "scheduler_time": 82.19780408229275
}
#Debug simulation 
Total elapsed time: 21.641673686914146. Arrivals time: 0.21796577470377088 Scheduler time: 21.302573314867914 Scheduler overhead time: 0.046441796235740185 Adapter cache time: 0.008024003356695175 Engine time: 0.04681628616526723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 135, 135, 135, 135, 4320, 135, 33, 4320, 33, 135, 135, 33, 4320, 4320, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 33, 135, 135, 4320, 135, 33, 33, 135, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 4320, 4320, 33, 135, 4320, 33, 135, 33, 135, 33, 33, 135, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 135, 33, 33, 33, 33, 33, 4320, 33, 135, 33, 4320, 4320, 33, 4320, 33, 135, 135, 4320, 33, 33, 33, 4320, 4320, 135, 4320, 135, 4320, 4320, 135, 135, 33, 4320, 135, 135, 135, 4320, 135, 33, 135, 4320, 4320, 135, 4320, 135, 33, 4320, 135, 33, 135, 135, 33, 135, 4320, 33, 135, 4320, 33, 135, 4320, 4320, 135, 33, 4320, 135, 33, 33, 135, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 135, 33, 4320, 4320, 135, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 135, 135, 135, 135, 135, 135, 33, 4320, 135, 4320, 33, 4320, 135, 33, 135, 33, 33, 135, 135, 135, 4320, 135, 33, 33]
Prompts retrieved: 287232 . Total input tokens: 64128402 . Total output tokens: 57391951
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 21.546791860833764,
    "estimated_duration": 3600.1004588863743,
    "input_throughput": 6322.697452459734,
    "output_throughput": 5516.17273650829,
    "total_throughput": 11838.870188968023,
    "itl": 137.321854177434,
    "ttft": 171907.07240116032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34708404481410954,
    "arrivals": 95849,
    "finished_requests": 91311,
    "scheduler_time": 82.19966048517425
}
#Debug simulation 
Total elapsed time: 21.546884312760085. Arrivals time: 0.21476999763399363 Scheduler time: 21.213358463719487 Scheduler overhead time: 0.04635691177099943 Adapter cache time: 0.007656858302652836 Engine time: 0.04520470602437854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 14.779626821633428,
    "estimated_duration": 3600.0203227297206,
    "input_throughput": 6357.496888419177,
    "output_throughput": 5578.762951196769,
    "total_throughput": 11936.259839615946,
    "itl": 143.47809086453162,
    "ttft": 115776.53965655956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 94443,
    "finished_requests": 91504,
    "scheduler_time": 80.90472983624355
}
#Debug simulation 
Total elapsed time: 14.779697286896408. Arrivals time: 0.20175733836367726 Scheduler time: 14.468792432919145 Scheduler overhead time: 0.04157209303230047 Adapter cache time: 0.007140005007386208 Engine time: 0.041771280113607645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.776802369393408,
    "estimated_duration": 3600.119529054748,
    "input_throughput": 6357.473360338515,
    "output_throughput": 5578.699217598833,
    "total_throughput": 11936.172577937346,
    "itl": 143.47883346386112,
    "ttft": 115891.73007495861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31659357430413365,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90743292132404
}
#Debug simulation 
Total elapsed time: 14.776877175085247. Arrivals time: 0.20113416900858283 Scheduler time: 14.465734769124538 Scheduler overhead time: 0.04160240385681391 Adapter cache time: 0.007154567167162895 Engine time: 0.04253514017909765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.746042556129396,
    "estimated_duration": 3600.1279578930325,
    "input_throughput": 6357.458475835664,
    "output_throughput": 5578.686156409315,
    "total_throughput": 11936.14463224498,
    "itl": 143.4790382402057,
    "ttft": 115891.29112343551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3171424154751006,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90760957731528
}
#Debug simulation 
Total elapsed time: 14.746126712299883. Arrivals time: 0.20224120235070586 Scheduler time: 14.420931997243315 Scheduler overhead time: 0.05575196258723736 Adapter cache time: 0.00711811613291502 Engine time: 0.041423410177230835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 14.89979990804568,
    "estimated_duration": 3600.041984733519,
    "input_throughput": 6357.6102992849355,
    "output_throughput": 5578.819381876362,
    "total_throughput": 11936.429681161297,
    "itl": 143.4780947004903,
    "ttft": 115738.87844853658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30310992879560233,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90521626538691
}
#Debug simulation 
Total elapsed time: 14.899901609867811. Arrivals time: 0.20586123643442988 Scheduler time: 14.583604037761688 Scheduler overhead time: 0.04206553939729929 Adapter cache time: 0.007217023521661758 Engine time: 0.04242420056834817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 14.856261264998466,
    "estimated_duration": 3600.037320023483,
    "input_throughput": 6357.618537090805,
    "output_throughput": 5578.8266105721905,
    "total_throughput": 11936.445147662997,
    "itl": 143.48075768328889,
    "ttft": 115738.50919611883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3212922904267907,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90538323859741
}
#Debug simulation 
Total elapsed time: 14.856365498155355. Arrivals time: 0.20526733342558146 Scheduler time: 14.540906513109803 Scheduler overhead time: 0.04205095116049051 Adapter cache time: 0.007164826616644859 Engine time: 0.042213999666273594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 14.791551208123565,
    "estimated_duration": 3600.12796997419,
    "input_throughput": 6357.458454501573,
    "output_throughput": 5578.686137688596,
    "total_throughput": 11936.14459219017,
    "itl": 143.47826754493647,
    "ttft": 115891.1706716341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29003487860551136,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90754698755369
}
#Debug simulation 
Total elapsed time: 14.791639296803623. Arrivals time: 0.2039849148131907 Scheduler time: 14.478133333381265 Scheduler overhead time: 0.041510209906846285 Adapter cache time: 0.007190072908997536 Engine time: 0.04206976853311062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 66, 66, 66, 66, 4320, 66, 33, 4320, 33, 66, 66, 33, 4320, 4320, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 33, 66, 66, 4320, 66, 33, 33, 66, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 4320, 4320, 33, 66, 4320, 33, 66, 33, 66, 33, 33, 66, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 66, 33, 33, 33, 33, 33, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 33, 66, 66, 4320, 33, 33, 33, 4320, 4320, 66, 4320, 66, 4320, 4320, 66, 66, 33, 4320, 66, 66, 66, 4320, 66, 33, 66, 4320, 4320, 66, 4320, 66, 33, 4320, 66, 33, 66, 66, 33, 66, 4320, 33, 66, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 66, 33, 33, 66, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 66, 33, 4320, 4320, 66, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 66, 66, 66, 66, 66, 66, 33, 4320, 66, 4320, 33, 4320, 66, 33, 66, 33, 33, 66, 66, 66, 4320, 66, 33, 33]
Prompts retrieved: 282816 . Total input tokens: 63124961 . Total output tokens: 56509939
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 14.8137541199103,
    "estimated_duration": 3600.0370815380807,
    "input_throughput": 6357.618958252916,
    "output_throughput": 5578.826980143025,
    "total_throughput": 11936.44593839594,
    "itl": 143.4798377368819,
    "ttft": 115736.75075603175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.325316411592066,
    "arrivals": 94443,
    "finished_requests": 91505,
    "scheduler_time": 80.90510783562439
}
#Debug simulation 
Total elapsed time: 14.813860886264592. Arrivals time: 0.20220301114022732 Scheduler time: 14.502183236647397 Scheduler overhead time: 0.041893749963492155 Adapter cache time: 0.007143239490687847 Engine time: 0.0417965236119926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.9742789873853326,
    "estimated_duration": 3599.757867525493,
    "input_throughput": 2795.201613634293,
    "output_throughput": 2441.464210491863,
    "total_throughput": 5236.665824126156,
    "itl": 42.56723538868558,
    "ttft": 27215.494925245297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.88343930534651,
    "arrivals": 40505,
    "finished_requests": 40293,
    "scheduler_time": 21.492220407265346
}
#Debug simulation 
Total elapsed time: 3.974352182354778. Arrivals time: 0.10295915836468339 Scheduler time: 3.5344406869262457 Scheduler overhead time: 0.09026906825602055 Adapter cache time: 0.1193819991312921 Engine time: 0.08610000275075436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.959760702215135,
    "estimated_duration": 3599.760117042822,
    "input_throughput": 2795.530166678684,
    "output_throughput": 2441.685199629496,
    "total_throughput": 5237.21536630818,
    "itl": 42.61462645543882,
    "ttft": 26762.686647718285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.469953513335646,
    "arrivals": 40505,
    "finished_requests": 40296,
    "scheduler_time": 21.48841217539876
}
#Debug simulation 
Total elapsed time: 3.9598334482870996. Arrivals time: 0.10292140254750848 Scheduler time: 3.516667447052896 Scheduler overhead time: 0.09317088313400745 Adapter cache time: 0.1198693267069757 Engine time: 0.0855393223464489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.9168138508684933,
    "estimated_duration": 3599.781316259988,
    "input_throughput": 2795.5137037199956,
    "output_throughput": 2441.6708204741385,
    "total_throughput": 5237.184524194134,
    "itl": 42.61666837516345,
    "ttft": 26732.13643913909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11507,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.58013088629112,
    "arrivals": 40505,
    "finished_requests": 40296,
    "scheduler_time": 21.485474572407583
}
#Debug simulation 
Total elapsed time: 3.916906022001058. Arrivals time: 0.1024150918237865 Scheduler time: 3.4757159813307226 Scheduler overhead time: 0.0932148965075612 Adapter cache time: 0.11884220130741596 Engine time: 0.0851950878277421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 3.9653396769426763,
    "estimated_duration": 3599.7652141565454,
    "input_throughput": 2795.195909008088,
    "output_throughput": 2441.4592277955717,
    "total_throughput": 5236.65513680366,
    "itl": 42.57688645479717,
    "ttft": 27245.923433276792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 35.606902443218154,
    "arrivals": 40505,
    "finished_requests": 40293,
    "scheduler_time": 21.500227924649707
}
#Debug simulation 
Total elapsed time: 3.965419127140194. Arrivals time: 0.10283638443797827 Scheduler time: 3.5245766658335924 Scheduler overhead time: 0.09053993504494429 Adapter cache time: 0.11925293179228902 Engine time: 0.0867755264043808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 3.9471881510689855,
    "estimated_duration": 3599.73803911106,
    "input_throughput": 2795.4506385372943,
    "output_throughput": 2441.6582274888224,
    "total_throughput": 5237.108866026117,
    "itl": 42.623767338794316,
    "ttft": 26833.755663073927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.970428853427975,
    "arrivals": 40505,
    "finished_requests": 40295,
    "scheduler_time": 21.48907505403915
}
#Debug simulation 
Total elapsed time: 3.9472521380521357. Arrivals time: 0.10278665646910667 Scheduler time: 3.506896165665239 Scheduler overhead time: 0.09055320033803582 Adapter cache time: 0.1195628959685564 Engine time: 0.085683508310467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.9798340308479965,
    "estimated_duration": 3599.7398256958113,
    "input_throughput": 2795.2156231332797,
    "output_throughput": 2441.4764470655023,
    "total_throughput": 5236.692070198782,
    "itl": 42.5500274084405,
    "ttft": 27219.356625524422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11400,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 34.086573361883104,
    "arrivals": 40505,
    "finished_requests": 40293,
    "scheduler_time": 21.487881236643958
}
#Debug simulation 
Total elapsed time: 3.97989801177755. Arrivals time: 0.1031471649184823 Scheduler time: 3.5377564462833107 Scheduler overhead time: 0.09105990175157785 Adapter cache time: 0.12006799690425396 Engine time: 0.08622910780832171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [64 64 64]
Adapter prompts. [1080, 270, 1080, 270, 1080, 270, 1080, 270, 270, 270, 540, 540, 540, 540, 1080, 540, 270, 1080, 270, 540, 540, 270, 1080, 1080, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 270, 540, 540, 1080, 540, 270, 270, 540, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 1080, 1080, 270, 540, 1080, 270, 540, 270, 540, 270, 270, 540, 1080, 1080, 270, 270, 270, 1080, 1080, 1080, 540, 270, 270, 270, 270, 270, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 270, 540, 540, 1080, 270, 270, 270, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 270, 1080, 540, 540, 540, 1080, 540, 270, 540, 1080, 1080, 540, 1080, 540, 270, 1080, 540, 270, 540, 540, 270, 540, 1080, 270, 540, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 540, 270, 270, 540, 1080, 1080, 1080, 270, 1080, 270, 1080, 270, 540, 270, 1080, 1080, 540, 1080, 270, 1080, 1080, 1080, 270, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 270, 1080, 540, 1080, 270, 1080, 540, 270, 540, 270, 270, 540, 540, 540, 1080, 540, 270, 270]
Prompts retrieved: 120960 . Total input tokens: 26955942 . Total output tokens: 24190742
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.9546812237240374,
    "estimated_duration": 3599.772685307538,
    "input_throughput": 2795.2178872484455,
    "output_throughput": 2441.5280542218843,
    "total_throughput": 5236.74594147033,
    "itl": 42.61882386671687,
    "ttft": 27326.370267054263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 11251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.69986113231506,
    "arrivals": 40505,
    "finished_requests": 40293,
    "scheduler_time": 21.523796147140875
}
#Debug simulation 
Total elapsed time: 3.954743368551135. Arrivals time: 0.10270560998469591 Scheduler time: 3.515831436496228 Scheduler overhead time: 0.09061887953430414 Adapter cache time: 0.11793631827458739 Engine time: 0.08636392559856176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.4516680277884007,
    "estimated_duration": 3600.035465264698,
    "input_throughput": 2581.775954620105,
    "output_throughput": 2336.7847570305694,
    "total_throughput": 4918.560711650674,
    "itl": 41.70076284607596,
    "ttft": 14600.452492914845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 38.813105568552885,
    "arrivals": 37714,
    "finished_requests": 37609,
    "scheduler_time": 19.114263801457955
}
#Debug simulation 
Total elapsed time: 3.451731509063393. Arrivals time: 0.09651762573048472 Scheduler time: 3.009184857364744 Scheduler overhead time: 0.09141921112313867 Adapter cache time: 0.1268258229829371 Engine time: 0.0859666895121336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.5197117500938475,
    "estimated_duration": 3600.0254617610753,
    "input_throughput": 2581.7264624160143,
    "output_throughput": 2336.790694776013,
    "total_throughput": 4918.517157192027,
    "itl": 41.7456781241984,
    "ttft": 14622.066207469577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.22619284267275,
    "arrivals": 37714,
    "finished_requests": 37608,
    "scheduler_time": 19.131789646081327
}
#Debug simulation 
Total elapsed time: 3.5197758739814162. Arrivals time: 0.0973915969952941 Scheduler time: 3.0713289827108383 Scheduler overhead time: 0.0921551794745028 Adapter cache time: 0.12952625332400203 Engine time: 0.0873859217390418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.5196204017847776,
    "estimated_duration": 3600.023926103989,
    "input_throughput": 2581.7275636994,
    "output_throughput": 2336.7916915774963,
    "total_throughput": 4918.519255276896,
    "itl": 41.74685323891174,
    "ttft": 14622.787388143004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.28110244652925,
    "arrivals": 37714,
    "finished_requests": 37608,
    "scheduler_time": 19.1322777881375
}
#Debug simulation 
Total elapsed time: 3.5196836008690298. Arrivals time: 0.09647967433556914 Scheduler time: 3.070642292033881 Scheduler overhead time: 0.09449504548683763 Adapter cache time: 0.12916046055033803 Engine time: 0.08667467813938856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 3.5014300527982414,
    "estimated_duration": 3600.0368941483694,
    "input_throughput": 2581.7749298924114,
    "output_throughput": 2336.7838295418574,
    "total_throughput": 4918.558759434269,
    "itl": 41.71392046307286,
    "ttft": 14603.183025140857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.62148745818953,
    "arrivals": 37714,
    "finished_requests": 37609,
    "scheduler_time": 19.11960009212656
}
#Debug simulation 
Total elapsed time: 3.501491861883551. Arrivals time: 0.0965908570215106 Scheduler time: 3.0561096742749214 Scheduler overhead time: 0.09162362711504102 Adapter cache time: 0.12807537661865354 Engine time: 0.08717497996985912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 3.494097793009132,
    "estimated_duration": 3600.017880843602,
    "input_throughput": 2581.6424550152847,
    "output_throughput": 2336.709782682731,
    "total_throughput": 4918.352237698016,
    "itl": 41.75378431311043,
    "ttft": 14917.156818499821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 41.75999926479867,
    "arrivals": 37714,
    "finished_requests": 37605,
    "scheduler_time": 19.13622905784025
}
#Debug simulation 
Total elapsed time: 3.49418041203171. Arrivals time: 0.09659262700006366 Scheduler time: 3.048538299277425 Scheduler overhead time: 0.09172122506424785 Adapter cache time: 0.12835264904424548 Engine time: 0.08698352100327611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.5105716111138463,
    "estimated_duration": 3600.0298076054523,
    "input_throughput": 2581.7800120333436,
    "output_throughput": 2336.788429425686,
    "total_throughput": 4918.56844145903,
    "itl": 41.68661187171118,
    "ttft": 14577.334931567837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12703,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 37.98260889614397,
    "arrivals": 37714,
    "finished_requests": 37609,
    "scheduler_time": 19.106303446264917
}
#Debug simulation 
Total elapsed time: 3.5106550068594515. Arrivals time: 0.09720224374905229 Scheduler time: 3.064998237416148 Scheduler overhead time: 0.09175385162234306 Adapter cache time: 0.12806844897568226 Engine time: 0.08633940853178501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 540, 540, 540, 540, 1080, 540, 135, 1080, 135, 540, 540, 135, 1080, 1080, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 135, 540, 540, 1080, 540, 135, 135, 540, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 1080, 1080, 135, 540, 1080, 135, 540, 135, 540, 135, 135, 540, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 540, 135, 135, 135, 135, 135, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 135, 540, 540, 1080, 135, 135, 135, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 135, 1080, 540, 540, 540, 1080, 540, 135, 540, 1080, 1080, 540, 1080, 540, 135, 1080, 540, 135, 540, 540, 135, 540, 1080, 135, 540, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 540, 135, 135, 540, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 540, 135, 1080, 1080, 540, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 135, 1080, 540, 1080, 135, 1080, 540, 135, 540, 135, 135, 540, 540, 540, 1080, 540, 135, 135]
Prompts retrieved: 112320 . Total input tokens: 25029474 . Total output tokens: 22460635
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.523798632901162,
    "estimated_duration": 3600.0330257364294,
    "input_throughput": 2581.631594365391,
    "output_throughput": 2336.6999524342377,
    "total_throughput": 4918.331546799629,
    "itl": 41.762438813233764,
    "ttft": 15020.960071330333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12632,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.270120678246016,
    "arrivals": 37714,
    "finished_requests": 37605,
    "scheduler_time": 19.140530459499832
}
#Debug simulation 
Total elapsed time: 3.5238608112558722. Arrivals time: 0.09681467246264219 Scheduler time: 3.079036964569241 Scheduler overhead time: 0.09145792992785573 Adapter cache time: 0.12745213136076927 Engine time: 0.08703944692388177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.1582238590344787,
    "estimated_duration": 3600.015542341077,
    "input_throughput": 2492.627294093449,
    "output_throughput": 2193.451919061705,
    "total_throughput": 4686.079213155154,
    "itl": 40.316783145538956,
    "ttft": 12916.916387682495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.70682547109727,
    "arrivals": 36267,
    "finished_requests": 36158,
    "scheduler_time": 16.25838077418369
}
#Debug simulation 
Total elapsed time: 3.158284931909293. Arrivals time: 0.09347682213410735 Scheduler time: 2.6994544859044254 Scheduler overhead time: 0.0927723697386682 Adapter cache time: 0.1411181348375976 Engine time: 0.08866131212562323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.134306746069342,
    "estimated_duration": 3600.021161771021,
    "input_throughput": 2492.342293784187,
    "output_throughput": 2193.287107266792,
    "total_throughput": 4685.629401050979,
    "itl": 40.378942418262014,
    "ttft": 13475.134291401484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13998,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.54556553951753,
    "arrivals": 36267,
    "finished_requests": 36154,
    "scheduler_time": 16.30269994388348
}
#Debug simulation 
Total elapsed time: 3.1343841343186796. Arrivals time: 0.09268290223553777 Scheduler time: 2.677593973930925 Scheduler overhead time: 0.0955834980122745 Adapter cache time: 0.1376038920134306 Engine time: 0.08802843233570457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.16656384896487,
    "estimated_duration": 3600.0006971067437,
    "input_throughput": 2492.3561840449156,
    "output_throughput": 2193.2048530839184,
    "total_throughput": 4685.561037128834,
    "itl": 40.38102378203151,
    "ttft": 13578.534577900327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13993,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.63364137391542,
    "arrivals": 36267,
    "finished_requests": 36153,
    "scheduler_time": 16.30353562100754
}
#Debug simulation 
Total elapsed time: 3.166627387981862. Arrivals time: 0.09318207204341888 Scheduler time: 2.7102478370070457 Scheduler overhead time: 0.09343307418748736 Adapter cache time: 0.13856669515371323 Engine time: 0.08836558787152171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 3.1790719986893237,
    "estimated_duration": 3600.0123310349923,
    "input_throughput": 2492.3484074346043,
    "output_throughput": 2193.292487342664,
    "total_throughput": 4685.640894777268,
    "itl": 40.35036216912573,
    "ttft": 13462.831926247818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14008,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.75885125668073,
    "arrivals": 36267,
    "finished_requests": 36154,
    "scheduler_time": 16.28901245876058
}
#Debug simulation 
Total elapsed time: 3.1791537436656654. Arrivals time: 0.09351741569116712 Scheduler time: 2.7221183823421597 Scheduler overhead time: 0.09381674462929368 Adapter cache time: 0.1385667002759874 Engine time: 0.08819089643657207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 3.1416094922460616,
    "estimated_duration": 3600.015129488034,
    "input_throughput": 2492.3464700205295,
    "output_throughput": 2193.2907823981536,
    "total_throughput": 4685.637252418683,
    "itl": 40.3904834316031,
    "ttft": 13481.260403343058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.17233235646571,
    "arrivals": 36267,
    "finished_requests": 36154,
    "scheduler_time": 16.307540704906486
}
#Debug simulation 
Total elapsed time: 3.141690941993147. Arrivals time: 0.09296969650313258 Scheduler time: 2.6882934104651213 Scheduler overhead time: 0.09292119881138206 Adapter cache time: 0.1369709726423025 Engine time: 0.08783272048458457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.15130918007344,
    "estimated_duration": 3599.9987860965784,
    "input_throughput": 2492.6388960619124,
    "output_throughput": 2193.4621285142175,
    "total_throughput": 4686.10102457613,
    "itl": 40.298066501659626,
    "ttft": 12909.558808766556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 42.71884856327307,
    "arrivals": 36267,
    "finished_requests": 36158,
    "scheduler_time": 16.25062303934696
}
#Debug simulation 
Total elapsed time: 3.1514096348546445. Arrivals time: 0.09384998772293329 Scheduler time: 2.6914550056681037 Scheduler overhead time: 0.09351506736129522 Adapter cache time: 0.14047803776338696 Engine time: 0.08887298358604312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 540, 540, 540, 540, 1080, 540, 66, 1080, 66, 540, 540, 66, 1080, 1080, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 66, 540, 540, 1080, 540, 66, 66, 540, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 1080, 1080, 66, 540, 1080, 66, 540, 66, 540, 66, 66, 540, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 540, 66, 66, 66, 66, 66, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 66, 540, 540, 1080, 66, 66, 66, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 66, 1080, 540, 540, 540, 1080, 540, 66, 540, 1080, 1080, 540, 1080, 540, 66, 1080, 540, 66, 540, 540, 66, 540, 1080, 66, 540, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 540, 66, 66, 540, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 540, 66, 1080, 1080, 540, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 66, 1080, 540, 1080, 66, 1080, 540, 66, 540, 66, 66, 540, 540, 540, 1080, 540, 66, 66]
Prompts retrieved: 107904 . Total input tokens: 24015577 . Total output tokens: 21599596
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.1373970676213503,
    "estimated_duration": 3600.028347893602,
    "input_throughput": 2492.337318746352,
    "output_throughput": 2193.2827291818203,
    "total_throughput": 4685.620047928172,
    "itl": 40.399065786712434,
    "ttft": 13486.347354305348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 13972,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.698464903981815,
    "arrivals": 36267,
    "finished_requests": 36154,
    "scheduler_time": 16.31161552386734
}
#Debug simulation 
Total elapsed time: 3.1374591016210616. Arrivals time: 0.09272454865276814 Scheduler time: 2.682912131305784 Scheduler overhead time: 0.09301016153767705 Adapter cache time: 0.13854300556704402 Engine time: 0.08758049132302403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.011788318865001,
    "estimated_duration": 3600.037847636907,
    "input_throughput": 2445.768175959477,
    "output_throughput": 2144.623564185011,
    "total_throughput": 4590.391740144489,
    "itl": 39.778546378010404,
    "ttft": 12247.336867237786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14821,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.359488852819574,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.223033499134237
}
#Debug simulation 
Total elapsed time: 3.011850330978632. Arrivals time: 0.09195557096973062 Scheduler time: 2.5497988807037473 Scheduler overhead time: 0.09335424331948161 Adapter cache time: 0.14471789682283998 Engine time: 0.08888424513861537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.002288954798132,
    "estimated_duration": 3600.0433741270067,
    "input_throughput": 2445.7644214175994,
    "output_throughput": 2144.62027193554,
    "total_throughput": 4590.3846933531395,
    "itl": 39.83006890511387,
    "ttft": 12261.284199106796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.118949052125785,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.245767687730941
}
#Debug simulation 
Total elapsed time: 3.002363078761846. Arrivals time: 0.0919686988927424 Scheduler time: 2.5419612042605877 Scheduler overhead time: 0.09272910980507731 Adapter cache time: 0.145158302038908 Engine time: 0.08752781711518764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.0293190218508244,
    "estimated_duration": 3600.0024594337024,
    "input_throughput": 2445.792217982275,
    "output_throughput": 2144.6446459412996,
    "total_throughput": 4590.4368639235745,
    "itl": 39.832163413967535,
    "ttft": 12160.707940694401,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.22522678949997,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.246022055989677
}
#Debug simulation 
Total elapsed time: 3.029381573665887. Arrivals time: 0.09167289221659303 Scheduler time: 2.5666645551100373 Scheduler overhead time: 0.09411664586514235 Adapter cache time: 0.14617130812257528 Engine time: 0.08739264076575637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 3.0217491020448506,
    "estimated_duration": 3600.024186632656,
    "input_throughput": 2445.7774569108587,
    "output_throughput": 2144.6317023835645,
    "total_throughput": 4590.409159294423,
    "itl": 39.797052128702404,
    "ttft": 12252.233793316953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14810,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.247579753888,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.230168650524007
}
#Debug simulation 
Total elapsed time: 3.0218278667889535. Arrivals time: 0.09215848194435239 Scheduler time: 2.552094967570156 Scheduler overhead time: 0.09573907451704144 Adapter cache time: 0.14648548513650894 Engine time: 0.09221310541033745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 3.0286065791733563,
    "estimated_duration": 3600.016431071721,
    "input_throughput": 2445.7827258801713,
    "output_throughput": 2144.636322590769,
    "total_throughput": 4590.419048470941,
    "itl": 39.8430904864667,
    "ttft": 12264.975157292336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14780,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.79952639168765,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.250913565528124
}
#Debug simulation 
Total elapsed time: 3.0286944289691746. Arrivals time: 0.09227631473913789 Scheduler time: 2.5642666020430624 Scheduler overhead time: 0.09413217287510633 Adapter cache time: 0.1464205109514296 Engine time: 0.08808641042560339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 3.0296334070153534,
    "estimated_duration": 3600.0006251236377,
    "input_throughput": 2445.79346418797,
    "output_throughput": 2144.645738703126,
    "total_throughput": 4590.439202891096,
    "itl": 39.76081220932525,
    "ttft": 12242.78077428602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14826,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 44.33048567222672,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.214667483232912
}
#Debug simulation 
Total elapsed time: 3.0296982750296593. Arrivals time: 0.09205547859892249 Scheduler time: 2.56539144506678 Scheduler overhead time: 0.09366949228569865 Adapter cache time: 0.1456719203852117 Engine time: 0.08985948888584971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [64 64 64]
Adapter prompts. [1080, 33, 1080, 33, 1080, 33, 1080, 33, 33, 33, 540, 540, 540, 540, 1080, 540, 33, 1080, 33, 540, 540, 33, 1080, 1080, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 33, 540, 540, 1080, 540, 33, 33, 540, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 1080, 1080, 33, 540, 1080, 33, 540, 33, 540, 33, 33, 540, 1080, 1080, 33, 33, 33, 1080, 1080, 1080, 540, 33, 33, 33, 33, 33, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 33, 540, 540, 1080, 33, 33, 33, 1080, 1080, 540, 1080, 540, 1080, 1080, 540, 540, 33, 1080, 540, 540, 540, 1080, 540, 33, 540, 1080, 1080, 540, 1080, 540, 33, 1080, 540, 33, 540, 540, 33, 540, 1080, 33, 540, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 540, 33, 33, 540, 1080, 1080, 1080, 33, 1080, 33, 1080, 33, 540, 33, 1080, 1080, 540, 1080, 33, 1080, 1080, 1080, 33, 1080, 1080, 1080, 540, 540, 540, 540, 540, 540, 33, 1080, 540, 1080, 33, 1080, 540, 33, 540, 33, 33, 540, 540, 540, 1080, 540, 33, 33]
Prompts retrieved: 105792 . Total input tokens: 23538003 . Total output tokens: 21187347
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 2.993115544319153,
    "estimated_duration": 3600.007728572837,
    "input_throughput": 2445.788638206768,
    "output_throughput": 2144.641506939418,
    "total_throughput": 4590.430145146186,
    "itl": 39.85394454107081,
    "ttft": 12268.213818213946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.39904929127754,
    "arrivals": 35501,
    "finished_requests": 35392,
    "scheduler_time": 15.255667073382156
}
#Debug simulation 
Total elapsed time: 2.9932192312553525. Arrivals time: 0.09095073817297816 Scheduler time: 2.5343286362476647 Scheduler overhead time: 0.09297675499692559 Adapter cache time: 0.14513501059263945 Engine time: 0.08695489540696144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 2.6644623829051852,
    "estimated_duration": 3599.996925353962,
    "input_throughput": 2180.769918082108,
    "output_throughput": 1937.9019328784723,
    "total_throughput": 4118.67185096058,
    "itl": 36.39940914223239,
    "ttft": 12083.005653291808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 45.04119812745083,
    "arrivals": 31858,
    "finished_requests": 31753,
    "scheduler_time": 10.450807604814068
}
#Debug simulation 
Total elapsed time: 2.6645256229676306. Arrivals time: 0.08361226739361882 Scheduler time: 2.196010410785675 Scheduler overhead time: 0.09770426619797945 Adapter cache time: 0.15019928477704525 Engine time: 0.09170932322740555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 2.6537984162569046,
    "estimated_duration": 3599.978941081408,
    "input_throughput": 2180.7808124682215,
    "output_throughput": 1937.9116139785883,
    "total_throughput": 4118.69242644681,
    "itl": 36.455771020160846,
    "ttft": 12086.289532673363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 47.91477328897443,
    "arrivals": 31858,
    "finished_requests": 31753,
    "scheduler_time": 10.478911137870554
}
#Debug simulation 
Total elapsed time: 2.6538608041591942. Arrivals time: 0.08312272047623992 Scheduler time: 2.186570605263114 Scheduler overhead time: 0.09741986868903041 Adapter cache time: 0.14975886838510633 Engine time: 0.09162806486710906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 2.6463194582611322,
    "estimated_duration": 3599.97154410173,
    "input_throughput": 2180.7852933901268,
    "output_throughput": 1937.915595869181,
    "total_throughput": 4118.700889259308,
    "itl": 36.46039504678326,
    "ttft": 12086.356494767006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14706,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.018159719622844,
    "arrivals": 31858,
    "finished_requests": 31753,
    "scheduler_time": 10.479967812251719
}
#Debug simulation 
Total elapsed time: 2.6463826010003686. Arrivals time: 0.08320171991363168 Scheduler time: 2.1766530526801944 Scheduler overhead time: 0.09933091653510928 Adapter cache time: 0.14983566803857684 Engine time: 0.09210953628644347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 2.6478105559945107,
    "estimated_duration": 3599.9694210570146,
    "input_throughput": 2180.786579485688,
    "output_throughput": 1937.9167387348512,
    "total_throughput": 4118.70331822054,
    "itl": 36.417634143545605,
    "ttft": 12083.974842788379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 46.01077320362434,
    "arrivals": 31858,
    "finished_requests": 31753,
    "scheduler_time": 10.460202679202338
}
#Debug simulation 
Total elapsed time: 2.647873836569488. Arrivals time: 0.08301806030794978 Scheduler time: 2.1804396039806306 Scheduler overhead time: 0.09798342827707529 Adapter cache time: 0.14928678143769503 Engine time: 0.09203665796667337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 2.700027170125395,
    "estimated_duration": 3599.996795115204,
    "input_throughput": 2180.7372191698773,
    "output_throughput": 1937.7633917523424,
    "total_throughput": 4118.5006109222195,
    "itl": 36.79486235160076,
    "ttft": 12382.42371184609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 48.77035641838492,
    "arrivals": 31858,
    "finished_requests": 31751,
    "scheduler_time": 10.664640167621677
}
#Debug simulation 
Total elapsed time: 2.7000906807370484. Arrivals time: 0.08386311074718833 Scheduler time: 2.2307260995730758 Scheduler overhead time: 0.097645727917552 Adapter cache time: 0.15021452913060784 Engine time: 0.0922163869254291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 2.6693728221580386,
    "estimated_duration": 3599.982782094253,
    "input_throughput": 2180.7784856773396,
    "output_throughput": 1937.9095463177541,
    "total_throughput": 4118.688031995093,
    "itl": 36.37874923514944,
    "ttft": 12081.744887059362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 43.998590089492296,
    "arrivals": 31858,
    "finished_requests": 31753,
    "scheduler_time": 10.440526309691943
}
#Debug simulation 
Total elapsed time: 2.6694366629235446. Arrivals time: 0.08266647811979055 Scheduler time: 2.204034015070647 Scheduler overhead time: 0.09739365568384528 Adapter cache time: 0.14969050651416183 Engine time: 0.09030899265781045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_192_slots_64_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [64 64 64]
Adapter prompts. [1080, 135, 1080, 135, 1080, 135, 1080, 135, 135, 135, 270, 270, 270, 270, 1080, 270, 135, 1080, 135, 270, 270, 135, 1080, 1080, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 135, 270, 270, 1080, 270, 135, 135, 270, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 1080, 1080, 135, 270, 1080, 135, 270, 135, 270, 135, 135, 270, 1080, 1080, 135, 135, 135, 1080, 1080, 1080, 270, 135, 135, 135, 135, 135, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 135, 270, 270, 1080, 135, 135, 135, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 135, 1080, 270, 270, 270, 1080, 270, 135, 270, 1080, 1080, 270, 1080, 270, 135, 1080, 270, 135, 270, 270, 135, 270, 1080, 135, 270, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 270, 135, 135, 270, 1080, 1080, 1080, 135, 1080, 135, 1080, 135, 270, 135, 1080, 1080, 270, 1080, 135, 1080, 1080, 1080, 135, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 135, 1080, 270, 1080, 135, 1080, 270, 135, 270, 135, 135, 270, 270, 270, 1080, 270, 135, 135]
Prompts retrieved: 95040 . Total input tokens: 21168775 . Total output tokens: 19051203
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 2.6737756319344044,
    "estimated_duration": 3599.9973946432524,
    "input_throughput": 2180.7368559993006,
    "output_throughput": 1937.7630690455799,
    "total_throughput": 4118.499925044881,
    "itl": 36.80499250544663,
    "ttft": 12383.234790815992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 14747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 49.36740287002361,
    "arrivals": 31858,
    "finished_requests": 31751,
    "scheduler_time": 10.670338085994096
}
#Debug simulation 
Total elapsed time: 2.6738400389440358. Arrivals time: 0.08340707002207637 Scheduler time: 2.21078208880499 Scheduler overhead time: 0.09664568118751049 Adapter cache time: 0.14874307438731194 Engine time: 0.08925403095781803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.1-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_192_slots_64_rate_0.1-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [64 64 64]
Adapter prompts. [1080, 66, 1080, 66, 1080, 66, 1080, 66, 66, 66, 270, 270, 270, 270, 1080, 270, 66, 1080, 66, 270, 270, 66, 1080, 1080, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 1080, 270, 1080, 66, 270, 270, 1080, 270, 66, 66, 270, 1080, 66, 270, 66, 1080, 1080, 270, 1080, 1080, 1080, 66, 270, 1080, 66, 270, 66, 270, 66, 66, 270, 1080, 1080, 66, 66, 66, 1080, 1080, 1080, 270, 66, 66, 66, 66, 66, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 66, 270, 270, 1080, 66, 66, 66, 1080, 1080, 270, 1080, 270, 1080, 1080, 270, 270, 66, 1080, 270, 270, 270, 1080, 270, 66, 270, 1080, 1080, 270, 1080, 270, 66, 1080, 270, 66, 270, 270, 66, 270, 1080, 66, 270, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 270, 66, 66, 270, 1080, 1080, 1080, 66, 1080, 66, 1080, 66, 270, 66, 1080, 1080, 270, 1080, 66, 1080, 1080, 1080, 66, 1080, 1080, 1080, 270, 270, 270, 270, 270, 270, 66, 1080, 270, 1080, 66, 1080, 270, 66, 270, 66, 66, 270, 270, 270, 1080, 270, 66, 66]
Prompts retrieved: 90624 . Total input tokens: 20161865 . Total output tokens: 18163334
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 2.507588622160256,
    "estimated_duration": 3599.982099425214,
    "input_throughput": 2066.7149987184434,
    "output_throughput": 1844.8502844112459,
    "total_throughput": 3911.5652831296893,
    "itl": 34.19038769010021,
    "ttft": 9184.971056769189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 12925,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 39.55680409032792,
    "arrivals": 30402,
    "finished_requests": 30325,
    "scheduler_time": 7.915671378478169
}
#Debug simulation 
Total elapsed time: 2.507683719974011. Arrivals time: 0.07872335938736796 Scheduler time: 2.0434349831193686 Scheduler overhead time: 0.10169814201071858 Adapter cache time: 0.1409417144022882 Engine time: 0.0958825503475964 
