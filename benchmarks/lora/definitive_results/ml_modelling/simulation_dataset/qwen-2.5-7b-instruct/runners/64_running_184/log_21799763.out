INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.591363142244518,
    "estimated_duration": 3600.1610592600737,
    "input_throughput": 5242.567398881625,
    "output_throughput": 4569.816941295768,
    "total_throughput": 9812.384340177392,
    "itl": 154.08548493634353,
    "ttft": 2102560.2870322866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 757465,
    "finished_requests": 75832,
    "scheduler_time": 41.40198189225297
}
#Debug simulation 
Total elapsed time: 5.591532947961241. Arrivals time: 0.26424419367685914 Scheduler time: 5.212756004650146 Scheduler overhead time: 0.03604934923350811 Adapter cache time: 0.024250961374491453 Engine time: 0.037438471801579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.681320554111153,
    "estimated_duration": 3600.1961251219186,
    "input_throughput": 5383.015626501101,
    "output_throughput": 4681.5277318896715,
    "total_throughput": 10064.543358390772,
    "itl": 182.31050121366846,
    "ttft": 2087905.4845590785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4002960364404137,
    "arrivals": 757465,
    "finished_requests": 77819,
    "scheduler_time": 47.20842610477955
}
#Debug simulation 
Total elapsed time: 5.6814367631450295. Arrivals time: 0.2685002158395946 Scheduler time: 5.315423548687249 Scheduler overhead time: 0.030787896364927292 Adapter cache time: 0.020649985410273075 Engine time: 0.031704978086054325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.796621846035123,
    "estimated_duration": 3600.166433688081,
    "input_throughput": 5242.55957263204,
    "output_throughput": 4569.810119346669,
    "total_throughput": 9812.369691978709,
    "itl": 154.08562265412417,
    "ttft": 2102564.223694938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 757465,
    "finished_requests": 75832,
    "scheduler_time": 41.40194890744403
}
#Debug simulation 
Total elapsed time: 5.7967463517561555. Arrivals time: 0.2730510924011469 Scheduler time: 5.40954838367179 Scheduler overhead time: 0.03580575715750456 Adapter cache time: 0.02421785844489932 Engine time: 0.037322350312024355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.712557976134121,
    "estimated_duration": 3600.1255241389076,
    "input_throughput": 5383.12091343964,
    "output_throughput": 4681.505655009407,
    "total_throughput": 10064.626568449046,
    "itl": 182.31006612044217,
    "ttft": 2087874.1279260644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 757465,
    "finished_requests": 77818,
    "scheduler_time": 47.20770651852956
}
#Debug simulation 
Total elapsed time: 5.712659373879433. Arrivals time: 0.31629608292132616 Scheduler time: 5.2988784234039485 Scheduler overhead time: 0.030738368164747953 Adapter cache time: 0.020966861862689257 Engine time: 0.031555600464344025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.603772532194853,
    "estimated_duration": 3600.0722062138725,
    "input_throughput": 5242.962895972169,
    "output_throughput": 4570.05694819183,
    "total_throughput": 9813.019844163999,
    "itl": 154.09495502033744,
    "ttft": 2102511.5872486318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 757465,
    "finished_requests": 75834,
    "scheduler_time": 41.403684997582765
}
#Debug simulation 
Total elapsed time: 5.6038819290697575. Arrivals time: 0.27505270950496197 Scheduler time: 5.215210422873497 Scheduler overhead time: 0.03581912349909544 Adapter cache time: 0.02413726458325982 Engine time: 0.036948677618056536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.840827172156423,
    "estimated_duration": 3600.002224572958,
    "input_throughput": 5500.666878712559,
    "output_throughput": 4838.812565474561,
    "total_throughput": 10339.47944418712,
    "itl": 177.0296172304558,
    "ttft": 2074312.284164032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 749804,
    "finished_requests": 80050,
    "scheduler_time": 48.89955928409642
}
#Debug simulation 
Total elapsed time: 5.840933541301638. Arrivals time: 0.27323185419663787 Scheduler time: 5.472624511923641 Scheduler overhead time: 0.03152306145057082 Adapter cache time: 0.01636633975431323 Engine time: 0.032574123702943325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2419809848070145,
    "estimated_duration": 3600.028213420112,
    "input_throughput": 5500.627169026334,
    "output_throughput": 4838.777633759386,
    "total_throughput": 10339.404802785719,
    "itl": 177.03022825201043,
    "ttft": 2074331.409435612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 749804,
    "finished_requests": 80050,
    "scheduler_time": 48.89942492734051
}
#Debug simulation 
Total elapsed time: 6.242057071998715. Arrivals time: 0.29249272821471095 Scheduler time: 5.854051135480404 Scheduler overhead time: 0.03154199430719018 Adapter cache time: 0.016529009211808443 Engine time: 0.03277716925367713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.734168295748532,
    "estimated_duration": 3600.1475988714574,
    "input_throughput": 5330.170353575316,
    "output_throughput": 4696.470501737253,
    "total_throughput": 10026.64085531257,
    "itl": 149.2731402835906,
    "ttft": 2091137.5061215023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 749804,
    "finished_requests": 77555,
    "scheduler_time": 42.50684396149871
}
#Debug simulation 
Total elapsed time: 5.734284440986812. Arrivals time: 0.26857155840843916 Scheduler time: 5.354944395367056 Scheduler overhead time: 0.03679950116202235 Adapter cache time: 0.018912970554083586 Engine time: 0.03790625464171171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.9116947669535875,
    "estimated_duration": 3600.010726549778,
    "input_throughput": 5500.653888045072,
    "output_throughput": 4838.8011378774245,
    "total_throughput": 10339.455025922496,
    "itl": 177.02973215931496,
    "ttft": 2074318.6634208022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 749804,
    "finished_requests": 80050,
    "scheduler_time": 48.89950765569982
}
#Debug simulation 
Total elapsed time: 5.911796496715397. Arrivals time: 0.2956711044535041 Scheduler time: 5.520256992895156 Scheduler overhead time: 0.03161748545244336 Adapter cache time: 0.016649480909109116 Engine time: 0.0328821437433362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0707962908782065,
    "estimated_duration": 3600.152976987207,
    "input_throughput": 5330.162391060026,
    "output_throughput": 4696.463485879279,
    "total_throughput": 10026.625876939304,
    "itl": 149.2732781666685,
    "ttft": 2091141.3984620632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 749804,
    "finished_requests": 77555,
    "scheduler_time": 42.506814664432376
}
#Debug simulation 
Total elapsed time: 6.070896758232266. Arrivals time: 0.2691076253540814 Scheduler time: 5.691082641482353 Scheduler overhead time: 0.03688514418900013 Adapter cache time: 0.018752260599285364 Engine time: 0.037948330864310265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.869409427978098,
    "estimated_duration": 3600.189173787715,
    "input_throughput": 5500.724557527855,
    "output_throughput": 4838.779341606675,
    "total_throughput": 10339.50389913453,
    "itl": 177.0301971009179,
    "ttft": 2074332.783845145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 749804,
    "finished_requests": 80054,
    "scheduler_time": 48.90231032957197
}
#Debug simulation 
Total elapsed time: 5.869512834586203. Arrivals time: 0.28388966107741 Scheduler time: 5.49050224525854 Scheduler overhead time: 0.03152864659205079 Adapter cache time: 0.01625259453430772 Engine time: 0.03268923982977867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.74727154430002,
    "estimated_duration": 3600.15834830112,
    "input_throughput": 5330.1544386388705,
    "output_throughput": 4696.456478915356,
    "total_throughput": 10026.610917554226,
    "itl": 149.2733720696963,
    "ttft": 2091145.362257089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 749804,
    "finished_requests": 77555,
    "scheduler_time": 42.50677856552968
}
#Debug simulation 
Total elapsed time: 5.74738002801314. Arrivals time: 0.27123137516900897 Scheduler time: 5.365330915432423 Scheduler overhead time: 0.036759383510798216 Adapter cache time: 0.01895994460210204 Engine time: 0.0379249295219779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.028361197561026,
    "estimated_duration": 3600.14899855643,
    "input_throughput": 5583.538072468731,
    "output_throughput": 4947.896602930222,
    "total_throughput": 10531.434675398952,
    "itl": 174.27055117736967,
    "ttft": 2062456.7487488093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 746017,
    "finished_requests": 81446,
    "scheduler_time": 50.05785078788035
}
#Debug simulation 
Total elapsed time: 6.028462014626712. Arrivals time: 0.35993062192574143 Scheduler time: 5.576860626693815 Scheduler overhead time: 0.03182218596339226 Adapter cache time: 0.011919205542653799 Engine time: 0.033008705358952284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.389274638146162,
    "estimated_duration": 3600.1750404567906,
    "input_throughput": 5583.497683893034,
    "output_throughput": 4947.860812273134,
    "total_throughput": 10531.358496166169,
    "itl": 174.27119766014988,
    "ttft": 2062475.4896629734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 746017,
    "finished_requests": 81446,
    "scheduler_time": 50.05770993953015
}
#Debug simulation 
Total elapsed time: 6.389386730268598. Arrivals time: 0.369743041228503 Scheduler time: 5.927360464818776 Scheduler overhead time: 0.03197716781869531 Adapter cache time: 0.011965903453528881 Engine time: 0.033451858442276716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.896069175098091,
    "estimated_duration": 3600.102776273703,
    "input_throughput": 5389.95195578404,
    "output_throughput": 4783.554823350056,
    "total_throughput": 10173.506779134097,
    "itl": 147.0468356652565,
    "ttft": 2082535.888404404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 746017,
    "finished_requests": 78644,
    "scheduler_time": 43.29672420598786
}
#Debug simulation 
Total elapsed time: 5.8961706990376115. Arrivals time: 0.35239509027451277 Scheduler time: 5.436421796679497 Scheduler overhead time: 0.03738213051110506 Adapter cache time: 0.013842409942299128 Engine time: 0.038646492175757885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.07832681434229,
    "estimated_duration": 3600.1574989352293,
    "input_throughput": 5583.524889104206,
    "output_throughput": 4947.884920387056,
    "total_throughput": 10531.40980949126,
    "itl": 174.27069657611653,
    "ttft": 2062463.2602554362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 746017,
    "finished_requests": 81446,
    "scheduler_time": 50.05779756146195
}
#Debug simulation 
Total elapsed time: 6.078430430032313. Arrivals time: 0.3610877119936049 Scheduler time: 5.625249287113547 Scheduler overhead time: 0.03212834196165204 Adapter cache time: 0.011884225998073816 Engine time: 0.03313356218859553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.956088331993669,
    "estimated_duration": 3600.00382055515,
    "input_throughput": 5389.813724422061,
    "output_throughput": 4783.61492331538,
    "total_throughput": 10173.42864773744,
    "itl": 147.04822813622272,
    "ttft": 2082571.982879444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 746017,
    "finished_requests": 78642,
    "scheduler_time": 43.29567665093379
}
#Debug simulation 
Total elapsed time: 5.956189496908337. Arrivals time: 0.28084554011002183 Scheduler time: 5.5680328803136945 Scheduler overhead time: 0.037390842102468014 Adapter cache time: 0.013749518431723118 Engine time: 0.038614431861788034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.0432275319471955,
    "estimated_duration": 3600.1095602141704,
    "input_throughput": 5583.599238797654,
    "output_throughput": 4947.950805958332,
    "total_throughput": 10531.550044755986,
    "itl": 174.27003034815328,
    "ttft": 2062449.8287194546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 746017,
    "finished_requests": 81446,
    "scheduler_time": 50.05746918869803
}
#Debug simulation 
Total elapsed time: 6.043358311988413. Arrivals time: 0.2800242006778717 Scheduler time: 5.671044651418924 Scheduler overhead time: 0.031930262222886086 Adapter cache time: 0.01197352446615696 Engine time: 0.03334684716537595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2150997230783105,
    "estimated_duration": 3600.0091953109563,
    "input_throughput": 5389.805677516889,
    "output_throughput": 4783.6077814552655,
    "total_throughput": 10173.413458972154,
    "itl": 147.04833273284558,
    "ttft": 2082575.9399108766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 746017,
    "finished_requests": 78642,
    "scheduler_time": 43.29564399392421
}
#Debug simulation 
Total elapsed time: 6.215168186929077. Arrivals time: 0.5989827499724925 Scheduler time: 5.509315677918494 Scheduler overhead time: 0.03725188039243221 Adapter cache time: 0.013697685208171606 Engine time: 0.03853708179667592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.014633941929787,
    "estimated_duration": 3600.0871237237584,
    "input_throughput": 5682.457200880156,
    "output_throughput": 4998.005709758667,
    "total_throughput": 10680.462910638822,
    "itl": 171.53580399286844,
    "ttft": 2056358.011500711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38562145573552653,
    "arrivals": 744176,
    "finished_requests": 82555,
    "scheduler_time": 50.44646164209035
}
#Debug simulation 
Total elapsed time: 6.014789691660553. Arrivals time: 0.28074730886146426 Scheduler time: 5.643105805385858 Scheduler overhead time: 0.032496807631105185 Adapter cache time: 0.009595440700650215 Engine time: 0.03358644526451826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.050633430946618,
    "estimated_duration": 3600.112442729211,
    "input_throughput": 5682.417237082596,
    "output_throughput": 4997.970559597156,
    "total_throughput": 10680.387796679752,
    "itl": 171.5364209591789,
    "ttft": 2056375.8592417547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41106834390666336,
    "arrivals": 744176,
    "finished_requests": 82555,
    "scheduler_time": 50.44633375937147
}
#Debug simulation 
Total elapsed time: 6.050731515046209. Arrivals time: 0.29062525974586606 Scheduler time: 5.668625463731587 Scheduler overhead time: 0.03286938974633813 Adapter cache time: 0.00964024430140853 Engine time: 0.03379734978079796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.193848100956529,
    "estimated_duration": 3600.113025818658,
    "input_throughput": 5482.5806463428025,
    "output_throughput": 4824.385755514017,
    "total_throughput": 10306.966401856818,
    "itl": 145.15327866631003,
    "ttft": 2077115.3985128908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4118121644854556,
    "arrivals": 744176,
    "finished_requests": 79635,
    "scheduler_time": 43.61169400533095
}
#Debug simulation 
Total elapsed time: 6.193918158300221. Arrivals time: 0.5956065324135125 Scheduler time: 5.492494286969304 Scheduler overhead time: 0.037735708989202976 Adapter cache time: 0.011841922998428345 Engine time: 0.03870617225766182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.024319237098098,
    "estimated_duration": 3600.0953629381247,
    "input_throughput": 5682.444195951595,
    "output_throughput": 4997.994271272656,
    "total_throughput": 10680.438467224252,
    "itl": 171.5359583680988,
    "ttft": 2056363.9480300844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39390734053216897,
    "arrivals": 744176,
    "finished_requests": 82555,
    "scheduler_time": 50.44641497165882
}
#Debug simulation 
Total elapsed time: 6.024420697707683. Arrivals time: 0.2848262223415077 Scheduler time: 5.64865605160594 Scheduler overhead time: 0.032352779526263475 Adapter cache time: 0.009721731767058372 Engine time: 0.03373172041028738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.896644867025316,
    "estimated_duration": 3600.1182797376678,
    "input_throughput": 5482.572645207161,
    "output_throughput": 4824.3787149308855,
    "total_throughput": 10306.951360138048,
    "itl": 145.15341094578835,
    "ttft": 2077119.0233006463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41709382351487906,
    "arrivals": 744176,
    "finished_requests": 79635,
    "scheduler_time": 43.61166626531152
}
#Debug simulation 
Total elapsed time: 5.896741840988398. Arrivals time: 0.2812838996760547 Scheduler time: 5.509364001918584 Scheduler overhead time: 0.03782048122957349 Adapter cache time: 0.01183242816478014 Engine time: 0.038823152892291546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.366385392844677,
    "estimated_duration": 3600.0782935956427,
    "input_throughput": 5682.471138584006,
    "output_throughput": 4998.017968667263,
    "total_throughput": 10680.48910725127,
    "itl": 171.53558837911487,
    "ttft": 2056351.7516159436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3767463371576746,
    "arrivals": 744176,
    "finished_requests": 82555,
    "scheduler_time": 50.44650663255024
}
#Debug simulation 
Total elapsed time: 6.366488358937204. Arrivals time: 0.6083234390243888 Scheduler time: 5.667204117868096 Scheduler overhead time: 0.032584553584456444 Adapter cache time: 0.009585786145180464 Engine time: 0.03355009946972132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9200421259738505,
    "estimated_duration": 3600.1235298869847,
    "input_throughput": 5482.564649835672,
    "output_throughput": 4824.371679419908,
    "total_throughput": 10306.93632925558,
    "itl": 145.15351636574582,
    "ttft": 2077122.738087241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42237548254430274,
    "arrivals": 744176,
    "finished_requests": 79635,
    "scheduler_time": 43.61163475559962
}
#Debug simulation 
Total elapsed time: 5.920143465977162. Arrivals time: 0.27981014596298337 Scheduler time: 5.533994758035988 Scheduler overhead time: 0.03772680275142193 Adapter cache time: 0.011903885286301374 Engine time: 0.039028940722346306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.149087039753795,
    "estimated_duration": 3600.083165204332,
    "input_throughput": 5678.917419908997,
    "output_throughput": 5032.61901700309,
    "total_throughput": 10711.536436912087,
    "itl": 171.30588080701565,
    "ttft": 2052945.5127989026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37337950475979553,
    "arrivals": 743225,
    "finished_requests": 82807,
    "scheduler_time": 50.910649052905974
}
#Debug simulation 
Total elapsed time: 6.149218904785812. Arrivals time: 0.36559668090194464 Scheduler time: 5.693839887622744 Scheduler overhead time: 0.03272793861106038 Adapter cache time: 0.007893878500908613 Engine time: 0.033954393584281206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.131496178917587,
    "estimated_duration": 3600.1478834074373,
    "input_throughput": 5678.9806036104355,
    "output_throughput": 5032.595211853283,
    "total_throughput": 10711.575815463719,
    "itl": 171.30604721394266,
    "ttft": 2052979.2188735374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39829095209017396,
    "arrivals": 743225,
    "finished_requests": 82809,
    "scheduler_time": 50.91127847407253
}
#Debug simulation 
Total elapsed time: 6.13159474497661. Arrivals time: 0.36555691435933113 Scheduler time: 5.676356609445065 Scheduler overhead time: 0.032815379090607166 Adapter cache time: 0.007738148793578148 Engine time: 0.03392724506556988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.994615904055536,
    "estimated_duration": 3600.1537134174664,
    "input_throughput": 5468.8034365373105,
    "output_throughput": 4851.733395410876,
    "total_throughput": 10320.536831948188,
    "itl": 144.78147939898977,
    "ttft": 2073606.3892966076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39896359410137033,
    "arrivals": 743225,
    "finished_requests": 79768,
    "scheduler_time": 43.87620319423797
}
#Debug simulation 
Total elapsed time: 5.994744951836765. Arrivals time: 0.35371654806658626 Scheduler time: 5.536669543012977 Scheduler overhead time: 0.03789821919053793 Adapter cache time: 0.00956641510128975 Engine time: 0.03913529822602868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.225095729809254,
    "estimated_duration": 3600.0907597943187,
    "input_throughput": 5678.905439919533,
    "output_throughput": 5032.608400415747,
    "total_throughput": 10711.51384033528,
    "itl": 171.30598772625115,
    "ttft": 2052950.958157643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3815385440341199,
    "arrivals": 743225,
    "finished_requests": 82807,
    "scheduler_time": 50.910568537204234
}
#Debug simulation 
Total elapsed time: 6.225202742964029. Arrivals time: 0.4189834278076887 Scheduler time: 5.71629566187039 Scheduler overhead time: 0.03257509134709835 Adapter cache time: 0.008405797183513641 Engine time: 0.03377836523577571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.990527633111924,
    "estimated_duration": 3600.003276328384,
    "input_throughput": 5468.913633900842,
    "output_throughput": 4851.83308438932,
    "total_throughput": 10320.746718290162,
    "itl": 144.7814883421008,
    "ttft": 2073570.7549848708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40411949934437896,
    "arrivals": 743225,
    "finished_requests": 79767,
    "scheduler_time": 43.87434236360894
}
#Debug simulation 
Total elapsed time: 5.990656956098974. Arrivals time: 0.3509066868573427 Scheduler time: 5.535153163131326 Scheduler overhead time: 0.03798546502366662 Adapter cache time: 0.009710677899420261 Engine time: 0.03920212620869279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.2185989283025265,
    "estimated_duration": 3600.0225511587514,
    "input_throughput": 5678.814148933502,
    "output_throughput": 5032.557919440954,
    "total_throughput": 10711.372068374456,
    "itl": 171.3048627858943,
    "ttft": 2052938.848206979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36478613597806586,
    "arrivals": 743225,
    "finished_requests": 82805,
    "scheduler_time": 50.90974800633747
}
#Debug simulation 
Total elapsed time: 6.218700211960822. Arrivals time: 0.3850032789632678 Scheduler time: 5.743070962838829 Scheduler overhead time: 0.03279014257714152 Adapter cache time: 0.008555865846574306 Engine time: 0.03389735333621502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.978389969095588,
    "estimated_duration": 3600.07094699473,
    "input_throughput": 5468.941943056885,
    "output_throughput": 4851.859104216029,
    "total_throughput": 10320.801047272913,
    "itl": 144.80311535820908,
    "ttft": 2073626.117483425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40927540458738776,
    "arrivals": 743225,
    "finished_requests": 79769,
    "scheduler_time": 43.88167168923378
}
#Debug simulation 
Total elapsed time: 5.978493561036885. Arrivals time: 0.35521486308425665 Scheduler time: 5.5188340549357235 Scheduler overhead time: 0.03780680662021041 Adapter cache time: 0.009777414612472057 Engine time: 0.039140129927545786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.125893972814083,
    "estimated_duration": 3600.090112031346,
    "input_throughput": 5740.783523981982,
    "output_throughput": 5042.894881805093,
    "total_throughput": 10783.678405787075,
    "itl": 169.9421475861853,
    "ttft": 2054640.4645318822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36113755378406454,
    "arrivals": 742746,
    "finished_requests": 83364,
    "scheduler_time": 50.935091983292025
}
#Debug simulation 
Total elapsed time: 6.126014423556626. Arrivals time: 0.30788179486989975 Scheduler time: 5.728211347479373 Scheduler overhead time: 0.03302617697045207 Adapter cache time: 0.007456132676452398 Engine time: 0.03406430780887604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.067635051906109,
    "estimated_duration": 3600.1139555343666,
    "input_throughput": 5740.745502855156,
    "output_throughput": 5042.8614827847205,
    "total_throughput": 10783.606985639875,
    "itl": 169.9427107639192,
    "ttft": 2054656.168627897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38510496495524416,
    "arrivals": 742746,
    "finished_requests": 83364,
    "scheduler_time": 50.93496807514019
}
#Debug simulation 
Total elapsed time: 6.067736246157438. Arrivals time: 0.28702365420758724 Scheduler time: 5.691278223879635 Scheduler overhead time: 0.03307387372478843 Adapter cache time: 0.007069730665534735 Engine time: 0.03404341358691454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.94785088300705,
    "estimated_duration": 3600.0772859903354,
    "input_throughput": 5527.665496916064,
    "output_throughput": 4863.017543575868,
    "total_throughput": 10390.683040491931,
    "itl": 144.21506423540927,
    "ttft": 2076447.28536822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3827342573739597,
    "arrivals": 742746,
    "finished_requests": 80263,
    "scheduler_time": 43.9772988048717
}
#Debug simulation 
Total elapsed time: 5.947955118957907. Arrivals time: 0.27838811185210943 Scheduler time: 5.5654038595966995 Scheduler overhead time: 0.038124905433505774 Adapter cache time: 0.00898461788892746 Engine time: 0.03928283974528313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.047016960103065,
    "estimated_duration": 3600.097683878462,
    "input_throughput": 5740.771449772062,
    "output_throughput": 5042.8842754181505,
    "total_throughput": 10783.655725190212,
    "itl": 169.94227911143983,
    "ttft": 2054646.2293189662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3687611522176305,
    "arrivals": 742746,
    "finished_requests": 83364,
    "scheduler_time": 50.935040231970675
}
#Debug simulation 
Total elapsed time: 6.047145357355475. Arrivals time: 0.2872515618801117 Scheduler time: 5.670988796278834 Scheduler overhead time: 0.03267041454091668 Adapter cache time: 0.007165924180299044 Engine time: 0.03379756212234497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.909883386921138,
    "estimated_duration": 3600.082293892409,
    "input_throughput": 5527.657807645307,
    "output_throughput": 4863.01077886505,
    "total_throughput": 10390.668586510357,
    "itl": 144.21518532487377,
    "ttft": 2076450.2926933814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38776440883055346,
    "arrivals": 742746,
    "finished_requests": 80263,
    "scheduler_time": 43.977276555491265
}
#Debug simulation 
Total elapsed time: 5.910016017965972. Arrivals time: 0.27791555458679795 Scheduler time: 5.527856322936714 Scheduler overhead time: 0.03819710295647383 Adapter cache time: 0.008856830187141895 Engine time: 0.039358699694275856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.194756722077727,
    "estimated_duration": 3600.081842944329,
    "input_throughput": 5740.796710081792,
    "output_throughput": 5042.906464913038,
    "total_throughput": 10783.703174994831,
    "itl": 169.941956710773,
    "ttft": 2054634.9949894398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35282593479845714,
    "arrivals": 742746,
    "finished_requests": 83364,
    "scheduler_time": 50.93513451525684
}
#Debug simulation 
Total elapsed time: 6.1948818461969495. Arrivals time: 0.34823652962222695 Scheduler time: 5.7569624572061 Scheduler overhead time: 0.03304890403524041 Adapter cache time: 0.007192776072770357 Engine time: 0.03406064445152879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.968200273811817,
    "estimated_duration": 3600.0871649774704,
    "input_throughput": 5527.650328467682,
    "output_throughput": 4863.00419898571,
    "total_throughput": 10390.654527453393,
    "itl": 144.21528233033786,
    "ttft": 2076453.7116020147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3926688065007326,
    "arrivals": 742746,
    "finished_requests": 80263,
    "scheduler_time": 43.977243242882984
}
#Debug simulation 
Total elapsed time: 5.968300919979811. Arrivals time: 0.32124580489471555 Scheduler time: 5.5434091705828905 Scheduler overhead time: 0.03803878650069237 Adapter cache time: 0.00880788080394268 Engine time: 0.03906258847564459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.622878024820238,
    "estimated_duration": 3600.0576590552764,
    "input_throughput": 4859.737442258385,
    "output_throughput": 4275.838183113847,
    "total_throughput": 9135.575625372232,
    "itl": 200.47122462626817,
    "ttft": 2121724.125986536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 679344,
    "finished_requests": 70668,
    "scheduler_time": 43.29849030021967
}
#Debug simulation 
Total elapsed time: 5.622947048861533. Arrivals time: 0.6563423085026443 Scheduler time: 4.872757064644247 Scheduler overhead time: 0.028110128827393055 Adapter cache time: 0.02360218670219183 Engine time: 0.029072826262563467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.303819850087166,
    "estimated_duration": 3600.1022272821992,
    "input_throughput": 4859.677280110914,
    "output_throughput": 4275.785249470744,
    "total_throughput": 9135.462529581659,
    "itl": 200.47277094770544,
    "ttft": 2121746.6961694066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 679344,
    "finished_requests": 70668,
    "scheduler_time": 43.29845238665161
}
#Debug simulation 
Total elapsed time: 5.303924710024148. Arrivals time: 0.32671614922583103 Scheduler time: 4.883254074025899 Scheduler overhead time: 0.028314228169620037 Adapter cache time: 0.023527242708951235 Engine time: 0.02904026908800006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.263152781873941,
    "estimated_duration": 3600.1497304503846,
    "input_throughput": 4741.530291259438,
    "output_throughput": 4179.359506283002,
    "total_throughput": 8920.889797542439,
    "itl": 167.59422230448632,
    "ttft": 2136168.643878838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 679344,
    "finished_requests": 68968,
    "scheduler_time": 37.86879824562498
}
#Debug simulation 
Total elapsed time: 5.26325007295236. Arrivals time: 0.3245074516162276 Scheduler time: 4.821881340350956 Scheduler overhead time: 0.033017173409461975 Adapter cache time: 0.0343410330824554 Engine time: 0.034033588133752346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.304072862956673,
    "estimated_duration": 3600.0847552447863,
    "input_throughput": 4859.700865239883,
    "output_throughput": 4275.806000837706,
    "total_throughput": 9135.506866077589,
    "itl": 200.47228495661713,
    "ttft": 2121734.043870801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4002960364404137,
    "arrivals": 679344,
    "finished_requests": 70668,
    "scheduler_time": 43.298549947931214
}
#Debug simulation 
Total elapsed time: 5.304181603714824. Arrivals time: 0.2534549543634057 Scheduler time: 4.956466872710735 Scheduler overhead time: 0.028243799228221178 Adapter cache time: 0.023605135269463062 Engine time: 0.02909786533564329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.237903362605721,
    "estimated_duration": 3600.1551038540188,
    "input_throughput": 4741.523214298762,
    "output_throughput": 4179.353268389102,
    "total_throughput": 8920.876482687863,
    "itl": 167.5943726750661,
    "ttft": 2136172.5169491414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 679344,
    "finished_requests": 68968,
    "scheduler_time": 37.86876423644309
}
#Debug simulation 
Total elapsed time: 5.238003116566688. Arrivals time: 0.3216926590539515 Scheduler time: 4.7988969422876835 Scheduler overhead time: 0.033164024353027344 Adapter cache time: 0.034595911391079426 Engine time: 0.03419107478111982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.2981106149964035,
    "estimated_duration": 3600.0486959604195,
    "input_throughput": 4859.749541619076,
    "output_throughput": 4275.848828731854,
    "total_throughput": 9135.59837035093,
    "itl": 200.47100918688076,
    "ttft": 2121717.6274990216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 679344,
    "finished_requests": 70668,
    "scheduler_time": 43.29854319883872
}
#Debug simulation 
Total elapsed time: 5.29821194242686. Arrivals time: 0.3270317427814007 Scheduler time: 4.877274171914905 Scheduler overhead time: 0.02813222026452422 Adapter cache time: 0.023723124526441097 Engine time: 0.02899552835151553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.241701690945774,
    "estimated_duration": 3600.1604759054803,
    "input_throughput": 4741.516139140062,
    "output_throughput": 4179.34703208353,
    "total_throughput": 8920.863171223593,
    "itl": 167.59448017631357,
    "ttft": 2136176.399079191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 679344,
    "finished_requests": 68968,
    "scheduler_time": 37.86872887508891
}
#Debug simulation 
Total elapsed time: 5.241806318052113. Arrivals time: 0.321707118768245 Scheduler time: 4.8029572307132185 Scheduler overhead time: 0.0330483615398407 Adapter cache time: 0.03437993163242936 Engine time: 0.03427822096273303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.764384055975825,
    "estimated_duration": 3600.1216664429226,
    "input_throughput": 5265.312607817815,
    "output_throughput": 4664.574854936015,
    "total_throughput": 9929.88746275383,
    "itl": 184.5594905404804,
    "ttft": 2064998.2289742066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 634153,
    "finished_requests": 76899,
    "scheduler_time": 47.26659560333047
}
#Debug simulation 
Total elapsed time: 5.764494333881885. Arrivals time: 0.3423067880794406 Scheduler time: 5.321030453778803 Scheduler overhead time: 0.030562841799110174 Adapter cache time: 0.024789288640022278 Engine time: 0.03161654295399785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.74222620902583,
    "estimated_duration": 3600.0639186292547,
    "input_throughput": 5264.691246708906,
    "output_throughput": 4664.128020925065,
    "total_throughput": 9928.819267633971,
    "itl": 184.59356138061563,
    "ttft": 2064992.3864210146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 634153,
    "finished_requests": 76890,
    "scheduler_time": 47.25742685092974
}
#Debug simulation 
Total elapsed time: 5.742330260109156. Arrivals time: 0.33433384727686644 Scheduler time: 5.3073708498850465 Scheduler overhead time: 0.030592203605920076 Adapter cache time: 0.024409533943980932 Engine time: 0.03153449669480324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.664064744953066,
    "estimated_duration": 3600.0251863604635,
    "input_throughput": 5124.643313579516,
    "output_throughput": 4543.225436857356,
    "total_throughput": 9667.868750436872,
    "itl": 154.023314601337,
    "ttft": 2080904.0201697357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 634153,
    "finished_requests": 74858,
    "scheduler_time": 41.12258653441844
}
#Debug simulation 
Total elapsed time: 5.664162282831967. Arrivals time: 0.3324640579521656 Scheduler time: 5.211049336474389 Scheduler overhead time: 0.03572427434846759 Adapter cache time: 0.0310562364757061 Engine time: 0.037061662413179874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.299728401005268,
    "estimated_duration": 3600.005764924741,
    "input_throughput": 5264.776291378028,
    "output_throughput": 4664.203364227396,
    "total_throughput": 9928.979655605424,
    "itl": 184.59184735383383,
    "ttft": 2064951.327724882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 634153,
    "finished_requests": 76890,
    "scheduler_time": 47.257326556159505
}
#Debug simulation 
Total elapsed time: 6.299828971736133. Arrivals time: 0.33497329661622643 Scheduler time: 5.86382830934599 Scheduler overhead time: 0.030570470727980137 Adapter cache time: 0.024636605754494667 Engine time: 0.031497009098529816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.648577831219882,
    "estimated_duration": 3600.0299689592666,
    "input_throughput": 5124.636505549252,
    "output_throughput": 4543.219401234118,
    "total_throughput": 9667.85590678337,
    "itl": 154.02344451673417,
    "ttft": 2080907.5292542353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879445,
    "arrivals": 634153,
    "finished_requests": 74858,
    "scheduler_time": 41.12253868157471
}
#Debug simulation 
Total elapsed time: 5.648680174257606. Arrivals time: 0.32921063946560025 Scheduler time: 5.198487259447575 Scheduler overhead time: 0.03576526278629899 Adapter cache time: 0.03131412947550416 Engine time: 0.037159401923418045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.724571962840855,
    "estimated_duration": 3600.112098781025,
    "input_throughput": 5265.326600918427,
    "output_throughput": 4664.587251515311,
    "total_throughput": 9929.913852433738,
    "itl": 184.5592792780398,
    "ttft": 2064991.185540173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 634153,
    "finished_requests": 76899,
    "scheduler_time": 47.26666248024495
}
#Debug simulation 
Total elapsed time: 5.724708585999906. Arrivals time: 0.33410071209073067 Scheduler time: 5.28979049064219 Scheduler overhead time: 0.03035593405365944 Adapter cache time: 0.024793976452201605 Engine time: 0.03143420163542032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.633887053001672,
    "estimated_duration": 3600.0356711050495,
    "input_throughput": 5124.6283885673365,
    "output_throughput": 4543.212205166713,
    "total_throughput": 9667.84059373405,
    "itl": 154.02345488215605,
    "ttft": 2080911.7295392875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378303,
    "arrivals": 634153,
    "finished_requests": 74858,
    "scheduler_time": 41.122495054995944
}
#Debug simulation 
Total elapsed time: 5.6339896051213145. Arrivals time: 0.259960004594177 Scheduler time: 5.2534915641881526 Scheduler overhead time: 0.03562865173444152 Adapter cache time: 0.031172326765954494 Engine time: 0.03699101321399212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.09617809811607,
    "estimated_duration": 3600.1659633657077,
    "input_throughput": 5452.228647161975,
    "output_throughput": 4824.117603668313,
    "total_throughput": 10276.346250830287,
    "itl": 178.25358794396988,
    "ttft": 2044044.2557855803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 626696,
    "finished_requests": 79530,
    "scheduler_time": 48.93955847564286
}
#Debug simulation 
Total elapsed time: 6.096293982118368. Arrivals time: 0.3476385376416147 Scheduler time: 5.650208157952875 Scheduler overhead time: 0.03148058848455548 Adapter cache time: 0.019640394486486912 Engine time: 0.03260493092238903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.917477990034968,
    "estimated_duration": 3600.0798627454374,
    "input_throughput": 5452.525429541563,
    "output_throughput": 4824.473528971857,
    "total_throughput": 10276.99895851342,
    "itl": 178.24328498786772,
    "ttft": 2043971.993219831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 626696,
    "finished_requests": 79534,
    "scheduler_time": 48.94101855156239
}
#Debug simulation 
Total elapsed time: 5.917579288128763. Arrivals time: 0.33994387835264206 Scheduler time: 5.479323248844594 Scheduler overhead time: 0.03140307869762182 Adapter cache time: 0.019786184653639793 Engine time: 0.03250006027519703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.813269531819969,
    "estimated_duration": 3600.1316445623293,
    "input_throughput": 5304.776848603753,
    "output_throughput": 4701.222808216944,
    "total_throughput": 10005.999656820697,
    "itl": 149.74524337367936,
    "ttft": 2059856.3274909165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 626696,
    "finished_requests": 77358,
    "scheduler_time": 42.76544375928492
}
#Debug simulation 
Total elapsed time: 5.813368170987815. Arrivals time: 0.33559394534677267 Scheduler time: 5.360655535478145 Scheduler overhead time: 0.03656752221286297 Adapter cache time: 0.02532006846740842 Engine time: 0.038037384394556284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.908772895112634,
    "estimated_duration": 3600.138948847866,
    "input_throughput": 5452.4359417521755,
    "output_throughput": 4824.39434887877,
    "total_throughput": 10276.830290630945,
    "itl": 178.24746918922452,
    "ttft": 2043986.6943382528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 626696,
    "finished_requests": 79534,
    "scheduler_time": 48.94062331947651
}
#Debug simulation 
Total elapsed time: 5.908905597869307. Arrivals time: 0.3403160800226033 Scheduler time: 5.4704970279708505 Scheduler overhead time: 0.03144179377704859 Adapter cache time: 0.01949354074895382 Engine time: 0.03246253402903676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.82837995281443,
    "estimated_duration": 3600.137016502247,
    "input_throughput": 5304.768933087656,
    "output_throughput": 4701.215793293249,
    "total_throughput": 10005.984726380904,
    "itl": 149.74539412174556,
    "ttft": 2059860.239703983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 626696,
    "finished_requests": 77358,
    "scheduler_time": 42.765411552999424
}
#Debug simulation 
Total elapsed time: 5.828475679736584. Arrivals time: 0.3350717378780246 Scheduler time: 5.375932843890041 Scheduler overhead time: 0.03673980198800564 Adapter cache time: 0.02544614113867283 Engine time: 0.03814456379041076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.976916044950485,
    "estimated_duration": 3600.1291099604396,
    "input_throughput": 5452.284459935853,
    "output_throughput": 4824.166986664221,
    "total_throughput": 10276.451446600075,
    "itl": 178.25319091796325,
    "ttft": 2044039.1856596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 626696,
    "finished_requests": 79530,
    "scheduler_time": 48.93913199139169
}
#Debug simulation 
Total elapsed time: 5.977036010008305. Arrivals time: 0.35449530789628625 Scheduler time: 5.5237939092330635 Scheduler overhead time: 0.031536647118628025 Adapter cache time: 0.019916961900889874 Engine time: 0.03257635794579983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.811854632105678,
    "estimated_duration": 3600.106287636472,
    "input_throughput": 5304.68060501007,
    "output_throughput": 4700.652049667712,
    "total_throughput": 10005.332654677783,
    "itl": 149.74282456843204,
    "ttft": 2059835.2771709997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 626696,
    "finished_requests": 77354,
    "scheduler_time": 42.76426519815256
}
#Debug simulation 
Total elapsed time: 5.811957755126059. Arrivals time: 0.33206248097121716 Scheduler time: 5.362406402360648 Scheduler overhead time: 0.03664799593389034 Adapter cache time: 0.025657063350081444 Engine time: 0.03793927514925599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.978202014230192,
    "estimated_duration": 3600.113063847799,
    "input_throughput": 5590.138877052453,
    "output_throughput": 4943.27363735051,
    "total_throughput": 10533.412514402964,
    "itl": 174.06805985983618,
    "ttft": 2037639.0565714827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 623022,
    "finished_requests": 81656,
    "scheduler_time": 50.127958767533805
}
#Debug simulation 
Total elapsed time: 5.978301119059324. Arrivals time: 0.2744126133620739 Scheduler time: 5.608072295319289 Scheduler overhead time: 0.032184898853302 Adapter cache time: 0.015523817390203476 Engine time: 0.03312552720308304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.973532664123923,
    "estimated_duration": 3600.007777836843,
    "input_throughput": 5589.763478814352,
    "output_throughput": 4942.90182081003,
    "total_throughput": 10532.665299624383,
    "itl": 174.06974560092925,
    "ttft": 2037640.0189535683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 623022,
    "finished_requests": 81649,
    "scheduler_time": 50.126131741020124
}
#Debug simulation 
Total elapsed time: 6.97367451293394. Arrivals time: 0.3209563116542995 Scheduler time: 6.556431481614709 Scheduler overhead time: 0.03226772416383028 Adapter cache time: 0.01564488233998418 Engine time: 0.03327747853472829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.011091930791736,
    "estimated_duration": 3600.0507025903253,
    "input_throughput": 5416.80620941497,
    "output_throughput": 4802.658470215309,
    "total_throughput": 10219.464679630279,
    "itl": 146.74373161453113,
    "ttft": 2055056.9824517393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 623022,
    "finished_requests": 79120,
    "scheduler_time": 43.64395756592874
}
#Debug simulation 
Total elapsed time: 6.011194156948477. Arrivals time: 0.27752548921853304 Scheduler time: 5.6189633761532605 Scheduler overhead time: 0.03740979451686144 Adapter cache time: 0.020869158674031496 Engine time: 0.03887255536392331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.975240728352219,
    "estimated_duration": 3600.1702550998943,
    "input_throughput": 5590.1100709031825,
    "output_throughput": 4943.195665480049,
    "total_throughput": 10533.305736383232,
    "itl": 174.06743009234827,
    "ttft": 2037676.8746022515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 623022,
    "finished_requests": 81657,
    "scheduler_time": 50.12882461974125
}
#Debug simulation 
Total elapsed time: 5.975347197148949. Arrivals time: 0.28756127413362265 Scheduler time: 5.591609495226294 Scheduler overhead time: 0.03218731051310897 Adapter cache time: 0.015754112508147955 Engine time: 0.03336669970303774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.89901724178344,
    "estimated_duration": 3600.1093882167143,
    "input_throughput": 5416.781241099918,
    "output_throughput": 4802.686289642038,
    "total_throughput": 10219.467530741957,
    "itl": 146.73975514571077,
    "ttft": 2055071.9781103185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 623022,
    "finished_requests": 79121,
    "scheduler_time": 43.64267165880254
}
#Debug simulation 
Total elapsed time: 5.899121150840074. Arrivals time: 0.2688008244149387 Scheduler time: 5.516560355201364 Scheduler overhead time: 0.037424040492624044 Adapter cache time: 0.020256704185158014 Engine time: 0.03859888017177582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.9985712775960565,
    "estimated_duration": 3600.0685925463854,
    "input_throughput": 5589.8782155608915,
    "output_throughput": 4942.9616526871,
    "total_throughput": 10532.839868247991,
    "itl": 174.06769776687855,
    "ttft": 2037646.545038374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 623022,
    "finished_requests": 81652,
    "scheduler_time": 50.12720534458065
}
#Debug simulation 
Total elapsed time: 5.998708980623633. Arrivals time: 0.29585863882675767 Scheduler time: 5.606093397829682 Scheduler overhead time: 0.03218425111845136 Adapter cache time: 0.016263673081994057 Engine time: 0.03323967754840851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.830761402845383,
    "estimated_duration": 3600.1016445023847,
    "input_throughput": 5416.792892439425,
    "output_throughput": 4802.696620081097,
    "total_throughput": 10219.48951252052,
    "itl": 146.73957010353516,
    "ttft": 2055075.130661021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 623022,
    "finished_requests": 79121,
    "scheduler_time": 43.64283764808713
}
#Debug simulation 
Total elapsed time: 5.83086011512205. Arrivals time: 0.27119313506409526 Scheduler time: 5.445709660183638 Scheduler overhead time: 0.03733606589958072 Adapter cache time: 0.020589156541973352 Engine time: 0.0384665597230196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1552925813011825,
    "estimated_duration": 3600.1832666326336,
    "input_throughput": 5701.132270190734,
    "output_throughput": 5005.311581500327,
    "total_throughput": 10706.443851691061,
    "itl": 171.16969820637735,
    "ttft": 2024152.0404479215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 621138,
    "finished_requests": 82946,
    "scheduler_time": 50.620892127253235
}
#Debug simulation 
Total elapsed time: 6.155413130298257. Arrivals time: 0.3706414420157671 Scheduler time: 5.689634462352842 Scheduler overhead time: 0.032708650920540094 Adapter cache time: 0.013248435221612453 Engine time: 0.03376915259286761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.701793277170509,
    "estimated_duration": 3600.087224869548,
    "input_throughput": 5701.219919954505,
    "output_throughput": 5005.420389683188,
    "total_throughput": 10706.640309637693,
    "itl": 171.1709070635165,
    "ttft": 2024140.7325822539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 621138,
    "finished_requests": 82945,
    "scheduler_time": 50.61922524235552
}
#Debug simulation 
Total elapsed time: 6.701901390217245. Arrivals time: 0.3471943372860551 Scheduler time: 6.259658896364272 Scheduler overhead time: 0.03276083339005709 Adapter cache time: 0.01302571827545762 Engine time: 0.03385630901902914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.006773094646633,
    "estimated_duration": 3600.07125454225,
    "input_throughput": 5510.096494610146,
    "output_throughput": 4842.66442726749,
    "total_throughput": 10352.760921877636,
    "itl": 143.88584040261676,
    "ttft": 2044471.0863982772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 621138,
    "finished_requests": 80136,
    "scheduler_time": 43.73101148157011
}
#Debug simulation 
Total elapsed time: 6.006880443077534. Arrivals time: 0.34859479684382677 Scheduler time: 5.545571010559797 Scheduler overhead time: 0.03818845795467496 Adapter cache time: 0.017252938356250525 Engine time: 0.03944743983447552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.438023528084159,
    "estimated_duration": 3600.0205781392556,
    "input_throughput": 5701.093800582739,
    "output_throughput": 5005.423054918706,
    "total_throughput": 10706.516855501446,
    "itl": 171.1705036977366,
    "ttft": 2024127.6329778947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 621138,
    "finished_requests": 82943,
    "scheduler_time": 50.61844794861862
}
#Debug simulation 
Total elapsed time: 6.438125171232969. Arrivals time: 0.2785023278556764 Scheduler time: 6.065251717809588 Scheduler overhead time: 0.03267434611916542 Adapter cache time: 0.012925224378705025 Engine time: 0.03364519728347659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.316205496899784,
    "estimated_duration": 3600.055871795885,
    "input_throughput": 5507.096196845936,
    "output_throughput": 4840.359044568736,
    "total_throughput": 10347.455241414673,
    "itl": 143.51655838482333,
    "ttft": 2044458.4975775715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 621138,
    "finished_requests": 80097,
    "scheduler_time": 43.61753397376846
}
#Debug simulation 
Total elapsed time: 6.316283768974245. Arrivals time: 0.5905053932219744 Scheduler time: 5.612810761202127 Scheduler overhead time: 0.038251365534961224 Adapter cache time: 0.017461732495576143 Engine time: 0.03939638240262866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.425333607941866,
    "estimated_duration": 3600.1408605361635,
    "input_throughput": 5701.134981963805,
    "output_throughput": 5005.345817862337,
    "total_throughput": 10706.480799826142,
    "itl": 171.1698099856603,
    "ttft": 2024143.0193067915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 621138,
    "finished_requests": 82945,
    "scheduler_time": 50.620207975280756
}
#Debug simulation 
Total elapsed time: 6.425435825716704. Arrivals time: 0.5896575031802058 Scheduler time: 5.741698673460633 Scheduler overhead time: 0.03249220550060272 Adapter cache time: 0.01294930325821042 Engine time: 0.03351657371968031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.315007993020117,
    "estimated_duration": 3600.0347451577427,
    "input_throughput": 5509.838766606366,
    "output_throughput": 4842.659372510057,
    "total_throughput": 10352.498139116424,
    "itl": 143.85868604231797,
    "ttft": 2044481.5316686323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 621138,
    "finished_requests": 80133,
    "scheduler_time": 43.72201090314513
}
#Debug simulation 
Total elapsed time: 6.315076244063675. Arrivals time: 0.35667761135846376 Scheduler time: 5.845099336933345 Scheduler overhead time: 0.038199279457330704 Adapter cache time: 0.017469497863203287 Engine time: 0.03979454142972827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.232700212858617,
    "estimated_duration": 3600.113245197027,
    "input_throughput": 5725.173792101638,
    "output_throughput": 5031.5911656853,
    "total_throughput": 10756.76495778694,
    "itl": 170.15773134798226,
    "ttft": 2023104.9179465258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37337950475979553,
    "arrivals": 620208,
    "finished_requests": 83319,
    "scheduler_time": 50.92196779287183
}
#Debug simulation 
Total elapsed time: 6.232838951982558. Arrivals time: 0.28394016390666366 Scheduler time: 5.8563271183520555 Scheduler overhead time: 0.03281893488019705 Adapter cache time: 0.010392787866294384 Engine time: 0.034078824799507856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.085370230022818,
    "estimated_duration": 3600.0452832983697,
    "input_throughput": 5725.281872320202,
    "output_throughput": 5031.686152404071,
    "total_throughput": 10756.968024724274,
    "itl": 170.158777122637,
    "ttft": 2023053.5568349925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39869954740861424,
    "arrivals": 620208,
    "finished_requests": 83319,
    "scheduler_time": 50.9210025035527
}
#Debug simulation 
Total elapsed time: 6.0854633641429245. Arrivals time: 0.2863019430078566 Scheduler time: 5.7067648228257895 Scheduler overhead time: 0.03274941677227616 Adapter cache time: 0.010411188937723637 Engine time: 0.034090946428477764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.933759378734976,
    "estimated_duration": 3600.1412941740123,
    "input_throughput": 5530.635709276582,
    "output_throughput": 4865.6473645484975,
    "total_throughput": 10396.28307382508,
    "itl": 143.61942519438435,
    "ttft": 2045005.8851994516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39930084159597845,
    "arrivals": 620208,
    "finished_requests": 80491,
    "scheduler_time": 44.04488424607627
}
#Debug simulation 
Total elapsed time: 5.933894515968859. Arrivals time: 0.27486347034573555 Scheduler time: 5.548980368301272 Scheduler overhead time: 0.03817751770839095 Adapter cache time: 0.014650164172053337 Engine time: 0.03937860298901796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.086812959983945,
    "estimated_duration": 3600.1546997442774,
    "input_throughput": 5725.126201233531,
    "output_throughput": 5031.54989458833,
    "total_throughput": 10756.676095821862,
    "itl": 170.15783936114582,
    "ttft": 2023117.8735893765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38112994871567957,
    "arrivals": 620208,
    "finished_requests": 83320,
    "scheduler_time": 50.922604519306624
}
#Debug simulation 
Total elapsed time: 6.086919830180705. Arrivals time: 0.2825091411359608 Scheduler time: 5.71170296985656 Scheduler overhead time: 0.03278866270557046 Adapter cache time: 0.010480268858373165 Engine time: 0.034147456753998995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.922290270216763,
    "estimated_duration": 3600.0078780198487,
    "input_throughput": 5529.17540029068,
    "output_throughput": 4864.47379932746,
    "total_throughput": 10393.64919961814,
    "itl": 143.47595731071155,
    "ttft": 2044928.6056809356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40470825441181674,
    "arrivals": 620208,
    "finished_requests": 80466,
    "scheduler_time": 43.99842762258633
}
#Debug simulation 
Total elapsed time: 5.922390332911164. Arrivals time: 0.2739344039000571 Scheduler time: 5.538675143849105 Scheduler overhead time: 0.03801751369610429 Adapter cache time: 0.014690384268760681 Engine time: 0.03931879159063101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.0723859262652695,
    "estimated_duration": 3600.104727441779,
    "input_throughput": 5725.187337715672,
    "output_throughput": 5031.603070300667,
    "total_throughput": 10756.790408016339,
    "itl": 170.15752568086663,
    "ttft": 2023098.9359099423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36478613597806586,
    "arrivals": 620208,
    "finished_requests": 83319,
    "scheduler_time": 50.92201237358255
}
#Debug simulation 
Total elapsed time: 6.072487561963499. Arrivals time: 0.2861522124148905 Scheduler time: 5.693780774716288 Scheduler overhead time: 0.03255918761715293 Adapter cache time: 0.01032146904617548 Engine time: 0.03440319141373038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.939956247806549,
    "estimated_duration": 3600.0534926590763,
    "input_throughput": 5529.004788564393,
    "output_throughput": 4864.356331290318,
    "total_throughput": 10393.361119854711,
    "itl": 143.477554577388,
    "ttft": 2044952.4090433875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40973840586841076,
    "arrivals": 620208,
    "finished_requests": 80466,
    "scheduler_time": 43.99921828010327
}
#Debug simulation 
Total elapsed time: 5.940060894936323. Arrivals time: 0.2761533744633198 Scheduler time: 5.55333642102778 Scheduler overhead time: 0.038461409974843264 Adapter cache time: 0.014793877489864826 Engine time: 0.039404808077961206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1589741837233305,
    "estimated_duration": 3600.019052881278,
    "input_throughput": 5748.114578293157,
    "output_throughput": 5048.730501982539,
    "total_throughput": 10796.845080275696,
    "itl": 169.71771855641012,
    "ttft": 2016146.471385264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3703190170158628,
    "arrivals": 619792,
    "finished_requests": 83758,
    "scheduler_time": 51.03710078666209
}
#Debug simulation 
Total elapsed time: 6.159073467832059. Arrivals time: 0.2856253841891885 Scheduler time: 5.78036971250549 Scheduler overhead time: 0.03309089271351695 Adapter cache time: 0.010363179724663496 Engine time: 0.03411625139415264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.139824455138296,
    "estimated_duration": 3600.1237120310498,
    "input_throughput": 5747.947474928753,
    "output_throughput": 5048.583730403552,
    "total_throughput": 10796.531205332305,
    "itl": 169.71843843573544,
    "ttft": 2016191.8812083183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3953009017952718,
    "arrivals": 619792,
    "finished_requests": 83758,
    "scheduler_time": 51.038563985302616
}
#Debug simulation 
Total elapsed time: 6.139924703165889. Arrivals time: 0.350531798787415 Scheduler time: 5.696742930449545 Scheduler overhead time: 0.032980469055473804 Adapter cache time: 0.01029956666752696 Engine time: 0.034058467485010624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0785007099621,
    "estimated_duration": 3600.151517540657,
    "input_throughput": 5548.229818295259,
    "output_throughput": 4879.311305208593,
    "total_throughput": 10427.541123503852,
    "itl": 143.24672782738475,
    "ttft": 2036928.4273155045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3925393089093277,
    "arrivals": 619792,
    "finished_requests": 80828,
    "scheduler_time": 44.15651928093808
}
#Debug simulation 
Total elapsed time: 6.078600539825857. Arrivals time: 0.34417590172961354 Scheduler time: 5.624348595738411 Scheduler overhead time: 0.03835509205237031 Adapter cache time: 0.014308253303170204 Engine time: 0.039492602460086346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.135941022075713,
    "estimated_duration": 3600.0810545926506,
    "input_throughput": 5748.015582482893,
    "output_throughput": 5048.643551181533,
    "total_throughput": 10796.659133664425,
    "itl": 169.71779846770463,
    "ttft": 2016188.1288234384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37895708905765807,
    "arrivals": 619792,
    "finished_requests": 83758,
    "scheduler_time": 51.03812171403561
}
#Debug simulation 
Total elapsed time: 6.136045879218727. Arrivals time: 0.34728331910446286 Scheduler time: 5.696134326048195 Scheduler overhead time: 0.03289257315918803 Adapter cache time: 0.01025344617664814 Engine time: 0.03413111064583063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.014564509037882,
    "estimated_duration": 3600.148148872489,
    "input_throughput": 5549.567454955208,
    "output_throughput": 4880.304718988466,
    "total_throughput": 10429.872173943673,
    "itl": 143.3440053981739,
    "ttft": 2036842.4471292193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3974437065795066,
    "arrivals": 619792,
    "finished_requests": 80845,
    "scheduler_time": 44.18615620105746
}
#Debug simulation 
Total elapsed time: 6.0146656623110175. Arrivals time: 0.27363646030426025 Scheduler time: 5.630865064449608 Scheduler overhead time: 0.038338531740009785 Adapter cache time: 0.01451252680271864 Engine time: 0.039445935282856226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.770675067789853,
    "estimated_duration": 3600.0021582854383,
    "input_throughput": 5748.141553852719,
    "output_throughput": 5048.75419537426,
    "total_throughput": 10796.89574922698,
    "itl": 169.71741883539417,
    "ttft": 2016136.9440118908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3617960856831637,
    "arrivals": 619792,
    "finished_requests": 83758,
    "scheduler_time": 51.03702978303762
}
#Debug simulation 
Total elapsed time: 6.7708103051409125. Arrivals time: 0.2868747883476317 Scheduler time: 6.3908270839601755 Scheduler overhead time: 0.03305361373350024 Adapter cache time: 0.010397552978247404 Engine time: 0.03416289109736681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.979111253283918,
    "estimated_duration": 3600.0067853506703,
    "input_throughput": 5547.637877036559,
    "output_throughput": 4878.973581792594,
    "total_throughput": 10426.611458829153,
    "itl": 143.19799070937546,
    "ttft": 2036849.8023242615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4027253656089303,
    "arrivals": 619792,
    "finished_requests": 80818,
    "scheduler_time": 44.139990739258266
}
#Debug simulation 
Total elapsed time: 5.979212519247085. Arrivals time: 0.33797212643548846 Scheduler time: 5.531523715239018 Scheduler overhead time: 0.03808457963168621 Adapter cache time: 0.014427722431719303 Engine time: 0.03932169824838638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.4220538581721485,
    "estimated_duration": 3600.165903091904,
    "input_throughput": 5298.996077824084,
    "output_throughput": 4699.088168539919,
    "total_throughput": 9998.084246364004,
    "itl": 183.5434597817597,
    "ttft": 2049387.0508131972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 572657,
    "finished_requests": 77452,
    "scheduler_time": 47.71866598587428
}
#Debug simulation 
Total elapsed time: 6.422171496320516. Arrivals time: 0.3675486338324845 Scheduler time: 5.943930197972804 Scheduler overhead time: 0.030972319189459085 Adapter cache time: 0.033277102280408144 Engine time: 0.03202411811798811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.347215285059065,
    "estimated_duration": 3600.1973750589427,
    "input_throughput": 5298.949755411026,
    "output_throughput": 4699.047090362102,
    "total_throughput": 9997.996845773127,
    "itl": 183.5442328657066,
    "ttft": 2049407.3419262154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 572657,
    "finished_requests": 77452,
    "scheduler_time": 47.71856772798517
}
#Debug simulation 
Total elapsed time: 6.347344459965825. Arrivals time: 0.2675947337411344 Scheduler time: 5.968979290686548 Scheduler overhead time: 0.031024316791445017 Adapter cache time: 0.033480626065284014 Engine time: 0.03181014070287347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.313258855137974,
    "estimated_duration": 3600.141560880755,
    "input_throughput": 5218.410910320942,
    "output_throughput": 4635.790209293049,
    "total_throughput": 9854.201119613992,
    "itl": 152.04622051301826,
    "ttft": 2061470.7836936477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 572657,
    "finished_requests": 76322,
    "scheduler_time": 42.15644399143859
}
#Debug simulation 
Total elapsed time: 6.313361821230501. Arrivals time: 0.3280984810553491 Scheduler time: 5.852023873478174 Scheduler overhead time: 0.036597512662410736 Adapter cache time: 0.04172268183901906 Engine time: 0.03765471000224352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.814170852769166,
    "estimated_duration": 3600.178880955553,
    "input_throughput": 5298.976976093073,
    "output_throughput": 4699.071229346746,
    "total_throughput": 9998.048205439818,
    "itl": 183.543775816853,
    "ttft": 2049395.3890118024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 572657,
    "finished_requests": 77452,
    "scheduler_time": 47.718633726893
}
#Debug simulation 
Total elapsed time: 5.814276770222932. Arrivals time: 0.3594040065072477 Scheduler time: 5.345311262644827 Scheduler overhead time: 0.030683666933327913 Adapter cache time: 0.03286584420129657 Engine time: 0.031725925859063864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.78596764896065,
    "estimated_duration": 3600.147377387573,
    "input_throughput": 5218.402479298693,
    "output_throughput": 4635.782719570398,
    "total_throughput": 9854.18519886909,
    "itl": 152.04638141596283,
    "ttft": 2061475.0724485365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794475,
    "arrivals": 572657,
    "finished_requests": 76322,
    "scheduler_time": 42.15640318042031
}
#Debug simulation 
Total elapsed time: 5.786077386699617. Arrivals time: 0.3483410547487438 Scheduler time: 5.303654860239476 Scheduler overhead time: 0.03666374180465937 Adapter cache time: 0.0422828602604568 Engine time: 0.038027381990104914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.821707870345563,
    "estimated_duration": 3600.124827956026,
    "input_throughput": 5298.849320963882,
    "output_throughput": 4699.0226196141875,
    "total_throughput": 9997.871940578068,
    "itl": 183.54371769612905,
    "ttft": 2049364.390713305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 572657,
    "finished_requests": 77450,
    "scheduler_time": 47.71816836921364
}
#Debug simulation 
Total elapsed time: 5.821813254151493. Arrivals time: 0.2844355423003435 Scheduler time: 5.4267960069701076 Scheduler overhead time: 0.030696805100888014 Adapter cache time: 0.0333802318200469 Engine time: 0.032199143432080746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.865457225125283,
    "estimated_duration": 3600.0826947759124,
    "input_throughput": 5218.388185155112,
    "output_throughput": 4635.798234362176,
    "total_throughput": 9854.18641951729,
    "itl": 152.0408949794275,
    "ttft": 2061481.8976266847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 572657,
    "finished_requests": 76320,
    "scheduler_time": 42.15384743526735
}
#Debug simulation 
Total elapsed time: 5.865558165125549. Arrivals time: 0.39577022660523653 Scheduler time: 5.336378842592239 Scheduler overhead time: 0.03662823745980859 Adapter cache time: 0.041755595710128546 Engine time: 0.03790802974253893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.975085438694805,
    "estimated_duration": 3600.162280795562,
    "input_throughput": 5528.838826565748,
    "output_throughput": 4865.42317646005,
    "total_throughput": 10394.262003025797,
    "itl": 176.59390289071425,
    "ttft": 2024831.6407218797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 564989,
    "finished_requests": 80391,
    "scheduler_time": 49.298517120247524
}
#Debug simulation 
Total elapsed time: 5.97522005578503. Arrivals time: 0.28897714940831065 Scheduler time: 5.579840004909784 Scheduler overhead time: 0.031802025623619556 Adapter cache time: 0.026628888212144375 Engine time: 0.033099554013460875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.966932725626975,
    "estimated_duration": 3600.121518275871,
    "input_throughput": 5528.857539656735,
    "output_throughput": 4865.399379182227,
    "total_throughput": 10394.256918838963,
    "itl": 176.59666902121828,
    "ttft": 2024829.6240405657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 564989,
    "finished_requests": 80390,
    "scheduler_time": 49.29744492740358
}
#Debug simulation 
Total elapsed time: 5.967034421861172. Arrivals time: 0.3525418946519494 Scheduler time: 5.5080712623894215 Scheduler overhead time: 0.03176968265324831 Adapter cache time: 0.02673456771299243 Engine time: 0.03306932421401143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.940079836174846,
    "estimated_duration": 3600.0493789921393,
    "input_throughput": 5420.39620730508,
    "output_throughput": 4778.609176970837,
    "total_throughput": 10199.005384275917,
    "itl": 147.51973808116543,
    "ttft": 2037833.9813420344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 564989,
    "finished_requests": 78860,
    "scheduler_time": 43.46790585552231
}
#Debug simulation 
Total elapsed time: 5.940186869353056. Arrivals time: 0.356448776088655 Scheduler time: 5.455644655972719 Scheduler overhead time: 0.03745389776304364 Adapter cache time: 0.03438258869573474 Engine time: 0.038840814493596554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.0661211255937815,
    "estimated_duration": 3600.0292655020917,
    "input_throughput": 5528.829221121351,
    "output_throughput": 4865.206004806525,
    "total_throughput": 10394.035225927877,
    "itl": 176.5946288603395,
    "ttft": 2024819.9250761496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 564989,
    "finished_requests": 80388,
    "scheduler_time": 49.296752141479885
}
#Debug simulation 
Total elapsed time: 6.066246610600501. Arrivals time: 0.37208409141749144 Scheduler time: 5.586944377981126 Scheduler overhead time: 0.031906133983284235 Adapter cache time: 0.02723273914307356 Engine time: 0.03311315830796957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.933694491162896,
    "estimated_duration": 3600.0547518630988,
    "input_throughput": 5420.388117681066,
    "output_throughput": 4778.602045176394,
    "total_throughput": 10198.99016285746,
    "itl": 147.5198944293175,
    "ttft": 2037837.948947372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 564989,
    "finished_requests": 78860,
    "scheduler_time": 43.46787131366649
}
#Debug simulation 
Total elapsed time: 5.933798904996365. Arrivals time: 0.3523929645307362 Scheduler time: 5.4531856388784945 Scheduler overhead time: 0.037514832336455584 Adapter cache time: 0.03443236416205764 Engine time: 0.03875415679067373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.629171513952315,
    "estimated_duration": 3600.1551203021777,
    "input_throughput": 5528.849823095763,
    "output_throughput": 4865.432853495983,
    "total_throughput": 10394.282676591747,
    "itl": 176.5938072908546,
    "ttft": 2024826.3352386479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 564989,
    "finished_requests": 80391,
    "scheduler_time": 49.29858693871686
}
#Debug simulation 
Total elapsed time: 6.629305553622544. Arrivals time: 0.33021405804902315 Scheduler time: 6.191884161438793 Scheduler overhead time: 0.03196248738095164 Adapter cache time: 0.02718224562704563 Engine time: 0.03306525805965066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.955045243259519,
    "estimated_duration": 3600.060247138366,
    "input_throughput": 5420.379843784876,
    "output_throughput": 4778.594750928013,
    "total_throughput": 10198.97459471289,
    "itl": 147.5200618097611,
    "ttft": 2037842.0197578995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 564989,
    "finished_requests": 78860,
    "scheduler_time": 43.46783708790118
}
#Debug simulation 
Total elapsed time: 5.955156403128058. Arrivals time: 0.35297378059476614 Scheduler time: 5.473319707904011 Scheduler overhead time: 0.037630217149853706 Adapter cache time: 0.03459671000018716 Engine time: 0.03906055958941579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.291802769061178,
    "estimated_duration": 3600.1428620414513,
    "input_throughput": 5653.109551452481,
    "output_throughput": 5006.520488406239,
    "total_throughput": 10659.63003985872,
    "itl": 172.08349798763615,
    "ttft": 2010028.2979458286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 561196,
    "finished_requests": 82375,
    "scheduler_time": 50.801408341956794
}
#Debug simulation 
Total elapsed time: 6.2919381111860275. Arrivals time: 0.36450986796990037 Scheduler time: 5.821900589391589 Scheduler overhead time: 0.03276553889736533 Adapter cache time: 0.02384019084274769 Engine time: 0.033654989674687386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.195514438673854,
    "estimated_duration": 3600.042187327898,
    "input_throughput": 5653.117086137734,
    "output_throughput": 5006.407164738708,
    "total_throughput": 10659.524250876442,
    "itl": 172.08386772337653,
    "ttft": 2010007.6503251004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 561196,
    "finished_requests": 82373,
    "scheduler_time": 50.79987927905401
}
#Debug simulation 
Total elapsed time: 6.195617221761495. Arrivals time: 0.3038284978829324 Scheduler time: 5.785767635796219 Scheduler overhead time: 0.03274760814383626 Adapter cache time: 0.023937998805195093 Engine time: 0.034029480535537004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.092582233715802,
    "estimated_duration": 3600.110122849665,
    "input_throughput": 5517.566219412418,
    "output_throughput": 4894.898044410703,
    "total_throughput": 10412.46426382312,
    "itl": 143.5270261972409,
    "ttft": 2023974.1993229177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 561196,
    "finished_requests": 80395,
    "scheduler_time": 44.48144405149615
}
#Debug simulation 
Total elapsed time: 6.092684684786946. Arrivals time: 0.289814377669245 Scheduler time: 5.675143491011113 Scheduler overhead time: 0.03854353167116642 Adapter cache time: 0.031254587695002556 Engine time: 0.039961333852261305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.181607722304761,
    "estimated_duration": 3600.1695404217494,
    "input_throughput": 5653.067660145756,
    "output_throughput": 5006.4833885263415,
    "total_throughput": 10659.551048672098,
    "itl": 172.08424942008287,
    "ttft": 2010038.1557686543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 561196,
    "finished_requests": 82375,
    "scheduler_time": 50.80158040722602
}
#Debug simulation 
Total elapsed time: 6.181734784971923. Arrivals time: 0.30210672691464424 Scheduler time: 5.773366359528154 Scheduler overhead time: 0.03288308857008815 Adapter cache time: 0.024235741700977087 Engine time: 0.03385783964768052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0626387554220855,
    "estimated_duration": 3600.036887317022,
    "input_throughput": 5517.555131165192,
    "output_throughput": 4894.947621809797,
    "total_throughput": 10412.50275297499,
    "itl": 143.53008921419536,
    "ttft": 2023951.983986265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 561196,
    "finished_requests": 80394,
    "scheduler_time": 44.481557801309926
}
#Debug simulation 
Total elapsed time: 6.0627399873919785. Arrivals time: 0.3562272544950247 Scheduler time: 5.578829930629581 Scheduler overhead time: 0.03843652689829469 Adapter cache time: 0.031268916092813015 Engine time: 0.039977294858545065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.157625752966851,
    "estimated_duration": 3600.1282129698757,
    "input_throughput": 5653.132554190591,
    "output_throughput": 5006.540860146532,
    "total_throughput": 10659.673414337123,
    "itl": 172.083242731953,
    "ttft": 2010020.0920284642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 561196,
    "finished_requests": 82375,
    "scheduler_time": 50.80146226496063
}
#Debug simulation 
Total elapsed time: 6.157733272295445. Arrivals time: 0.29443402774631977 Scheduler time: 5.757813095115125 Scheduler overhead time: 0.03273903066292405 Adapter cache time: 0.023681883700191975 Engine time: 0.03384863492101431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.068901278078556,
    "estimated_duration": 3600.078656205487,
    "input_throughput": 5517.544725241197,
    "output_throughput": 4894.922495547319,
    "total_throughput": 10412.467220788516,
    "itl": 143.5341582191786,
    "ttft": 2023977.3182755494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 561196,
    "finished_requests": 80394,
    "scheduler_time": 44.48337393513882
}
#Debug simulation 
Total elapsed time: 6.069001206196845. Arrivals time: 0.35391905857250094 Scheduler time: 5.587828662246466 Scheduler overhead time: 0.038507513236254454 Adapter cache time: 0.03079495159909129 Engine time: 0.039943867828696966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.239794112276286,
    "estimated_duration": 3600.0369453146964,
    "input_throughput": 5749.2615532562895,
    "output_throughput": 5083.03422380583,
    "total_throughput": 10832.29577706212,
    "itl": 169.22597701306046,
    "ttft": 2001904.260058269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3886819434794593,
    "arrivals": 559351,
    "finished_requests": 83651,
    "scheduler_time": 51.57963034341296
}
#Debug simulation 
Total elapsed time: 6.239897795021534. Arrivals time: 0.3677115850150585 Scheduler time: 5.768248469103128 Scheduler overhead time: 0.03306948486715555 Adapter cache time: 0.020584349986165762 Engine time: 0.034966469276696444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.244632205925882,
    "estimated_duration": 3600.111131936978,
    "input_throughput": 5749.143079609334,
    "output_throughput": 5082.929478944856,
    "total_throughput": 10832.07255855419,
    "itl": 169.22681064371017,
    "ttft": 2001942.2840286433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4144669895200058,
    "arrivals": 559351,
    "finished_requests": 83651,
    "scheduler_time": 51.580012993105214
}
#Debug simulation 
Total elapsed time: 6.244737764354795. Arrivals time: 0.36673195706680417 Scheduler time: 5.774020452983677 Scheduler overhead time: 0.03318272111937404 Adapter cache time: 0.020866778679192066 Engine time: 0.03452683752402663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1642304668203,
    "estimated_duration": 3600.0628407279296,
    "input_throughput": 5604.96827214271,
    "output_throughput": 4959.902865581518,
    "total_throughput": 10564.871137724229,
    "itl": 141.50943071086252,
    "ttft": 2019280.7529135065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41519293082878095,
    "arrivals": 559351,
    "finished_requests": 81543,
    "scheduler_time": 45.057550337637345
}
#Debug simulation 
Total elapsed time: 6.164331613108516. Arrivals time: 0.2931676469743252 Scheduler time: 5.745904752518982 Scheduler overhead time: 0.03931531636044383 Adapter cache time: 0.02723444066941738 Engine time: 0.04053440457209945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.2434483407996595,
    "estimated_duration": 3600.0494953499706,
    "input_throughput": 5749.2415109109315,
    "output_throughput": 5083.016503977564,
    "total_throughput": 10832.258014888495,
    "itl": 169.2261521288597,
    "ttft": 2001913.081232786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3973059861455115,
    "arrivals": 559351,
    "finished_requests": 83651,
    "scheduler_time": 51.57963214906417
}
#Debug simulation 
Total elapsed time: 6.243583437986672. Arrivals time: 0.29893960105255246 Scheduler time: 5.840672428254038 Scheduler overhead time: 0.03312157979235053 Adapter cache time: 0.021061299834400415 Engine time: 0.0343178347684443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.155987652018666,
    "estimated_duration": 3600.093378264667,
    "input_throughput": 5604.920450626312,
    "output_throughput": 4959.78468442029,
    "total_throughput": 10564.705135046603,
    "itl": 141.50423996697393,
    "ttft": 2019303.2106225796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42047458985820446,
    "arrivals": 559351,
    "finished_requests": 81542,
    "scheduler_time": 45.05573190899661
}
#Debug simulation 
Total elapsed time: 6.156092087272555. Arrivals time: 0.29196366341784596 Scheduler time: 5.738997686188668 Scheduler overhead time: 0.03898510197177529 Adapter cache time: 0.02754793455824256 Engine time: 0.040417070500552654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.212489616125822,
    "estimated_duration": 3600.0286833723235,
    "input_throughput": 5749.274747614396,
    "output_throughput": 5083.045889195062,
    "total_throughput": 10832.320636809458,
    "itl": 169.22591055717547,
    "ttft": 2001899.2600297502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37973638745257676,
    "arrivals": 559351,
    "finished_requests": 83651,
    "scheduler_time": 51.579715629762546
}
#Debug simulation 
Total elapsed time: 6.212625198997557. Arrivals time: 0.2974620587192476 Scheduler time: 5.811433814000338 Scheduler overhead time: 0.033084554597735405 Adapter cache time: 0.02073949435725808 Engine time: 0.03445639554411173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.749028125777841,
    "estimated_duration": 3600.135626673861,
    "input_throughput": 5604.93856134048,
    "output_throughput": 4959.785366891007,
    "total_throughput": 10564.723928231486,
    "itl": 141.51041147256595,
    "ttft": 2019332.3692137292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4258820026740429,
    "arrivals": 559351,
    "finished_requests": 81543,
    "scheduler_time": 45.057835827111546
}
#Debug simulation 
Total elapsed time: 6.749129235744476. Arrivals time: 0.35906873596832156 Scheduler time: 6.264051716774702 Scheduler overhead time: 0.03909958293661475 Adapter cache time: 0.027850680984556675 Engine time: 0.04062929190695286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.293173494748771,
    "estimated_duration": 3600.133350853796,
    "input_throughput": 5781.575839423757,
    "output_throughput": 5104.9686800188665,
    "total_throughput": 10886.544519442623,
    "itl": 168.44105864311751,
    "ttft": 1997619.5704604676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 558425,
    "finished_requests": 84188,
    "scheduler_time": 51.770596413912216
}
#Debug simulation 
Total elapsed time: 6.293307658750564. Arrivals time: 0.301790704485029 Scheduler time: 5.886467800009996 Scheduler overhead time: 0.03346128761768341 Adapter cache time: 0.021445981692522764 Engine time: 0.034558961633592844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.350085238926113,
    "estimated_duration": 3600.006108279877,
    "input_throughput": 5781.504912486079,
    "output_throughput": 5104.830227296735,
    "total_throughput": 10886.335139782814,
    "itl": 168.44101084566725,
    "ttft": 1997600.4026542169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 558425,
    "finished_requests": 84185,
    "scheduler_time": 51.768216197028686
}
#Debug simulation 
Total elapsed time: 6.350190839264542. Arrivals time: 0.34706773655489087 Scheduler time: 5.897140476386994 Scheduler overhead time: 0.03345076786354184 Adapter cache time: 0.022064898628741503 Engine time: 0.034857464488595724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.772716822102666,
    "estimated_duration": 3600.056073750463,
    "input_throughput": 5637.61941042666,
    "output_throughput": 4984.12542260966,
    "total_throughput": 10621.74483303632,
    "itl": 141.26485881902966,
    "ttft": 2013958.1270802151,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 558425,
    "finished_requests": 82088,
    "scheduler_time": 45.33018662273891
}
#Debug simulation 
Total elapsed time: 6.772819478996098. Arrivals time: 0.293796350248158 Scheduler time: 6.352358186151832 Scheduler overhead time: 0.039399494882673025 Adapter cache time: 0.027869321405887604 Engine time: 0.04086036514490843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.299578084144741,
    "estimated_duration": 3600.184144985531,
    "input_throughput": 5781.494268561547,
    "output_throughput": 5104.896655244245,
    "total_throughput": 10886.390923805791,
    "itl": 168.4420659776632,
    "ttft": 1997646.5549073634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 558425,
    "finished_requests": 84188,
    "scheduler_time": 51.77094154541805
}
#Debug simulation 
Total elapsed time: 6.299698933027685. Arrivals time: 0.37352371541783214 Scheduler time: 5.820396564900875 Scheduler overhead time: 0.03344352729618549 Adapter cache time: 0.021495876368135214 Engine time: 0.03526471648365259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.18299520900473,
    "estimated_duration": 3600.0349793688756,
    "input_throughput": 5637.440501635883,
    "output_throughput": 4984.038239302205,
    "total_throughput": 10621.478740938088,
    "itl": 141.26419597306713,
    "ttft": 2013945.7070058521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 558425,
    "finished_requests": 82086,
    "scheduler_time": 45.329904902729055
}
#Debug simulation 
Total elapsed time: 6.183099429123104. Arrivals time: 0.3316149031743407 Scheduler time: 5.726458314340562 Scheduler overhead time: 0.03884638287127018 Adapter cache time: 0.02781400131061673 Engine time: 0.04017976578325033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.304755643941462,
    "estimated_duration": 3600.0560316374454,
    "input_throughput": 5781.612512995506,
    "output_throughput": 5104.949433701154,
    "total_throughput": 10886.56194669666,
    "itl": 168.43919057357968,
    "ttft": 1997619.799503887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 558425,
    "finished_requests": 84187,
    "scheduler_time": 51.76959103634493
}
#Debug simulation 
Total elapsed time: 6.304861434735358. Arrivals time: 0.3020333736203611 Scheduler time: 5.8976483484730124 Scheduler overhead time: 0.03345925081521273 Adapter cache time: 0.021661380771547556 Engine time: 0.03462410159409046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.185570660047233,
    "estimated_duration": 3600.093830738266,
    "input_throughput": 5637.533896120146,
    "output_throughput": 4984.121760048791,
    "total_throughput": 10621.655656168938,
    "itl": 141.26433746167186,
    "ttft": 2014029.2631927829,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 558425,
    "finished_requests": 82089,
    "scheduler_time": 45.33055791686911
}
#Debug simulation 
Total elapsed time: 6.185673933941871. Arrivals time: 0.2935801837593317 Scheduler time: 5.766615950036794 Scheduler overhead time: 0.0391010451130569 Adapter cache time: 0.027689114678651094 Engine time: 0.04033567197620869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.296659333165735,
    "estimated_duration": 3600.0155146338893,
    "input_throughput": 5811.1732893816525,
    "output_throughput": 5131.425107727257,
    "total_throughput": 10942.598397108908,
    "itl": 167.6316815800135,
    "ttft": 1991771.141083165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3764399925037283,
    "arrivals": 557951,
    "finished_requests": 84715,
    "scheduler_time": 52.06256614072751
}
#Debug simulation 
Total elapsed time: 6.29675992205739. Arrivals time: 0.3723843446932733 Scheduler time: 5.821504270192236 Scheduler overhead time: 0.033674647100269794 Adapter cache time: 0.018946326803416014 Engine time: 0.0347438040189445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.382011125795543,
    "estimated_duration": 3600.0988611500857,
    "input_throughput": 5811.189583087347,
    "output_throughput": 5131.566579840604,
    "total_throughput": 10942.756162927952,
    "itl": 167.63392448182117,
    "ttft": 1991780.9609819178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4012810023850761,
    "arrivals": 557951,
    "finished_requests": 84717,
    "scheduler_time": 52.063088462133905
}
#Debug simulation 
Total elapsed time: 6.382130392827094. Arrivals time: 0.37953520100563765 Scheduler time: 5.900272975675762 Scheduler overhead time: 0.03341185534372926 Adapter cache time: 0.0186443286947906 Engine time: 0.034667865838855505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.171560970135033,
    "estimated_duration": 3600.067646521702,
    "input_throughput": 5655.606504969951,
    "output_throughput": 5002.612941843073,
    "total_throughput": 10658.219446813024,
    "itl": 140.87368306652616,
    "ttft": 2009446.2377731367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39862634660676216,
    "arrivals": 557951,
    "finished_requests": 82494,
    "scheduler_time": 45.51279873292056
}
#Debug simulation 
Total elapsed time: 6.171672063414007. Arrivals time: 0.3558167847804725 Scheduler time: 5.693536247592419 Scheduler overhead time: 0.03910583630204201 Adapter cache time: 0.024571307003498077 Engine time: 0.04046354815363884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.2758634169586,
    "estimated_duration": 3600.0410808698675,
    "input_throughput": 5811.16285343751,
    "output_throughput": 5131.454776492805,
    "total_throughput": 10942.617629930315,
    "itl": 167.63246845618727,
    "ttft": 1991776.8122848154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3845285943290221,
    "arrivals": 557951,
    "finished_requests": 84716,
    "scheduler_time": 52.06262773766593
}
#Debug simulation 
Total elapsed time: 6.276001013815403. Arrivals time: 0.36864179745316505 Scheduler time: 5.805324616376311 Scheduler overhead time: 0.033213214948773384 Adapter cache time: 0.01849689381197095 Engine time: 0.034743902273476124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.185579744167626,
    "estimated_duration": 3600.015420942703,
    "input_throughput": 5655.6891066506505,
    "output_throughput": 5002.6063486372595,
    "total_throughput": 10658.29545528791,
    "itl": 140.877240121421,
    "ttft": 2009452.5457655482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40378225184977073,
    "arrivals": 557951,
    "finished_requests": 82493,
    "scheduler_time": 45.51339371536057
}
#Debug simulation 
Total elapsed time: 6.185693275183439. Arrivals time: 0.36704395059496164 Scheduler time: 5.694252758752555 Scheduler overhead time: 0.03901320742443204 Adapter cache time: 0.026620409917086363 Engine time: 0.040468753315508366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.32702952157706,
    "estimated_duration": 3600.0905554517844,
    "input_throughput": 5811.202989968842,
    "output_throughput": 5131.5784187772,
    "total_throughput": 10942.781408746043,
    "itl": 167.63206401526642,
    "ttft": 1991769.5390190482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36777618627296804,
    "arrivals": 557951,
    "finished_requests": 84717,
    "scheduler_time": 52.06305430660971
}
#Debug simulation 
Total elapsed time: 6.327136632986367. Arrivals time: 0.37648663157597184 Scheduler time: 5.848081015050411 Scheduler overhead time: 0.033635514322668314 Adapter cache time: 0.01869381219148636 Engine time: 0.034651693888008595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2135362308472395,
    "estimated_duration": 3600.080628433813,
    "input_throughput": 5655.687497437485,
    "output_throughput": 5002.696566780827,
    "total_throughput": 10658.384064218311,
    "itl": 140.87374972484864,
    "ttft": 2009480.6547003244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 122,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40881240330636476,
    "arrivals": 557951,
    "finished_requests": 82496,
    "scheduler_time": 45.51318394960465
}
#Debug simulation 
Total elapsed time: 6.213637343142182. Arrivals time: 0.3629665463231504 Scheduler time: 5.72844535857439 Scheduler overhead time: 0.039073335006833076 Adapter cache time: 0.024540669284760952 Engine time: 0.0403657709248364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.507167614065111,
    "estimated_duration": 3600.12162901653,
    "input_throughput": 5986.6435695639775,
    "output_throughput": 5318.616694967248,
    "total_throughput": 11305.260264531225,
    "itl": 162.5035661570764,
    "ttft": 1956915.1745079102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 518627,
    "finished_requests": 87390,
    "scheduler_time": 54.03517588393615
}
#Debug simulation 
Total elapsed time: 6.507270019967109. Arrivals time: 0.368460516911 Scheduler time: 6.0178130068816245 Scheduler overhead time: 0.034577971789985895 Adapter cache time: 0.03442035708576441 Engine time: 0.035965824499726295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.469413271639496,
    "estimated_duration": 3600.1198117148424,
    "input_throughput": 5986.646591557142,
    "output_throughput": 5318.619379747644,
    "total_throughput": 11305.265971304785,
    "itl": 162.50103239048195,
    "ttft": 1956887.353641161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 518627,
    "finished_requests": 87390,
    "scheduler_time": 54.03552984532543
}
#Debug simulation 
Total elapsed time: 6.469552994705737. Arrivals time: 0.3030037609860301 Scheduler time: 6.045776404440403 Scheduler overhead time: 0.034485263749957085 Adapter cache time: 0.03430020995438099 Engine time: 0.035969296004623175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.464122856035829,
    "estimated_duration": 3600.1277326084255,
    "input_throughput": 5910.466400197137,
    "output_throughput": 5257.90594276641,
    "total_throughput": 11168.372342963547,
    "itl": 134.20857880766596,
    "ttft": 1966680.2155657185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 518627,
    "finished_requests": 86283,
    "scheduler_time": 47.90424058942086
}
#Debug simulation 
Total elapsed time: 6.464227834250778. Arrivals time: 0.29765121126547456 Scheduler time: 6.023493445012718 Scheduler overhead time: 0.04102496011182666 Adapter cache time: 0.04047098429873586 Engine time: 0.04247914720326662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.506719445809722,
    "estimated_duration": 3600.1190103919994,
    "input_throughput": 5986.647924078831,
    "output_throughput": 5318.620563578287,
    "total_throughput": 11305.268487657118,
    "itl": 162.5029353419659,
    "ttft": 1956915.7512038678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 518627,
    "finished_requests": 87390,
    "scheduler_time": 54.0351396562483
}
#Debug simulation 
Total elapsed time: 6.5068499147892. Arrivals time: 0.36662133457139134 Scheduler time: 6.018621784634888 Scheduler overhead time: 0.034557396080344915 Adapter cache time: 0.034985593520104885 Engine time: 0.03595771035179496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.513305795378983,
    "estimated_duration": 3600.0178097318208,
    "input_throughput": 5910.402704809132,
    "output_throughput": 5257.950377031627,
    "total_throughput": 11168.35308184076,
    "itl": 134.20481418278544,
    "ttft": 1966657.5060837667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 518627,
    "finished_requests": 86280,
    "scheduler_time": 47.90160881836932
}
#Debug simulation 
Total elapsed time: 6.513414914254099. Arrivals time: 0.39772950718179345 Scheduler time: 5.973766730632633 Scheduler overhead time: 0.04093697341158986 Adapter cache time: 0.0392398159019649 Engine time: 0.04257277771830559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.510219593066722,
    "estimated_duration": 3600.0669675234963,
    "input_throughput": 5986.540860051193,
    "output_throughput": 5318.643562114524,
    "total_throughput": 11305.184422165716,
    "itl": 162.5026147165349,
    "ttft": 1956900.798586935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 518627,
    "finished_requests": 87389,
    "scheduler_time": 54.03448790376343
}
#Debug simulation 
Total elapsed time: 6.510324039962143. Arrivals time: 0.36715026339516044 Scheduler time: 6.021473801229149 Scheduler overhead time: 0.03460536152124405 Adapter cache time: 0.034890029579401016 Engine time: 0.03595540625974536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.503039901144803,
    "estimated_duration": 3600.0799008672298,
    "input_throughput": 5910.29437843155,
    "output_throughput": 5257.872747613245,
    "total_throughput": 11168.167126044795,
    "itl": 134.20533383044818,
    "ttft": 1966673.354744171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 518627,
    "finished_requests": 86281,
    "scheduler_time": 47.902691299131895
}
#Debug simulation 
Total elapsed time: 6.5031523341313004. Arrivals time: 0.3035236205905676 Scheduler time: 6.056658468209207 Scheduler overhead time: 0.04103496903553605 Adapter cache time: 0.04016491677612066 Engine time: 0.04256106726825237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.7202876238152385,
    "estimated_duration": 3600.0855989754564,
    "input_throughput": 6220.358206586262,
    "output_throughput": 5507.237385033957,
    "total_throughput": 11727.59559162022,
    "itl": 156.2518448349744,
    "ttft": 1940179.2728227603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 514902,
    "finished_requests": 91050,
    "scheduler_time": 55.885905046705744
}
#Debug simulation 
Total elapsed time: 6.720421485137194. Arrivals time: 0.31142189260572195 Scheduler time: 6.288786561693996 Scheduler overhead time: 0.03580141207203269 Adapter cache time: 0.03048366541042924 Engine time: 0.037130776327103376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.766216297168285,
    "estimated_duration": 3600.1304746539204,
    "input_throughput": 6220.505106041411,
    "output_throughput": 5507.370952133614,
    "total_throughput": 11727.876058175025,
    "itl": 156.2512706221317,
    "ttft": 1940178.402027965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 514902,
    "finished_requests": 91053,
    "scheduler_time": 55.88641551465226
}
#Debug simulation 
Total elapsed time: 6.766334933228791. Arrivals time: 0.314683232922107 Scheduler time: 6.330852007493377 Scheduler overhead time: 0.03597128018736839 Adapter cache time: 0.030716050416231155 Engine time: 0.03737393859773874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.66420839400962,
    "estimated_duration": 3600.020421897363,
    "input_throughput": 6120.194726114295,
    "output_throughput": 5423.670066212938,
    "total_throughput": 11543.864792327233,
    "itl": 129.97325210475006,
    "ttft": 1952383.226851521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 514902,
    "finished_requests": 89553,
    "scheduler_time": 49.44363640327358
}
#Debug simulation 
Total elapsed time: 6.6643149009905756. Arrivals time: 0.3040856486186385 Scheduler time: 6.219759820494801 Scheduler overhead time: 0.04246425721794367 Adapter cache time: 0.03434640308842063 Engine time: 0.043874545488506556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.7142358450219035,
    "estimated_duration": 3600.1144934179188,
    "input_throughput": 6220.308282123408,
    "output_throughput": 5507.193184063672,
    "total_throughput": 11727.501466187081,
    "itl": 156.2516165477034,
    "ttft": 1940191.0535080952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 514902,
    "finished_requests": 91050,
    "scheduler_time": 55.88598153786338
}
#Debug simulation 
Total elapsed time: 6.7143431580625474. Arrivals time: 0.31144807068631053 Scheduler time: 6.282604507170618 Scheduler overhead time: 0.03579343389719725 Adapter cache time: 0.03059933055192232 Engine time: 0.037180837243795395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.628112477250397,
    "estimated_duration": 3600.116004822333,
    "input_throughput": 6120.425833635713,
    "output_throughput": 5423.797448150183,
    "total_throughput": 11544.223281785897,
    "itl": 129.97492186906265,
    "ttft": 1952483.438547672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879448,
    "arrivals": 514902,
    "finished_requests": 89557,
    "scheduler_time": 49.44624399315487
}
#Debug simulation 
Total elapsed time: 6.628216451965272. Arrivals time: 0.3041715514846146 Scheduler time: 6.184302218258381 Scheduler overhead time: 0.04211925528943539 Adapter cache time: 0.034241749439388514 Engine time: 0.04367288248613477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.729733003303409,
    "estimated_duration": 3600.0776820070623,
    "input_throughput": 6220.371885840898,
    "output_throughput": 5507.249496057154,
    "total_throughput": 11727.621381898052,
    "itl": 156.2519949176937,
    "ttft": 1940169.0619029363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 514902,
    "finished_requests": 91050,
    "scheduler_time": 55.885785224433
}
#Debug simulation 
Total elapsed time: 6.729839129373431. Arrivals time: 0.3107914556749165 Scheduler time: 6.2985124620608985 Scheduler overhead time: 0.03594684926792979 Adapter cache time: 0.030410630628466606 Engine time: 0.03744344646111131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.655039624776691,
    "estimated_duration": 3600.094141556988,
    "input_throughput": 6120.356061153078,
    "output_throughput": 5423.750388803802,
    "total_throughput": 11544.10644995688,
    "itl": 129.9776442407596,
    "ttft": 1952463.4109068478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378326,
    "arrivals": 514902,
    "finished_requests": 89557,
    "scheduler_time": 49.446026771119165
}
#Debug simulation 
Total elapsed time: 6.6551484749652445. Arrivals time: 0.30446388525888324 Scheduler time: 6.210162366740406 Scheduler overhead time: 0.04215618781745434 Adapter cache time: 0.03448787238448858 Engine time: 0.044160871766507626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.782026792876422,
    "estimated_duration": 3600.0153510263485,
    "input_throughput": 6317.394450419816,
    "output_throughput": 5575.010116075782,
    "total_throughput": 11892.404566495597,
    "itl": 154.4566189614233,
    "ttft": 1925044.2400655483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 512974,
    "finished_requests": 91928,
    "scheduler_time": 56.52418947461134
}
#Debug simulation 
Total elapsed time: 6.782161102630198. Arrivals time: 0.3105603461153805 Scheduler time: 6.3520153146237135 Scheduler overhead time: 0.036146662663668394 Adapter cache time: 0.028629921842366457 Engine time: 0.037902372889220715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.748249516822398,
    "estimated_duration": 3600.0694370548376,
    "input_throughput": 6317.299540368163,
    "output_throughput": 5574.926359314632,
    "total_throughput": 11892.225899682795,
    "itl": 154.45770949817134,
    "ttft": 1925065.6433895747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 512974,
    "finished_requests": 91928,
    "scheduler_time": 56.5245371816288
}
#Debug simulation 
Total elapsed time: 6.748350623063743. Arrivals time: 0.37086018873378634 Scheduler time: 6.258090770337731 Scheduler overhead time: 0.03599798213690519 Adapter cache time: 0.028928813990205526 Engine time: 0.037659225054085255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.872294386848807,
    "estimated_duration": 3600.1403134168995,
    "input_throughput": 6200.547772209842,
    "output_throughput": 5479.533096663387,
    "total_throughput": 11680.080868873229,
    "itl": 128.49051853673996,
    "ttft": 1937103.4118473963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 512974,
    "finished_requests": 90215,
    "scheduler_time": 49.85722348790598
}
#Debug simulation 
Total elapsed time: 6.872435330878943. Arrivals time: 0.36746925953775644 Scheduler time: 6.365190151613206 Scheduler overhead time: 0.04269144777208567 Adapter cache time: 0.03280931757763028 Engine time: 0.044224581215530634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.7939623119309545,
    "estimated_duration": 3600.025095770936,
    "input_throughput": 6317.377350151418,
    "output_throughput": 5574.995025333855,
    "total_throughput": 11892.372375485273,
    "itl": 154.45663761636442,
    "ttft": 1925052.3010089395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 512974,
    "finished_requests": 91928,
    "scheduler_time": 56.5241105920459
}
#Debug simulation 
Total elapsed time: 6.794069239869714. Arrivals time: 0.375513406470418 Scheduler time: 6.299118615221232 Scheduler overhead time: 0.03622123971581459 Adapter cache time: 0.028595461044460535 Engine time: 0.037753849755972624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.712500151246786,
    "estimated_duration": 3600.063305244069,
    "input_throughput": 6200.466521653751,
    "output_throughput": 5479.566142979979,
    "total_throughput": 11680.03266463373,
    "itl": 128.49183268841324,
    "ttft": 1937089.3466528507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 512974,
    "finished_requests": 90213,
    "scheduler_time": 49.857245878188984
}
#Debug simulation 
Total elapsed time: 6.7126034600660205. Arrivals time: 0.3682709112763405 Scheduler time: 6.204678923357278 Scheduler overhead time: 0.042520299553871155 Adapter cache time: 0.03275397699326277 Engine time: 0.04432376939803362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.790233094710857,
    "estimated_duration": 3600.1612098370347,
    "input_throughput": 6317.247665981461,
    "output_throughput": 5574.940629091585,
    "total_throughput": 11892.188295073047,
    "itl": 154.45679894292633,
    "ttft": 1925069.8519418724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 512974,
    "finished_requests": 91930,
    "scheduler_time": 56.52627734141243
}
#Debug simulation 
Total elapsed time: 6.790343923959881. Arrivals time: 0.3164452607743442 Scheduler time: 6.354567871429026 Scheduler overhead time: 0.036220381036400795 Adapter cache time: 0.02878249203786254 Engine time: 0.037525230553001165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.72861427301541,
    "estimated_duration": 3600.092527931138,
    "input_throughput": 6200.552298812188,
    "output_throughput": 5479.591662422221,
    "total_throughput": 11680.14396123441,
    "itl": 128.49236552774576,
    "ttft": 1937060.8496175061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 512974,
    "finished_requests": 90214,
    "scheduler_time": 49.858349525354384
}
#Debug simulation 
Total elapsed time: 6.728717411868274. Arrivals time: 0.3061074218712747 Scheduler time: 6.282302473671734 Scheduler overhead time: 0.042866484727710485 Adapter cache time: 0.0330095118843019 Engine time: 0.04443675698712468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.8569134920835495,
    "estimated_duration": 3600.1169299191115,
    "input_throughput": 6313.882699501853,
    "output_throughput": 5635.70306047139,
    "total_throughput": 11949.585759973243,
    "itl": 153.57806170781663,
    "ttft": 1920954.2628955308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3886819434794593,
    "arrivals": 512003,
    "finished_requests": 92633,
    "scheduler_time": 57.31629045615075
}
#Debug simulation 
Total elapsed time: 6.857039433903992. Arrivals time: 0.3214569892734289 Scheduler time: 6.418368094600737 Scheduler overhead time: 0.03629746055230498 Adapter cache time: 0.02630084566771984 Engine time: 0.037725287955254316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.8512446489185095,
    "estimated_duration": 3600.0375096995276,
    "input_throughput": 6313.665604527849,
    "output_throughput": 5635.744612475825,
    "total_throughput": 11949.410217003675,
    "itl": 153.57923385747054,
    "ttft": 1920950.9514884606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4148755848384462,
    "arrivals": 512003,
    "finished_requests": 92631,
    "scheduler_time": 57.31481526935636
}
#Debug simulation 
Total elapsed time: 6.851354383863509. Arrivals time: 0.37662195041775703 Scheduler time: 6.357270079664886 Scheduler overhead time: 0.036400129552930593 Adapter cache time: 0.02632409753277898 Engine time: 0.037866372149437666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.747443481814116,
    "estimated_duration": 3600.0243455575205,
    "input_throughput": 6198.330582829518,
    "output_throughput": 5534.861458531109,
    "total_throughput": 11733.192041360628,
    "itl": 127.66421726257634,
    "ttft": 1934911.5217177602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4155301783233891,
    "arrivals": 512003,
    "finished_requests": 90885,
    "scheduler_time": 50.45223143760202
}
#Debug simulation 
Total elapsed time: 6.747546526137739. Arrivals time: 0.37037449795752764 Scheduler time: 6.239358191844076 Scheduler overhead time: 0.04286923538893461 Adapter cache time: 0.03005711594596505 Engine time: 0.0448414646089077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.862672072835267,
    "estimated_duration": 3600.168407853485,
    "input_throughput": 6313.792418825387,
    "output_throughput": 5635.622476921003,
    "total_throughput": 11949.414895746391,
    "itl": 153.57900522070983,
    "ttft": 1920974.8289743424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3973059861455115,
    "arrivals": 512003,
    "finished_requests": 92633,
    "scheduler_time": 57.3166477572068
}
#Debug simulation 
Total elapsed time: 6.862783279735595. Arrivals time: 0.3149469792842865 Scheduler time: 6.429932191036642 Scheduler overhead time: 0.03649944765493274 Adapter cache time: 0.026514628436416388 Engine time: 0.03796803066506982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.750391582027078,
    "estimated_duration": 3600.0784609929547,
    "input_throughput": 6198.136024470292,
    "output_throughput": 5534.845758473872,
    "total_throughput": 11732.981782944164,
    "itl": 127.65560533509579,
    "ttft": 1934881.2834589055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4209375911392274,
    "arrivals": 512003,
    "finished_requests": 90885,
    "scheduler_time": 50.450824598306106
}
#Debug simulation 
Total elapsed time: 6.7504955367185175. Arrivals time: 0.3026834325864911 Scheduler time: 6.309886178467423 Scheduler overhead time: 0.04303250974044204 Adapter cache time: 0.029624387621879578 Engine time: 0.04515324207022786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.82560491701588,
    "estimated_duration": 3600.115020860934,
    "input_throughput": 6313.886047608602,
    "output_throughput": 5635.706048955078,
    "total_throughput": 11949.59209656368,
    "itl": 153.57833304502316,
    "ttft": 1920953.015180843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37973638745257676,
    "arrivals": 512003,
    "finished_requests": 92633,
    "scheduler_time": 57.31642517579641
}
#Debug simulation 
Total elapsed time: 6.825708268210292. Arrivals time: 0.3698738398961723 Scheduler time: 6.338594357017428 Scheduler overhead time: 0.036276630125939846 Adapter cache time: 0.026436330284923315 Engine time: 0.03757207980379462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.727499207016081,
    "estimated_duration": 3600.118474042895,
    "input_throughput": 6198.457401025553,
    "output_throughput": 5534.92979291397,
    "total_throughput": 11733.387193939523,
    "itl": 127.66172200408255,
    "ttft": 1934894.8654666329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42634500395506597,
    "arrivals": 512003,
    "finished_requests": 90888,
    "scheduler_time": 50.4530873879469
}
#Debug simulation 
Total elapsed time: 6.727627442218363. Arrivals time: 0.29986293194815516 Scheduler time: 6.291453805752099 Scheduler overhead time: 0.042636117432266474 Adapter cache time: 0.02965697506442666 Engine time: 0.044024677481502295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.969531139824539,
    "estimated_duration": 3600.066307450266,
    "input_throughput": 6371.110707748225,
    "output_throughput": 5638.274761215746,
    "total_throughput": 12009.38546896397,
    "itl": 152.87465367599538,
    "ttft": 1921869.9133575102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3825609679915938,
    "arrivals": 511517,
    "finished_requests": 92837,
    "scheduler_time": 57.23402750495067
}
#Debug simulation 
Total elapsed time: 6.969634050969034. Arrivals time: 0.31087617902085185 Scheduler time: 6.543048926163465 Scheduler overhead time: 0.03651421703398228 Adapter cache time: 0.024521795567125082 Engine time: 0.03767201071605086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.770725245587528,
    "estimated_duration": 3600.128210018114,
    "input_throughput": 6371.001159396097,
    "output_throughput": 5638.177813644551,
    "total_throughput": 12009.178973040649,
    "itl": 152.87590662315932,
    "ttft": 1921914.5619693804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4084868889302015,
    "arrivals": 511517,
    "finished_requests": 92837,
    "scheduler_time": 57.23412819188864
}
#Debug simulation 
Total elapsed time: 6.770827475935221. Arrivals time: 0.31405591731891036 Scheduler time: 6.341395834926516 Scheduler overhead time: 0.03644022438675165 Adapter cache time: 0.02433320926502347 Engine time: 0.03767665755003691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.668273533694446,
    "estimated_duration": 3600.008067712827,
    "input_throughput": 6244.372117277492,
    "output_throughput": 5535.025651389598,
    "total_throughput": 11779.39776866709,
    "itl": 127.27967941952747,
    "ttft": 1935799.0897489712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40910589313134654,
    "arrivals": 511517,
    "finished_requests": 91015,
    "scheduler_time": 50.37786631834424
}
#Debug simulation 
Total elapsed time: 6.668378674890846. Arrivals time: 0.29868552973493934 Scheduler time: 6.234258454293013 Scheduler overhead time: 0.04288617195561528 Adapter cache time: 0.02815532125532627 Engine time: 0.0442468230612576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.697751120198518,
    "estimated_duration": 3600.081303226894,
    "input_throughput": 6371.084169527279,
    "output_throughput": 5638.251275549239,
    "total_throughput": 12009.335445076518,
    "itl": 152.87493677467876,
    "ttft": 1921878.6947884653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39132588555570713,
    "arrivals": 511517,
    "finished_requests": 92837,
    "scheduler_time": 57.23394661461422
}
#Debug simulation 
Total elapsed time: 6.697885757312179. Arrivals time: 0.2954155211336911 Scheduler time: 6.288264958187938 Scheduler overhead time: 0.0364938136190176 Adapter cache time: 0.023796151857823133 Engine time: 0.037105553317815065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.622346431016922,
    "estimated_duration": 3600.1003762622627,
    "input_throughput": 6244.4133914232625,
    "output_throughput": 5534.936506600447,
    "total_throughput": 11779.34989802371,
    "itl": 127.28239480218801,
    "ttft": 1935827.192595247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41438755216077,
    "arrivals": 511517,
    "finished_requests": 91018,
    "scheduler_time": 50.379432080490574
}
#Debug simulation 
Total elapsed time: 6.6224437351338565. Arrivals time: 0.28634352097287774 Scheduler time: 6.203166962135583 Scheduler overhead time: 0.04242085898295045 Adapter cache time: 0.02677651820704341 Engine time: 0.04379127314314246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.6618563532829285,
    "estimated_duration": 3600.0456695677285,
    "input_throughput": 6371.147231238893,
    "output_throughput": 5638.3070835979925,
    "total_throughput": 12009.454314836885,
    "itl": 152.87417023923462,
    "ttft": 1921862.5164235223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3737562868627724,
    "arrivals": 511517,
    "finished_requests": 92837,
    "scheduler_time": 57.234047424633076
}
#Debug simulation 
Total elapsed time: 6.661972159985453. Arrivals time: 0.289295116905123 Scheduler time: 6.259402243420482 Scheduler overhead time: 0.03602087078616023 Adapter cache time: 0.023567943833768368 Engine time: 0.036934840492904186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.658593927975744,
    "estimated_duration": 3600.0697399499472,
    "input_throughput": 6244.389032394842,
    "output_throughput": 5534.982775160639,
    "total_throughput": 11779.37180755548,
    "itl": 127.28475251217627,
    "ttft": 1935824.7834802414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4197949649766085,
    "arrivals": 511517,
    "finished_requests": 91017,
    "scheduler_time": 50.380006440777315
}
#Debug simulation 
Total elapsed time: 6.658691965974867. Arrivals time: 0.32278078189119697 Scheduler time: 6.202662718016654 Scheduler overhead time: 0.04238951625302434 Adapter cache time: 0.02710907207801938 Engine time: 0.04390588216483593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.930527160875499,
    "estimated_duration": 3600.0050289498577,
    "input_throughput": 6541.88308366611,
    "output_throughput": 5761.938339861391,
    "total_throughput": 12303.8214235275,
    "itl": 149.15203517777718,
    "ttft": 1894486.917398786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 507129,
    "finished_requests": 95408,
    "scheduler_time": 58.4376995493754
}
#Debug simulation 
Total elapsed time: 6.930657736025751. Arrivals time: 0.35634426260367036 Scheduler time: 6.456045210827142 Scheduler overhead time: 0.03697566781193018 Adapter cache time: 0.025944550521671772 Engine time: 0.038160566706210375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.866433455143124,
    "estimated_duration": 3600.1267497794456,
    "input_throughput": 6541.798841233249,
    "output_throughput": 5761.934076701833,
    "total_throughput": 12303.732917935082,
    "itl": 149.1541211459274,
    "ttft": 1894525.9877419244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 507129,
    "finished_requests": 95411,
    "scheduler_time": 58.43948830529193
}
#Debug simulation 
Total elapsed time: 6.866541516035795. Arrivals time: 0.29823259729892015 Scheduler time: 6.450012533459812 Scheduler overhead time: 0.03696143953129649 Adapter cache time: 0.026080697309225798 Engine time: 0.038083020597696304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.80255754198879,
    "estimated_duration": 3600.0241317773143,
    "input_throughput": 6418.697529283493,
    "output_throughput": 5662.774540884944,
    "total_throughput": 12081.472070168436,
    "itl": 124.76665366872732,
    "ttft": 1908100.181221643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 507129,
    "finished_requests": 93677,
    "scheduler_time": 51.634350972365304
}
#Debug simulation 
Total elapsed time: 6.802656315732747. Arrivals time: 0.34543440537527204 Scheduler time: 6.320009731221944 Scheduler overhead time: 0.04326394898816943 Adapter cache time: 0.028962832875549793 Engine time: 0.04475372936576605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.914970617275685,
    "estimated_duration": 3600.090948053702,
    "input_throughput": 6541.7269562959655,
    "output_throughput": 5761.800826508058,
    "total_throughput": 12303.527782804023,
    "itl": 149.1535658254538,
    "ttft": 1894525.2246576978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 507129,
    "finished_requests": 95408,
    "scheduler_time": 58.43848094337131
}
#Debug simulation 
Total elapsed time: 6.915073287207633. Arrivals time: 0.29765653563663363 Scheduler time: 6.499758485238999 Scheduler overhead time: 0.03683136170729995 Adapter cache time: 0.02565114200115204 Engine time: 0.03805737663060427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.826813985593617,
    "estimated_duration": 3600.029776028385,
    "input_throughput": 6418.687465827728,
    "output_throughput": 5662.765662591359,
    "total_throughput": 12081.453128419087,
    "itl": 124.76674682556094,
    "ttft": 1908104.4352684927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 507129,
    "finished_requests": 93677,
    "scheduler_time": 51.63430824723473
}
#Debug simulation 
Total elapsed time: 6.826912327669561. Arrivals time: 0.29289576737210155 Scheduler time: 6.395835368894041 Scheduler overhead time: 0.04337618453428149 Adapter cache time: 0.029336952604353428 Engine time: 0.045075076166540384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.88818790204823,
    "estimated_duration": 3600.168308759441,
    "input_throughput": 6541.965258317518,
    "output_throughput": 5762.036166344719,
    "total_throughput": 12304.001424662238,
    "itl": 149.1522187321654,
    "ttft": 1894523.243740175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 507129,
    "finished_requests": 95413,
    "scheduler_time": 58.44052742864658
}
#Debug simulation 
Total elapsed time: 6.888295059092343. Arrivals time: 0.2967506339773536 Scheduler time: 6.47384334821254 Scheduler overhead time: 0.03696794481948018 Adapter cache time: 0.025820652022957802 Engine time: 0.03774159122258425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.814474289305508,
    "estimated_duration": 3600.0469493777355,
    "input_throughput": 6418.875177167964,
    "output_throughput": 5662.838092576482,
    "total_throughput": 12081.713269744447,
    "itl": 124.77250074884795,
    "ttft": 1908082.6597828248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378326,
    "arrivals": 507129,
    "finished_requests": 93678,
    "scheduler_time": 51.636093907674436
}
#Debug simulation 
Total elapsed time: 6.81459543434903. Arrivals time: 0.29010816756635904 Scheduler time: 6.38743288256228 Scheduler overhead time: 0.04335702396929264 Adapter cache time: 0.02875433722510934 Engine time: 0.04470452619716525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.041646176017821,
    "estimated_duration": 3600.0365366137084,
    "input_throughput": 6639.38428316144,
    "output_throughput": 5879.936990837094,
    "total_throughput": 12519.321273998534,
    "itl": 146.5980886893651,
    "ttft": 1898254.0910599725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 505238,
    "finished_requests": 96765,
    "scheduler_time": 59.670703983210146
}
#Debug simulation 
Total elapsed time: 7.041746638249606. Arrivals time: 0.36467682756483555 Scheduler time: 6.56106933997944 Scheduler overhead time: 0.03750006062909961 Adapter cache time: 0.022476010024547577 Engine time: 0.03855297947302461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.047551081050187,
    "estimated_duration": 3600.134183164825,
    "input_throughput": 6639.358363856202,
    "output_throughput": 5879.987223529227,
    "total_throughput": 12519.34558738543,
    "itl": 146.5994478006724,
    "ttft": 1898252.7778125827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 505238,
    "finished_requests": 96768,
    "scheduler_time": 59.67220577436318
}
#Debug simulation 
Total elapsed time: 7.047658496070653. Arrivals time: 0.3986711543984711 Scheduler time: 6.533203241415322 Scheduler overhead time: 0.03748147515580058 Adapter cache time: 0.022434677463024855 Engine time: 0.03840269986540079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.93917735805735,
    "estimated_duration": 3600.107291395938,
    "input_throughput": 6482.012371067839,
    "output_throughput": 5751.611083782758,
    "total_throughput": 12233.623454850598,
    "itl": 122.4665534352359,
    "ttft": 1913913.5211848703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 505238,
    "finished_requests": 94515,
    "scheduler_time": 52.367595950540355
}
#Debug simulation 
Total elapsed time: 6.939275986049324. Arrivals time: 0.2935854229144752 Scheduler time: 6.510037838481367 Scheduler overhead time: 0.044284178875386715 Adapter cache time: 0.02527783391997218 Engine time: 0.04533412680029869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.043754206970334,
    "estimated_duration": 3600.120340858894,
    "input_throughput": 6639.383891899978,
    "output_throughput": 5880.009831824037,
    "total_throughput": 12519.393723724015,
    "itl": 146.59968206704502,
    "ttft": 1898245.904487706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4002960364404137,
    "arrivals": 505238,
    "finished_requests": 96768,
    "scheduler_time": 59.67233237236015
}
#Debug simulation 
Total elapsed time: 7.043853077106178. Arrivals time: 0.36387754743918777 Scheduler time: 6.563601065892726 Scheduler overhead time: 0.03742474550381303 Adapter cache time: 0.022939408663660288 Engine time: 0.03848379570990801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.9877003082074225,
    "estimated_duration": 3600.0851679155553,
    "input_throughput": 6482.052204757,
    "output_throughput": 5751.6464289618425,
    "total_throughput": 12233.698633718843,
    "itl": 122.46385724007499,
    "ttft": 1913898.1639767594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 505238,
    "finished_requests": 94515,
    "scheduler_time": 52.36774848251921
}
#Debug simulation 
Total elapsed time: 6.987795969005674. Arrivals time: 0.3964081616140902 Scheduler time: 6.455346622038633 Scheduler overhead time: 0.04423921462148428 Adapter cache time: 0.025703416671603918 Engine time: 0.04541371436789632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.066678275354207,
    "estimated_duration": 3600.034391274279,
    "input_throughput": 6639.3882397161115,
    "output_throughput": 5879.940494820471,
    "total_throughput": 12519.328734536582,
    "itl": 146.59841075708567,
    "ttft": 1898248.7630702835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 505238,
    "finished_requests": 96765,
    "scheduler_time": 59.67076744164919
}
#Debug simulation 
Total elapsed time: 7.067076995968819. Arrivals time: 0.3641414591111243 Scheduler time: 6.5862231850624084 Scheduler overhead time: 0.03758418280631304 Adapter cache time: 0.022811336908489466 Engine time: 0.03855910478159785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.953927102033049,
    "estimated_duration": 3600.079161836114,
    "input_throughput": 6482.224126454457,
    "output_throughput": 5751.698523606694,
    "total_throughput": 12233.922650061151,
    "itl": 122.46284521487858,
    "ttft": 1913918.1198550954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 505238,
    "finished_requests": 94516,
    "scheduler_time": 52.36655939077306
}
#Debug simulation 
Total elapsed time: 6.954031919129193. Arrivals time: 0.29445576714351773 Scheduler time: 6.522947113029659 Scheduler overhead time: 0.04403607128188014 Adapter cache time: 0.02662270050495863 Engine time: 0.04523818055167794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.087741625960916,
    "estimated_duration": 3600.0430647882504,
    "input_throughput": 6796.652584332123,
    "output_throughput": 5941.051430521711,
    "total_throughput": 12737.704014853834,
    "itl": 144.03615600212692,
    "ttft": 1880245.6545891403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 504263,
    "finished_requests": 98426,
    "scheduler_time": 60.11752809828499
}
#Debug simulation 
Total elapsed time: 7.08784208400175. Arrivals time: 0.30595755903050303 Scheduler time: 6.665194547735155 Scheduler overhead time: 0.0380742521956563 Adapter cache time: 0.02171558840200305 Engine time: 0.03916811337694526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.095743441954255,
    "estimated_duration": 3600.1261109562097,
    "input_throughput": 6796.495802059869,
    "output_throughput": 5940.914384890601,
    "total_throughput": 12737.410186950468,
    "itl": 144.0370581628354,
    "ttft": 1880277.6263880502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 504263,
    "finished_requests": 98426,
    "scheduler_time": 60.118804015510484
}
#Debug simulation 
Total elapsed time: 7.095860445871949. Arrivals time: 0.36663187062367797 Scheduler time: 6.61258123582229 Scheduler overhead time: 0.03812617436051369 Adapter cache time: 0.021285037975758314 Engine time: 0.03934914479032159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.980674035847187,
    "estimated_duration": 3600.0252230648734,
    "input_throughput": 6643.636785310658,
    "output_throughput": 5809.521796126348,
    "total_throughput": 12453.158581437006,
    "itl": 121.14404096451743,
    "ttft": 1897755.610128294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 504263,
    "finished_requests": 96161,
    "scheduler_time": 52.88195414788229
}
#Debug simulation 
Total elapsed time: 6.9807754387147725. Arrivals time: 0.3009575274772942 Scheduler time: 6.54528761934489 Scheduler overhead time: 0.04442646028473973 Adapter cache time: 0.02332463301718235 Engine time: 0.04582244157791138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.143301049713045,
    "estimated_duration": 3600.087864719256,
    "input_throughput": 6796.568005961181,
    "output_throughput": 5940.977499355532,
    "total_throughput": 12737.545505316713,
    "itl": 144.036656214511,
    "ttft": 1880271.556642252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 504263,
    "finished_requests": 98426,
    "scheduler_time": 60.11851962636615
}
#Debug simulation 
Total elapsed time: 7.143403465859592. Arrivals time: 0.3411428462713957 Scheduler time: 6.686089739669114 Scheduler overhead time: 0.03802568418905139 Adapter cache time: 0.02111950609833002 Engine time: 0.039262987207621336 
